****
# Введение <a name="1"></a>

Благодаря достижениям в области искусственного интеллекта в последние годы открываются новые области применения методов и алгоритмов машинного обучения. В то время как проекты машинного обучения различаются по размеру и сложности, требуя различных навыков работы с данными, их общая структура как правило одинакова. Можно констатировать, что для получения хороших результатов анализа, важно иметь хорошо подобранные данные и команду экспертов, обладающую необходимыми навыками для управления проектом машинного обучения. В этом исследовании мы будем следовать конвейеру машинного обучения, состоящему из шести ступеней.

![Последовательность этапов конвейера машинного обучения](assets/ml_pipeline.png)
**Последовательность этапов конвейера машинного обучения**

Далее мы кратко опишем цели указанных этапов и алгоритмы, применяемые для их реализации.

## Алгоритмы предварительной обработки данных <a name="1_1"></a>

Предварительная обработка данных - это метод анализа данных, который включает преобразование необработанных данных в понятный формат. Реальные данные часто являются неполными, непоследовательными и/или недостающими в определенных видах поведения или тенденциях, могут содержать много ошибок. Предварительная обработка данных является проверенным методом решения таких проблем. Поскольку это важно для успеха любого процесса машинного обучения, мы дадим краткое описание основных методов предварительной обработки данных, включая:

1) обработку пропущенных значений;

2) преобразование признаков, нормализацию и стандартизацию;

3) уменьшение размерности;

4) выбор признаков;

5) разделение данных.

Полученные от первоисточника числовые данные могут быть представлены в различной форме. Поэтому, сначала выполняется очистка данных, которая включает в себя обработку пропущенных значений и отбрасывание всех данных, которые имеют недостоверные значения или могут быть классифицированы как т.н. выбросы. Так как такой процесс требует глубокого знания предметной области, он должен выполняться совместно с прикладными специалистами. Например, искаженные значения могут быть вызваны сбоем в работе механизма, который сгенерировал набор данных, или лицом, ответственным за запись данных. Только специалисты с высоким уровнем знаний в предметной области могут определить, какие значения следует опускать, а какие показывают целесообразное измерение. Тщательный анализ первичных данных должен сопровождаться ведением протокола, отражающего внесенные изменения. Это может оказаться существенным при дальнейшем понимании результатов машинного обучения. Пропущенные значения обычно заменяются некоторыми правдоподобными значениями, такими как среднее или наиболее частое значение столбца.

Далее принято производить масштабирование данных для каждого из призаков, чтобы поместить весь набор данных в один общий интервал. Очень важно, чтобы диапазон всех атрибутов был нормализован, чтобы каждый атрибут вносил одинаковый вклад в конечный результат [[1]](https://arxiv.org/abs/1811.03402). Тем не менее, это действие не должно влиять на тип категориальных атрибутов. Некоторыми популярными методами масштабирования являются: нормализация мин-макс, нормализация среднего значения, логарифмическое преобразование и стандартизация атрибутов.

После выполняется разработка признаков. Разработка признаков в основном означает сохранение наиболее важных признаков для достижения желаемой цели. Это можно сделать извлекая новые признаки из имеющихся в настоящее время признаки или удаляя признаки, которые не влияют на результат или добавляют шум, что приводит к снижению точности результатов.

Последний этап предварительной обработки данных, это разделение доступных данных на две части. Первая часть данных используется для создания модели (обучающая выборка), а другая - для оценки качества модели (тестовая выборка).

## Алгоритмы машинного обучения <a name="1_2"></a>

Выбор правильного алгоритма является ключевой частью любого проекта по машинному обучению, и, поскольку есть десятки вариантов на выбор, важно понять их сильные и слабые стороны в различных бизнес-приложениях [[2]](https://www.researchgate.net/publication/316273553_A_Survey_on_Machine_Learning_Concept_Algorithms_and_Applications). В этом курсе мы поговорим об алгоритмах в двух видах обучения: 

- Обучение с учителем, 
   
- Обучение без учителя.


### Обучение с учителем <a name="1_2_1"></a>

Алгоритмы обучения с учителем (контролируемого обучения) строят математическую модель набора данных, который содержит как входы, так и желаемые результаты. Этот набор известен как данные обучения и состоит из обучающих примеров. Каждый обучающий пример имеет один или несколько входов и желаемый выход, также известный как контрольный сигнал. В случае полуобучаемых алгоритмов, обучения в некоторых учебных примерах отсутствует желаемый результат. В математической модели каждый обучающий пример представлен массивом или вектором, а обучающие данные матрицей. Посредством итеративной оптимизации целевой функции, алгоритмы обучения с учителем изучают функцию, которая может использоваться для прогнозирования результатов, связанных с новыми входными данными. Оптимальная функция позволит алгоритму правильно определять выходные данные для входов, которые не были частью обучающих данных. 

Говорят, что алгоритм, улучшающий точность результатов или прогнозов с течением времени, научился выполнять эту задачу. Алгоритмы обучения с учителем включают классификацию и регрессию. Алгоритмы классификации используются, когда выходные данные ограничены ограниченным набором значений, а алгоритмы регрессии используются, когда выходные данные могут иметь любое числовое значение в пределах диапазона. Обучение сходству является областью контролируемого машинного обучения, тесно связанной с регрессией и классификацией, но цель состоит в том, чтобы учиться на примерах с использованием функции сходства, которая измеряет, насколько похожи или связаны два объекта. Оно имеет приложения для ранжирования, системы рекомендаций, визуального отслеживания личности, проверки лица и проверки докладчика.

### Обучение без учителя <a name="1_2_2"></a>

Алгоритмы обучения без учителя берут набор данных, который содержит только входные данные, и находят структуру данных, такую ​​как группировка или кластеризация точек данных. Алгоритмы поэтому учатся на тестовых данных, которые не были помечены, классифицированы или разделены на кластеры. Вместо того, чтобы реагировать на обратную связь, неконтролируемые алгоритмы обучения выявляют общие черты в данных и реагируют на основании наличия или отсутствия таких общих черт в каждом новом фрагменте данных. 

*Кластерный анализ* - это распределение набора наблюдений в подмножества (называемые кластерами), чтобы наблюдения в пределах одного кластера были похожи в соответствии с одним или несколькими заранее определенными критериями, в то время как наблюдения, полученные из разных кластеров, отличаются. Различные методы кластеризации делают разные предположения о структуре данных, часто определяемые некоторой метрикой сходства и оцениваемые, например, по внутренней компактности или сходству между членами одного кластера и различию - разнице между кластерами. Другие методы основаны на оценке плотности и связности графа.



****
## Типовая вычислительная инфраструктура для машинного обучения  <a name="1_3"></a>

В ходе выполнения конкурсных заданий и самостоятельной разработки конвейера машинного обучения мы предлагаем каждой команде развернуть набор средств для обработки тестовых данных.

Примером подобной системы является структура, представленная на следующем рисунке.

![](assets/hackathon2019.png)

Для выполнения заданий хакатона каждой команде предоставляется следующие вычислительные ресурсы:

- Пул ресурсов в ЦОД МГТУ им Баумана: 8 ГБ ОЗУ, 4 ядра микропроцессора 2400 МГц, 50 ГБ дискового пространства.
- Предоставляется ip адрес в DMZ МГТУ с открытыми портами.
- Предоставляется Локальная сеть в среде виртуализации VMWare VSphere.
- Допускается объединять ресурсы команд в кластеры (например, )
- Дополнительно команды могут использовать ресурсы облачной платформы `IBM Cloud`, которая может использоваться для реализации сервисов аналитической обработки и визуализации данных.  Для получения доступ к ресурсам IBM, необходимо использовать почтовый адрес из домена `student.bmstu.ru` или `bmstu.ru` при регистрации в программе Академической инициативе по адресу [ibm.biz/academic](ibm.biz/academic). Инструкция по получению почтового адрес из домена `student.bmstu.ru` находится [тут](https://mail.bmstu.ru/~postmaster/mail_for_students_and_aspirants.pdf).
 


****
## Проект хакатона <a name="1_4"></a>

Всем командам предлагается собрать template-проект, который может быть модифицирован командами для реализации собственных идей.
В проекте использовано следующие технологии:

-  Платформой виртуализации VMware vSphere 6.5
-  ОС Linux (Debian/Ubuntu/CentOS)
-  Anaconda/Jupyter Notebooks
-  Язык Python 3
-  IBM Cloud / Watson Studio
 

****
## Оборудование и настройка компьютера разработчика <a name="1_5"></a>

Участник хакатона может выполнить все задания на компьютере в аудитории, однако рекомендуется использовать собственный ноутбук. Желательно использование операционной системы Linux (Ubuntu, Arch, CentOS, RHEL) или MacBook. В случае использования компьютера под управлением OC Windows рекомендуется воспользоваться готовым образом виртуальной машины Oracle Virtual Box, который можно найти на портале: [https://www.osboxes.org/ubuntu/](https://www.osboxes.org/ubuntu/).

****
# День 1. Управление виртуальным ЦОД <a name="2"></a>

Понятие виртуализации (от Virtual - действительный, лат.) в широком смысле представляет собой сокрытие настоящей реализации какого-либо объекта или процесса от истинного его представления для пользователя. Результатом виртуализации является нечто удобное для использования, на самом деле, имеющее более сложную или совсем иную структуру, отличную от той, которая воспринимается при работе с объектом. Происходит отделение представления от реализации. В компьютерных технологиях под термином «виртуализация» обычно понимается абстракция вычислительных ресурсов (чаще всего серверов) и предоставление пользователю системы, которая «инкапсулирует» (скрывает в себе) собственную реализацию. Проще говоря, пользователь работает с удобным для себя представлением объекта, и для него не имеет значения, как объект устроен в действительности.

![Виртуальные машины, работающие на физическом оборудовании](assets/virtualisation.png)
**Виртуальные машины, работающие на физическом оборудовании**

Долгое время в центрах обработки данных стремились к децентрализованной архитектуре, масштабированию приложений и системной инфраструктуры в горизонтальном направлении. Эта тенденция, как правило, приводила к росту числа серверов. Со временем инфраструктура ЦОД становилась все более сложной, что сказывалось на возможности его модернизации, обслуживании, стоимости и пр. Централизованная архитектура может уменьшить количество серверов и увеличить степень использования оборудования при сравнимой производительности, а также уменьшить стоимость владения оборудованием. 

`Частичная виртуализация` (нативная виртуализация) базируется на принципе эмуляции только необходимого количества ресурсов, чтобы виртуальная машина могла быть запущена изолировано. Напротив, при `полной эмуляции` различных архитектур, гостевая система работает с определенной специфической системой команд процессора, отличной от системы команд процессора хостовой системы. Каждую команду процессору гостевой системы нужно транслировать в соответствующую команду хостовой системы, что невероятно уменьшает быстродействие.

При использовании нативной виртуализации никакой трансляции команд не происходит, так как гостевая операционная система разработана под ту же архитектуру, на которой работает хостовая система. Это позволяет значительно повысить быстродействие гостевой системы и максимально приблизить его к быстродействию реальной системы.

Для повышения быстродействия нативной виртуализации применяется специализированная программная прослойка – `гипервизор`. Гипервизор является посредником между гостевой операционной системой и физическим аппаратным обеспечением. Он позволяет гостевой системе напрямую обращаться к аппаратным ресурсам, что и является секретом высокого быстродействия данного вида виртуализации. Гипервизор является одним из ключевых понятий в мире виртуализации.

Частичная эмуляция является самым распространенным видом виртуализации в наше время. Основным ее недостатком является зависимость виртуальных машин от конкретной аппаратной архитектуры [3].
Примеры продуктов для частичной эмуляции: `VMware Workstation, VMware Server, VMware ESXI Server, Virtual Iron, Microsoft Hyper-V Server, Microsoft Virtual PC, Sun VirtualBox, Parallels Desktop` и другие.

В хакатоне мы будем использовать ЦОД под управлением гипервизора `ESXi 6.5` и платформы централизованного управления `VMware vCenter`. Вы можете ознакомиться с документацией по `VMware vCenter` [[тут]](https://docs.vmware.com/en/VMware-vSphere/index.html). В комплекте с `vCenter` предоставляется веб-клиент `VMware vSphere Web Client`, с помощью которого можно управлять виртуальной инфраструктурой через один из распространенных браузеров c поддержкой `Flash Player`).

Для выполнения заданий хакатона для каждой команды выделен пул виртуальных ресурсов:

- Оперативная память:  8 ГБ.

- Эквивалентная микропроцессорная частота: 2400 МГц.

- Объем дискового пространства 50 ГБ.

- Доступ во внешнюю сеть VM-Netwotk. 

- Виртуальная локальная сеть team**.


****
## Доступ к виртальному ЦОД <a name="21"></a>

Для доступа к выделенному для команды пула ресурсов необходимо открыть страницу по адресу:

* [https://195.19.40.127/vsphere-client/?csp](https://195.19.40.127/vsphere-client/?csp)

Для доступа необходимо использовать логин и пароль, предоставленные организаторами хакатона.

![Доступ к VMware vCenter](assets/vmware.png)
**Доступ к VMware vCenter**

После ввода логина и пароля вы на главную административную панель `VMware vSphere Web Client` (т.н. Home).

![](assets/esx_1.png)
**Главную административную панель `VMware vSphere Web Client`**

При первом входе вам необходимо изменить пароль. 

Войдите в меню **Roles -> Users and Groups**. Выберите в списке вашу команду (team), введите текущий пароль и дважды новый пароль. Требования к сложности пароля: только цифры, не менее 6 цифр.

![](assets/esx_2.png)


****
## Создание виртуальной машины сетевого шлюза и установка ОС <a name="22"></a>

Перейдите из главной административной панели в раздел **Hosts and Clasters**.

Иерархия виртуального ЦОД состоит из следующих уровней:

1) Уровень Центра обработки данных: `195.19.40.127`

2) Уровен кластера: `IU6`

3) Уровень хоста: `195.19.40.124` или `195.19.40.125` или `195.19.40.126`. 

4) Уровень консолидированного вычислительного пула (так называемое виртуальное приложение `vAPP`): `team**`.

Дальнейшие действия необходимо производить только на уровне 4). Управление остальными уровнями иерархии выполняется только администратором ЦОД.

![](assets/esx_3.png)
**Иерархия виртуального ЦОД**

Вызовите контекстное меню для вычислительного пула. Выберите пункт **New Virtual Machine**.

![](assets/esx_4.png)
**Создание новой виртуальной машины**

Далее следуйте указаниям диалога. 

В пункте **2a** выберите кластер для развертывания виртуальной машины: `IU6`. 

![](assets/esx_5_u.png)

В пункте **2b** выберите вычислительный пул: `team**`. 

![](assets/esx_6.png)

В диалоге выбора ресурсов виртуальной машины выберите следующие ресурсы:

- 1 CPU

- 1 GB RAM

- 10 GB HDD. 

- Две сетевые карты типа `VMXNET 3`, подключенные к сетям `VM-Netwotrk` и `team**`.

![](assets/esx_8.png)

!!!По умолчанию выбирается так называемый Thick (толстый) тип диска. Все пространство такого диска выделяется в момент создания, при этом блоки не очищаются от данных, которые находились там ранее. Это может создавать потенциальные угрозы безопасности, поскольку виртуальная машина может получить доступ к данным на хранилище VMFS, которые ей не принадлежат. При обращении к блокам такого диска их содержимое предварительно не очищается со стороны ESX. Преимущество дисков типа thick - производительность и быстрота создания, недостаток - безопасность

Поэтому мы будем использовать диски типа Thin ("тонкие диски"), позволяющий автоматически расширять занимаемое дисковое пространство по мере заполнения диска. 
Эти диски создаются минимального размера и растут по мере их наполнения данными до выделенного объема. При выделении нового блока - он предварительно очищается. Эти диски наименее производительны (выделение нового блока и его очистка), однако наиболее оптимальны для экономии дискового пространства на системе хранения данных.

![](assets/esx_8_1.png)


Для установки операционной системы необходимо подключить виртуальный привод CD/DVD и смонтировать в него установочный образ системы. Выберите пункт **Datastore ISO file**. В открывшемся диалоге выберите один из образов Ubuntu в папке **OS_images**. Включите пункт **Connect** для CD/DVD.

!!! Рекомендуется использовать версию ОС Linux без установленного рабочего стола. Например: `Ubuntu-18.04.3-live-server-amd64.iso`. Вы также можете использовать любой другой дистрибутив Linux, имеющийся в папке OS_images или ваш дистрибутив, подключив к CD/DVD файл iso с локального компьютера !!!


![](assets/esx_7_u.png)


Проверьте параметры ВМ. Создайте виртуальную машину.

![](assets/esx_9.png)

Теперь необходимо установить операционную систему. 

В контекстом меню ВМ выберите пункт **Power -> Power On**.

![](assets/esx_10.png)

Откройте консоль ВМ:

![](assets/esx_11.png)

Установите ОС со следующими сетевыми настройками:

На сетевом интерфейсе ens160:

    Параметры сети: 195.19.36.64/27
    IP адрес: выдается организаторами из диапазона 195.19.36.64/27
    Шлюз: 195.19.36.65
    DNS: 195.19.32.2

На сетевом интерфейсе ens192:

    Параметры сети: 192.168.1.0/24
    IP адрес: 192.168.1.1

![](assets/esx_12u.png)

При установке серверной версии ОС Ubuntu обязательно разрешите использование LVM (Logical Volume Manager), что позволит расширять объем логических томов в процессе работы Linux.

![](assets/esx_13.png)

Разрешите установку и запуск OpenSSH сервера, что понадобится для соединения с Вашей виртуальной системой по протоколу SSH.

![](assets/esx_14.png)

После установки системы перегрузите виртуальную машину и выполните вход по введенным вами реквизитам пользователя.

![](assets/esx_15.png)

Проверьте, что сетевое соединение работает:

    ping www.ya.ru

Если соединение отсутствует, его необходимо установить. Использовать следует способы и ПО, поставляемое с выбранной вами ОС.

В случае использования серверной версии Ubuntu, отредактируйте файл /etc/netplan/50-cloud-init.yaml. После этого актуализируйте конфигурацию сети по команде:

    sudo netplan apply

Внимание! В других версиях ОС Linux, отличных от Ubuntu 18.04.3 может потребоваться изменение файла /etc/network/interfaces или использование другого механизма настройки сети (см. мануалы к версии вашей ОС).

Используя терминал операционной системы можно выполнить пинг вашей системы:

    ping 195.19.36.**
    PING 195.19.36.** (195.19.36.**) 56(84) bytes of data.
    64 bytes from 195.19.36.**: icmp_seq=1 ttl=58 time=5.94 ms
    64 bytes from 195.19.36.**: icmp_seq=2 ttl=58 time=5.79 ms


Если успешно установлено соединение с вашей ВМ, то можно подключиться к ней используя терминал с поддержкой протокола SSH (Tilix, Putty, term и пр.).
В ОС Linux это можно сделать по команде:

    ssh username@ip_address

где ip_address - ip адрес вашей ВМ.


![](assets/esx_16.png)



****
## Создание виртуальной машины в локальной сети и настройка шлюза<a name="24"></a>

Если в проекте понадобится использовать несколько серверов (например, сервер видеоаналитики) и обеспечить к ним доступ по единственному выделенному ip адресу, потребуется использовать локальную сеть (сеть team** с диапазоном адресов 192.168.1.0/24). Доступ ко всем локальным серверам будет осуществляться через шлюз, в котором необходимо выполнить конфигурирование доступа.

Создадим новую виртуальную машину в вашем пуле ресурсов со следующими параметрами:

В диалоге выбора ресурсов виртуальной машины выберите следующие ресурсы:

- 2 CPU

- 4 GB RAM

- 20 GB HDD. 

- Одна сетевая карта типа `VMXNET 3`, подключенная к сети `team**`.
 

![](assets/esx_17.png)

Далее повторите установку операционной системы аналогично предыдущей. 

Настройте сетевой интерфейс следующим образом:

    Параметры сети: 192.168.1.0/24
    IP адрес: 192.168.1.2/24
    Шлюз: 192.168.1.1
    DNS: 195.19.32.2

После установки проверьте доступность шлюза по команде:

    ping 192.168.1.1

При необходимости отредактируйте файл /etc/netplan/50-cloud-init.yaml и выполните команду:

    sudo netplan apply

Далее настроим шлюз (виртуальную машину с внешним ip адресом `195.19.36.**`) таким образом, чтобы соединение с портом 2022 на интерфейсе ens160 перенаправлялось через интерфейс ens192 на порт 22 по сетевому адресу 192.168.1.2.

Войдите через ssh консоль на шлюз:

    ssh username@ip_address

Далее получите права root:

    sudo -i

Введите следующие команды (замените `**` на ваш ip адрес):

    iptables -t nat -A POSTROUTING -s 192.168.1.2 -o ens160 -j MASQUERADE
    iptables -t nat -A PREROUTING -p tcp -d 195.19.36.** --dport 2022 -j DNAT --to-destination 192.168.1.2:22
    iptables -t filter -A FORWARD -i ens192 -d 192.168.1.2 -p tcp --dport 2022 -j ACCEPT

Сохраним конфигурацию  iptables в файл:

    iptables-save > /etc/iptable.restore

Удостоверьтесь, что в системе разрешен форвардинг:

    cat /proc/sys/net/ipv4/ip_forward
    1

Если форвардинг запрещен (ответ:`0`), разрешите его в файле /etc/sysctl.conf. Необходимо раскомментировать строку:

    net.ipv4.ip_forward = 1

Далее применим изменения:

    sysctl -p 

Далее разрешим настройку iptables из файла /etc/iptable.restore при старте системы. Для этого создадим сервис:

    nano /etc/systemd/system/restore-iptables-rules.service


Скопируйте в файл следующие строки:

    [Unit]
    Description = Apply iptables rules

    [Service]
    Type=oneshot
    ExecStart=/bin/sh -c 'iptables-restore < /etc/iptable.restore'

    [Install]
    WantedBy=network-pre.target

Далее разрешим сервис командой:

    systemctl enable restore-iptables-rules.service


Далее выполним `reboot` и проверим, что iptables настроены правильно: 

    sudo iptables -L

 
Проверьте, что форвардинг портов работает и из внешней сети (подключитесь с вашего рабочего компьютера или с компьютера на кафедре):

    ssh username@ip_address -p 2022
 

****
## Уcтановка фреймворка Anaconda и Jupyter Notebooks <a name="25"></a>


Anaconda Distribution это фреймворк с открытым исходным кодом, который объединяет библиотеки и средства разработки кода на языках Python / R для решения задач в области машинного обучения и науки о данных. Anaconda доступна как в виде десктопного приложения на платформах Linux, Windows и Mac OS X, так и в составе серверного ПО. В состав дистрибутива входит более 1500 пакетов Python / R для научных исследований, средста управления бибилиотеками Conda, средства разработки  обучения моделей машинного обучения и глубокого обучения scikit-learn, TensorFlow и Theano, известные бибилотеки для анализа данных (Dask, NumPy, pandas и Numba и т.д.) и визуализации результатов (Matplotlib, Bokeh, Datashader, Holoviews и пр.)


Подключимся к виртуальной машине:

    ssh username@ip_address -p 2022


Подготовим виртуальную машину, созданную нами в локальной сети, к установке дистрибутива. Необходимо по крайней мере 2 ГБ свободного места на жестком диске в томе **/**. Проверим это: 

    df -h
    Filesystem                         Size  Used Avail Use% Mounted on
    udev                               463M     0  463M   0% /dev
    tmpfs                               99M  1.1M   98M   2% /run
    /dev/mapper/ubuntu--vg-ubuntu--lv  3.9G  3.7G     0 100% /
    tmpfs                              493M     0  493M   0% /dev/shm
    tmpfs                              5.0M     0  5.0M   0% /run/lock
    tmpfs                              493M     0  493M   0% /sys/fs/cgroup
    /dev/loop0                          89M   89M     0 100% /snap/core/7270
    /dev/sda2                          976M   76M  834M   9% /boot
    tmpfs                               99M     0   99M   0% /run/user/1000


Место, выделенное на диске во время установки системы явно недостаточно. Если на вашей виртуальной машине достаточно места, пропустите следющие шаги, перейдите [сюда](#230).

Так как мы использовали `LVM`, нам необходимо расширить логический том за счет имеющегося свободного пространства на диске. Выведем список логических томов:

    sudo lvdisplay

    --- Logical volume ---
    LV Path                /dev/ubuntu-vg/ubuntu-lv
    LV Name                ubuntu-lv
    VG Name                ubuntu-vg
    LV UUID                nZSozr-hX7F-foyY-op40-8uKp-IYL9-wLplX0
    LV Write Access        read/write
    LV Creation host, time ubuntu-server, 2019-09-29 15:54:06 +0000
    LV Status              available
    # open                 1
    LV Size                4.00 GiB
    Current LE             1024
    Segments               1
    Allocation             inherit
    Read ahead sectors     auto
    - currently set to     256
    Block device           253:0

В отчете указано, что логический том размещен в группе физических томов **ubuntu-vg**, где мы и будем выделять пространство. Уточним, есть ли свободное место в **ubuntu-vg**:

    sudo vgdisplay

    --- Volume group ---
    VG Name               ubuntu-vg
    System ID             
    Format                lvm2
    Metadata Areas        1
    Metadata Sequence No  3
    VG Access             read/write
    VG Status             resizable
    MAX LV                0
    Cur LV                1
    Open LV               1
    Max PV                0
    Cur PV                1
    Act PV                1
    VG Size               <49.00 GiB
    PE Size               4.00 MiB
    Total PE              12543
    Alloc PE / Size       1024 / 3.70 GiB
    Free  PE / Size       11519 / 46.30 GiB
    VG UUID               dvjRPM-MLcl-QsFw-sXPp-72CA-mZpy-Oeb3fy


Также можно узнать, на каком из физических дисков емеется свободное пространство (т.к. у нас только один диск, показан будет он)

    sudo pvdisplay

    --- Physical volume ---
    PV Name               /dev/sda3
    VG Name               ubuntu-vg
    PV Size               <49.00 GiB / not usable 0   
    Allocatable           yes 
    PE Size               4.00 MiB
    Total PE              12543
    Free PE               1024
    Allocated PE          11519
    PV UUID               Z1Ya4U-czwy-37JR-DAYw-SDTm-v9JI-oP3zCT


В отчете lvdisplay указан системный путь к логическому устройству тома: **/dev/ubuntu-vg/ubuntu-lv**. Расширим его на 2048 блоков по 4 MiB: 


    sudo lvextend -l +2048 /dev/ubuntu-vg/ubuntu-lv

    sudo resize2fs /dev/ubuntu-vg/ubuntu-lv 

    sudo lvdisplay

    --- Logical volume ---
    LV Path                /dev/ubuntu-vg/ubuntu-lv
    LV Name                ubuntu-lv
    VG Name                ubuntu-vg
    LV UUID                nZSozr-hX7F-foyY-op40-8uKp-IYL9-wLplX0
    LV Write Access        read/write
    LV Creation host, time ubuntu-server, 2019-09-29 15:54:06 +0000
    LV Status              available
    # open                 1
    LV Size                12.00 GiB
    Current LE             3072
    Segments               1
    Allocation             inherit
    Read ahead sectors     auto
    - currently set to     256
    Block device           253:0


Далее уточним состояние логического тома:

    df -h

    Filesystem                         Size  Used Avail Use% Mounted on
    udev                               463M     0  463M   0% /dev
    tmpfs                               99M  1.1M   98M   2% /run
    /dev/mapper/ubuntu--vg-ubuntu--lv   12G  3.7G    8G  30% /
    tmpfs                              493M     0  493M   0% /dev/shm
    tmpfs                              5.0M     0  5.0M   0% /run/lock
    tmpfs                              493M     0  493M   0% /sys/fs/cgroup
    /dev/loop0                          89M   89M     0 100% /snap/core/7270
    /dev/sda2                          976M   76M  834M   9% /boot
    tmpfs                               99M     0   99M   0% /run/user/1000

<a name="230"></a>
Теперь места на диске достаточно, начнем установку Anaconda. Установим дистрибутив в домашней директории пользователя виртуальной машины:

    cd ~
    wget https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh
    chmod +x Anaconda3-2019.03-Linux-x86_64.sh
    bash ./Anaconda3-2019.03-Linux-x86_64.sh

Проверим список установленных пакетов:

    conda list

Обновим устаревшие пакеты:

    conda update conda

Теперь необходимо настроить Jupyter Notebook. Сгенерируем шаблон конфигурационного файла:

    jupyter notebook --generate-config

Теперь добавим в конфигурационный файл, указанный при создании (`~/.jupyter/jupyter_notebook_config.py`) следующие строки:


    # Разрешим доступ из внешней сети
    c.NotebookApp.ip = '192.168.1.2'
    c.NotebookApp.open_browser = False
    # Укажем порт, на котором будет работать Jupyter Notebook
    c.NotebookApp.port = 8888

Далее создадим файл с хешированным паролем:

    jupyter notebook password


Теперь все готово для запуска Jupyter Notebook. Вы можете сделать это в консоли по команде:


    jupyter notebook &
    # или
    jupyter lab &
    # если вы не хотите читать лог приложения в консоли, вы можете перенаправить поток сообщений на устройство /dev/null:
    jupyter notebook &>/dev/null &


Далее необходимо разрешить фовардинг порта 8888 на шлюзе `195.19.36.**`. Подключимся к виртуальной машине шлюза (не перепутайте!!!):

    ssh username@ip_address

Далее:

    sudo -i
    iptables -t nat -A PREROUTING -p tcp -d 195.19.36.** --dport 8888 -j DNAT --to-destination 192.168.1.2:8888
    iptables -t filter -A FORWARD -i ens192 -d 192.168.1.2 -p tcp --dport 8888 -j ACCEPT

Сохраним конфигурацию  iptables в файл:

    iptables-save > /etc/iptable.restore


Теперь вы можете подключиться к Jupyter Notebook в броузере с вашего десктопа по адресу:

    http://<ip адрес вашего сервера>:8888/

Помните, что порт 8888 должен быть открыт в firewall вашей ОС


****
# День 2. Практикум в Jupyter Notebook <a name="3"></a>

## Методология машинного обучения <a name="3_1"></a>

Существует мнение о том, что не существует единого метода машинного обучения, который лучше всего справлялся бы со всеми проблемами. Независимо от того, насколько сложен или прост применяемый метод или алгоритм, он не будет работать наилучшим образом для всех проблем. Поэтому, чтобы найти наилучший метод и его алгоритмическую реализацию, которые соответствовали бы потребностям конкретной задачи, необходимо обладать широким кругозором методов и алгоритмов, а также владеть техническими средствами анализа данных.

В нашем конкурсе будем придерживаться следующей упрощенной методологии машинного обучения:

    - Прежде чем углубляться в сложные методы и тратить время на тонкую настройку модели, лучше попробовать более простые методы и алгоритмы. По мере продвижения к более сложным методам мы можем обнаружить, что для  наших нужд оказывается достаточно уже примененнного и более простого подхода.

    - Гибкая методология машинного обучения предполагают, что мы должны развивать понимание данных итерационно. Это означает, что мы не должны пытаться разрешить все проблемы сразу. Для науки о данных это означает, что мы начинаем с простого подхода и готовимся к применению более сложных методов, методик, алгоритмов, моделей и т.д и т.п.. 

    - Первая итерация должна иметь наиболее простой вариант реализации каждого этапа конвейера машинного обучения(например, обработка, извлечение признаков и пр.). Дополнительным достоинством такого подхода является то, что применение упрощенных подходов менее затратно с точки зрения вычислительной мощьности, не требует интенсивных вычислений или дорогих поисков гиперпараметров. Бесспорно, простая модель может работать плохо, но получение этой модели возможно максимально быстро и с минимальными затратами ресурсов. 

    - Для интерпретации полученных результатов и уточнения применяемых методов необходимо привлечение специалистов по предметной области, которые могут интерпретировать полученный результат. 

Пример такого простого метода: поиск ближайшего соседа и другие подобные методы. Для их реализации требуется лишь несколько строк кода. Вместе с тем нет никаких причин, по которым более простые методы не могут быть лучше сложных. На последующих итерациях конверйера машинного обучения исследуются другие подходы. Это позволит нам сравнить, как методологические изменения влияют на производительность, и отслеживать улучшения с течением времени. Тем не менее, по мере дальнейших исследований улучшение качества модели становится все более проблематичным. Например, если мы достигли точности 99%, возможно, нам не следует тратить больше времени и ресурсов, чтобы пробовать доводить ее до 99,2%. 


[Словарь терминов машинного обучения](https://docs.microsoft.com/ru-ru/dotnet/machine-learning/resources/glossary)

Далее подробнее рассмотрим этапы конвейера машинного обучения.


## Предварительная обработка данных <a name="3_2"></a>


Целью предварительной обработки является преобразование необработанных данных в форму, которая подходит для машинного обучения. Структурированные и чистые данные позволяют получать более точные результаты из прикладной модели. Этап предполагает форматирование данных, очистку и выборку достоверных значений. Этот этап может также включать в себя сокращение числа несвязанных признаков с помощью композиции признаков, если к исследованиям подключены специалисты, которые могут это сделать (для этого требуются расширенные знания предметной области). Привлечение специалистов в преметной области является важнейшим фактором успеха применения машинного обучения. 


### Очистка данных <a name="3_2_1"></a>

*Очистка данных*, это набор процедур, которые позволяют удалить шум и устранить несоответствия в данных. Этот процесс включает в себя заполнение недостающих данных с использованием метдов подстановки правдоподобных значений (таких, как замена отсутствующих значений на средние или максимальные значения и т.д.). Обнаружение выбросов (наблюдений, которые значительно отличаются от остальной части распределения) также важно для понимания данных. Анализ таких данных и принятие решения об очистке должен приниматься совместно со специалистом по предметной области.  Если какие-либо выбросы указывают на ошибочные данные, их следует удалить или исправить, если это возможно. Этот этап также включает в себя удаление неполных и бесполезных записей данных и столбцов.

### Масштабирование <a name="3_2_2"></a>

*Масштабирование* – данные могут иметь числовые атрибуты (признаки), которые охватывают сильно отличающиеся диапазоны, например, миллиметры, метры и километры. Масштабирование - это преобразование таких атрибутов таким образом, чтобы они имели одинаковый масштаб, например, в диапазоне от 0 до 1 или от 1 до 10 для наименьшего и наибольшего значения для атрибута. 

*Минимаксная* нормализация представляет собой линейное отображение данных из одного интервала в другой. Данный подход предполагает вычитание минимального значения объекта из остальных значений, после чего значения делятся на полученный диапазон. Он сохраняет исходное распределение функции и не вносит существенных изменений в информацию, встроенную в исходные данные. 

*Нормализация стандартным отклонением* модифицирует признаки путем вычитания среднего значения, а затем деления на стандартное отклонение. Это изменяет распределение признаков и приводит к распределению со средним значением, равным нулю, и стандартным отклонением, равным единице. 

*Логарифмическое преобразование* важно для преобразования мультипликативных отношений между признаками в аддитивные отношения. Так как большие значения уменьшаются больше, чем маленькие, логарифмическое преобразование имеет эффект псевдоскейлинга, поскольку различия между большими и малыми значениями в наборе данных уменьшаются. 

Выбор одного метода масштабирования среди других зависит не только от набора данных, но и от выбранного метода машинного обучения, поскольку различные методы машинного обучения фокусируются на разных аспектах данных. Например, метод кластеризации фокусируется на анализе сходства точек данных, в то время как анализ главных компонентов (PCA) выявляет наиболее существенные признаки данных. Использование наиболее адекватного метода масштабирования может улучшить результаты кластеризации.

**Дополнительные источники литературы по данному разделу:**

- [Практическое руководство по подготовке данных (en)](https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/)

- [Масштабирование данных данных (en)](https://codefellows.github.io/sea-python-401d5/lectures/rescaling_data.html#)


### Уменьшение размерности<a name="3_2_3"></a>

*Уменьшение размерности* (Dimesion Reduction) – основная проблема при интерпретации многомерных данных заключается в том, что соответствующие истинные закономерности скрыты нерелевантными данными или шумом. Для малых и средних наборов данных можно использовать классические статистические методы, которые фокусируются на идентификации сильных статистических закономеростей. Такие методы, однако, не могут быть распространены на случаи, когда размерность данных намного превышает размер записей, а слабые истинные сигналы окружены значительным количеством шума (например, как в случае современного геномного анализа). Кроме того, диапазон шума имеет тенденцию увеличиваться с увеличением размерности данных, что часто делает существующие методы непрактичными [3]. По мере роста количества признаков или измерений, объем данных, которые нам необходимы для точного обобщения, растет в геометрической прогрессии.

При добавлении нового признака в модель иногда не хватает данных для поддержания отношений, и, следовательно, новый признак может не иметь положительное влияние на модель. Например, в нашем исследовании мы имеем 299 переменных (p = 299). В этом случае мы можем иметь 299 (299-1) / 2 = 44551 различных парных групп признаков. Нет смысла визуализировать каждый из них в отдельности. В тех случаях, когда у нас большое количество переменных, лучше выбрать подмножество этих переменных (p << 100), которое собирает столько информации, сколько и исходный набор переменных. Вот некоторые преимущества применения уменьшения размерности к набору данных:

• пространство, необходимое для хранения данных, уменьшается по мере уменьшения количества измерений;

• уменьшение размеров приводит к меньшему времени вычислений / обучения;

• некоторые алгоритмы плохо обрабатывают большие размеры данных. Таким образом, чтобы эти алгоритмы были полезны, необходимо уменьшить размерность задачи;

• уменьшение размерности способствует т.н. мультиколлинеарности (наличии линейной зависимости между объясняющими переменными), удаляя лишние признаки. Например, у вас есть две переменные - «время, проведенное на беговой дорожке в минутах» и «потраченные калории». Эти переменные сильно коррелируют: чем больше времени вы проводите на беговой дорожке, тем больше калорий вы будете сжигать. Следовательно, нет смысла хранить оба, так как достаточно одного из них для создания адекватной модели.

• это помогает в визуализации данных. Как обсуждалось ранее, очень трудно визуализировать данные для многих измерений. Поэтому сокращение пространства до 2D или 3D может позволить нам более четко отображать и наблюдать кластеры данных.

Уменьшение размерности может быть сделано двумя различными способами:

• выбором признаков: сохраняются только самые важные переменные из исходного набора данных,

• путем нахождения меньшего набора новых переменных, каждая из которых является комбинацией входных переменных, содержащих в основном ту же информацию, что и входные переменные.


**Дополнительные источники литературы по данному разделу:**

- [Практическое руководство по подготовке данных (en)](https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/)

- [Масштабирование данных данных (en)](https://codefellows.github.io/sea-python-401d5/lectures/rescaling_data.html#)

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	



### Алгоритм отбора признаков<a name="3_2_4"></a>

*Алгоритм отбора признаков* – один из наиболее широко используемых алгоритмов выбора объектов для построения модели данных. Этот алгоритм помогает выбрать меньшее подмножество признаков по сравнению с первоначальным. Алгоритм отбора признаков можно рассматривать как комбинацию техник поиска для представления нового поднабора признаков вместе с вычислением меры, которая отражает различие подмножеств признаков. 

    Wiki цитата: В традиционной статистике наиболее популярной формой отбора признаков является ступенчатая регрессия, которая является техникой оборачивания. Это жадный алгоритм, который добавляет лучший признак (или удаляет худший) на каждом шаге алгоритма. Главная проблема — когда остановить алгоритм. При обучении машин это обычно делается путём перекрёстной проверки. В статистике некоторые критерии оптимизированы. Это ведёт к наследованию проблемы вложения. Исследовались и более устойчивые методы, такие как метод ветвей и границ и кусочно-линейная сеть.

**Дополнительные источники литературы по данному разделу:**

- [Алгоритмы отбора признаков (ру)](https://ru.wikipedia.org/wiki/%D0%9E%D1%82%D0%B1%D0%BE%D1%80_%D0%BF%D1%80%D0%B8%D0%B7%D0%BD%D0%B0%D0%BA%D0%BE%D0%B2)

### Обратное удаление признаков<a name="3_2_5"></a>

*Обратное удаление признаков* (Backward Feature Elimination) - рекурсивно удаляет некоторые отобранные признаки, строит модель с использованием оставшихся признаков и вычисляет точность модели. Он включает в себя следующие этапы:

1) сначала берутся все «n» переменных, присутствующих в наборе данных, и на них обучатся модель;

2) далее рассчитывается точность модели,

3) затем вычисляется точность модели после исключения каждой из переменной (всего получается n упрощенных моделей), то есть каждый раз отбрасывается одна переменная и модель строится на оставшихся «n-1» переменных;

4) определяется переменная, удаление которой дало наименьшее изменение точности модели, а затем эта переменная отбрасывается;

5) весь процесс повторяется пока какая-либо переменная может быть отброшена.

Для работы алгоритма нужно указать алгоритм и количество объектов, которые нужно выбрать. С помощью данного алгоритма также может быть выполнено ранжирование переменных.


### Выбор вперед<a name="3_2_6"></a>

*Выбор вперед* (Forward Feature Elimination) - это противоположный *обратному удалению признаков* процесс. Вместо того, чтобы исключать признаки и удалять их, мы пытаемся найти лучшие признаки для представления более точной модели. Эта техника работает следующим образом:

1) мы начинаем с одного признака. По сути, мы обучаем модель «n» раз, используя каждый признак отдельно;

2) переменная, дающая наилучшую точность обученной модели выбирается в качестве начальной переменной;

3) затем мы повторяем этот процесс и добавляем одну переменную за каждый проход алгоритма;

4) переменная, которая дает наибольшее увеличение производительности, фиксируется в модели;

5) мы повторяем этот процесс до тех пор, пока не будут замечены значительные улучшения в производительности модели.

    ПРИМЕЧАНИЕ. Как обратное удаление, так и прямой выбор функций требуют много времени и вычислительных затрат.

**Дополнительные источники литературы по данному разделу:**

- [Forward Feature Elimination Wiki](https://en.wikipedia.org/wiki/Feature_selection)

### Анализ главных компонентов<a name="3_2_7"></a>

*Анализ главных компонентов* (Principal Component Analysis) PCA, это методика, которая помогает нам извлекать новый набор переменных из существующего большого набора переменных. Эти вновь извлеченные переменные и называются главными компонентами. 

Вот некоторые из ключевых моментов, которые следует знать о PCA:

1) главные компоненты представляет собой линейную комбинацию исходных переменных;

2) основные компоненты выбираются таким образом, что первый основной компонент обеспечивал максимальную дисперсию в наборе данных;

3) второй основной компонент пытается объяснить оставшуюся дисперсию в наборе данных и не связан с первым главным компонентом,

4) каждое дополнительное измерение, которое мы добавляем в методике PCA, отражает все меньше и меньше дисперсии в модели. Первый компонент является наиболее важным, затем следует второй, затем третий и т. д.

5) чтобы найти каждый компонент, алгоритм пытается максимизировать дисперсию, и каждый новый компонент должен быть ортогональным к другим.

![](assets/ml_01.png)
**Анализ главных компонентов**

Слева мы имеем представление простого двумерного набора данных с тремя одномерными гиперплоскостями. С другой стороны, справа показан результат проецирования набора данных на каждую из этих одномерных гиперплоскостей. Cтановится ясно, что гиперплоскость, представленная сплошной линией, выявляет максимальную дисперсию в наборе данных. Она также позволяет найти вторую ось (пунктирную линию), ортогональную первой, которая учитывает наибольшее количество оставшихся отклонений данных.

Если бы мы имели дело с большим количеством измерений (как это и есть в реальных задачах), PCA нашел бы большее количество ортогональных осей к предыдущим осям (фактически столько осей, каково число измерений в наборе данных). Единичный вектор, который определяет i-ю ось, называется главным компонентом (PC). В этом случае первый PC - это С1, а второй PC - С2. После того, как мы определили наши основные компоненты, пришло время уменьшить размерность набора данных до d измерений, проецируя его на гиперплоскость, определенную первыми d основными компонентами. Обычно, в практических задачах, большое количество измерений, это то, что составляет достаточно большую часть дисперсии (~ 90%).

**Дополнительные источники литературы по данному разделу:**

- [Метод главных компонент (Wiki)](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D1%8B%D1%85_%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82)


### Независимый компонентный анализ<a name="3_2_8"></a>


*Независимый компонентный анализ* (Independent Component Analysis, ICA) - основан на теории информации, а также является одним из наиболее широко используемых методов уменьшения размерности. Основное различие между PCA и ICA состоит в том, что PCA ищет некоррелированные факторы, в то время как ICA ищет независимые факторы. Если две переменные некоррелированы, это означает, что между ними нет линейной зависимости. Если они независимы, это означает, что они не зависят от других переменных. Например, возраст человека не зависит от того, что он ест, или от того, сколько он смотрит телевизор.

Этот алгоритм предполагает, что данные переменные представляют собой линейные смеси некоторых неизвестных скрытых переменных. Также предполагается, что эти скрытые переменные являются взаимно независимыми, то есть они не зависят от других переменных и, следовательно, их называют независимыми компонентами наблюдаемых данных.

**Дополнительные источники литературы по данному разделу:**

- [Анализ независимых компонент Wiki](https://ru.wikipedia.org/wiki/%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D0%BD%D0%B5%D0%B7%D0%B0%D0%B2%D0%B8%D1%81%D0%B8%D0%BC%D1%8B%D1%85_%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82)


### Факторный анализ<a name="3_2_9"></a>


*Факторный анализ* (Factor Analysis) –  предположим, у нас есть две переменные: доход и образование. Эти переменные потенциально могут иметь высокую корреляцию, поскольку люди с более высоким уровнем образования, как правило, имеют значительно более высокий доход, и наоборот. В методе факторного анализа переменные сгруппированы по их корреляциям, то есть все переменные в определенной группе будут иметь высокую корреляцию между собой, но низкую корреляцию с переменными другой группы (групп). Здесь каждая группа известна как фактор. Эти факторы невелики по сравнению с исходными размерами данных. Однако эти факторы трудно наблюдать.

**Дополнительные источники литературы по данному разделу:**

- [Факторный анализ](https://ru.wikipedia.org/wiki/%D0%A4%D0%B0%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7)

- [Просто о факторном анализе](https://habr.com/ru/post/224495/)


### Многообразное обучение или нелинейное уменьшение размерности<a name="3_2_10"></a>


*Многообразное обучение или нелинейное уменьшение размерности* (Manifold Learning) –  это нелинейная версия PCA. Проблема состоит в том, что PCA ищет плоские поверхности для описания данных, которые не всегда наилучшим образом показывают макимальную дисперсию. Если плоской поверхности не существует, мы используем Mainfold Learning, чтобы попытаться решить эту проблему более эффективно.
Существует много подходов для решения этой проблемы, таких как Isomap, *Локально линейное вложение*, *Лапласово собственное отображение*, *Полуопределенное вложение* и т.д. Эти алгоритмы работают для извлечения низкоразмерного многообразия, которое можно использовать для описания многомерных данных.


### Локально линейное вложение<a name="3_2_11"></a>

*Локально линейное вложение* (LLE, Locally-Linear Embedding) –  это метод коллективного обучения, который не опирается на проекции в гиперплоскости, подобно PCA. Он работает, изучая, как каждое наблюдение линейно связано с его ближайшими соседями, а затем ищет низкоразмерное представление обучающего набора, где эти отношения лучше всего сохраняются. Для случаев, когда нет большого шума, LLE показывает лучшие результаты по сравнению с PCA. 

**Дополнительные источники литературы по данному разделу:**

- [An Introduction to Locally Linear Embedding](https://cs.nyu.edu/~roweis/lle/papers/lleintro.pdf)

### Cтохастическое вложение соседей с t-распределением<a name="3_2_12"></a>

*Cтохастическое вложение соседей с t-распределением* (t-SNE, t- Distributed Stochastic Neighbor Embedding) –  ищет шаблоны нелинейным способом. t-SNE - это один из немногих алгоритмов, который способен одновременно сохранять как локальную, так и глобальную структуру данных. Он рассчитывает вероятностное сходство точек в многомерном пространстве, а также в низкоразмерном пространстве. Эвклидовы расстояния больших размеров между точками данных преобразуются в условные вероятности, которые представляют сходства.
UMAP –  t-SNE очень хорошо работает с большими наборами данных, но также имеет свои ограничения, такие как потеря крупномасштабной информации, медленное время вычислений и неспособность осмысленно представлять очень большие наборы данных. 

**Дополнительные источники литературы по данному разделу:**

- [t-SNE Вики](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%B2%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9_%D1%81_t-%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%D0%BC)

- [Алгоритм t-SNE. Иллюстрированный вводный курс](http://datareview.info/article/algoritm-t-sne-illyustrirovannyiy-vvodnyiy-kurs/)

### Унифицированная аппроксимация и проекция многообразия<a name="3_2_13"></a>

*Унифицированная аппроксимация и проекция многообразия* (UMAP) - это метод сокращения размерности, который может сохранить как большую часть локальной, так и большей глобальной структуры данных по сравнению с t-SNE, с более коротким временем выполнения. 

Некоторые из ключевых преимуществ UMAP:

• он может обрабатывать большие наборы данных и данных большого размера без особых проблем,

• он сочетает в себе возможности визуализации с возможностью уменьшения размеров данных,

• наряду с сохранением локальной структуры, он также сохраняет глобальную структуру данных. UMAP отображает близкие точки на многообразии в соседние точки в низкоразмерном представлении и делает то же самое для удаленных точек,

• этот метод использует концепцию k-ближайшего соседа и оптимизирует результаты с использованием стохастического градиентного спуска. Сначала он вычисляет расстояние между точками в многомерном пространстве, проецирует их на низкоразмерное пространство и вычисляет расстояние между точками в этом низкоразмерном пространстве. Затем он использует Stochastic Gradient Descent, чтобы минимизировать разницу между этими расстояниями.

Корреляция между компонентами, полученными из UMAP, значительно меньше по сравнению с корреляцией между компонентами, полученными из t-SNE. Следовательно, UMAP имеет тенденцию давать лучшие результаты.
Краткое описание того, когда использовать каждую методику уменьшения размерности

**Дополнительные источники литературы по данному разделу:**

- [Uniform Approximation and Projection (UMAP) Вики](https://ru.wikipedia.org/wiki/UMAP)



### Краткий обзор алгоритмов и методов уменьшения размерности <a name="3_2_14"></a>
Кратко подведем итоги использования каждого метода уменьшения размерности, который мы рассмотрели. Важно понимать, где использовать определенную технику, поскольку это помогает сэкономить время, усилия и вычислительные мощности.

![](assets/ml_02.png)
**Использование методов уменьшения размерности**

#### Соотношение пропущенных значений<a name="3_2_14_1"></a>

Если в наборе данных слишком много пропущенных значений, мы используем этот подход для уменьшения количества переменных. Мы можем отбросить переменные с большим количеством пропущенных значений.

#### Фильтр низкой дисперсии<a name="3_2_14_2"></a>

Мы применяем этот подход для определения и удаления постоянных переменных из набора данных. На целевую переменную не влияют чрезмерно переменные с низкой дисперсией, и, следовательно, эти переменные могут быть безопасно отброшены

#### Фильтр высокой корреляции<a name="3_2_14_3"></a>

Пара переменных, имеющих высокую корреляцию, увеличивает мультиколлинеарность в наборе данных. Таким образом, мы можем использовать эту технику, чтобы найти сильно коррелированные функции и отбросить их соответственно.

#### Случайный лес<a name="3_2_14_4"></a>

Это один из наиболее часто используемых методов, который говорит нам о важности каждого атрибута, присутствующего в наборе данных. Мы можем найти важность каждого атрибута и сохранить самые верхние атрибуты, что приведет к уменьшению размерности. Как методы обратного удаления, так и прямого выбора элементов занимают много вычислительного времени и поэтому обычно используются в небольших наборах данных.

#### Факторный анализ<a name="3_2_14_5"></a>

Этот метод лучше всего подходит для ситуаций, когда у нас есть сильно коррелированный набор переменных. Он делит переменные на основе их соотношения на разные группы и представляет каждую группу с коэффициентом

#### Анализ основных компонентов<a name="3_2_14_6"></a>

Это один из наиболее широко используемых методов работы с линейными данными. Он делит данные на набор компонентов, которые пытаются объяснить как можно больше различий
Независимый анализ компонентов: мы можем использовать ICA для преобразования данных в независимые компоненты, которые описывают данные с использованием меньшего количества компонентов

- ISOMAP: мы используем эту технику, когда данные сильно нелинейны;

- t-SNE: этот метод также хорошо работает, когда данные сильно нелинейны или же требуется более понятная визуализаци данных.

- UMAP: эта методика хорошо работает для многомерных данных. Время его выполнения короче по сравнению с t-SNE.

### Разделение набора данных<a name="3_2_20"></a>

**Разделение набора данных** (Dataset splitting) – набор данных, используемый для машинного обучения, должен быть разделен на три подмножества - наборы обучения, тестирования и проверки.

**Обучающий набор**: используется для обучения модели и определения ее оптимальных параметров - параметров, которые он должен изучить из данных.

**Тестовый набор**: тестовый набор необходим для оценки обученной модели и ее способности к обобщению. Обобщение означает способность модели идентифицировать закономерности в новых невидимых данных после того, как они прошли обучение по данным обучения. Крайне важно использовать различные подмножества для обучения и тестирования, чтобы избежать переобучения модели, что является неспособностью к обобщению, о котором мы упоминали выше.

**Валидационный набор**: цель выделения валидационный набор набора состоит в том, чтобы настроить гиперпараметры модели - структурные параметры более высокого уровня, которые не могут быть непосредственно изучены из данных. Эти параметры могут указывать, например, насколько сложна модель и как быстро она находит шаблоны в данных.

Соотношение обучения и тестового набора обычно составляет 80 процентов. Затем обучающий набор снова разделяется, и его 20 процентов будут использоваться для формирования валидационного набора.
Чем больше используемых данных обучения, тем лучше будет работать потенциальная модель. Следовательно, больше используемых данных тестирования приводит к лучшей производительности модели и возможности обобщения.


## Практическая часть <a name="3_3"></a>

Чтобы работать с данными, необходимо понимать, что они из себя представляют. В начале любого исследования необходимо их загрузить и вывелить некоторые статистики.

Создайте новый Notebook и перенести в папку проекта наборы данных с сайта хакатона:

- [Обучающая выборка данных](data/train_longevity.csv)
- [Тестовая выборка данных](data/test_longevity.csv)

### Первичный анализ данных <a name="3_3_1"></a>


Импортируем библиотеки, нужные нам для работы

```python
 # data analysis and wrangling
import pandas as pd
import numpy as np
import random as rnd

 # visualization
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline 
```

Загрузим данные из исходных файлов:

```python
train_df = pd.read_csv('train_longevity.csv')
test_df = pd.read_csv('test_longevity.csv')
combine = [train_df, test_df]
```

Далее выведем названия столбцов в обучающем датасете:

```python
print(train_df.columns.values)
['Id' 'Longevity' 'Education' 'Sex' 'Age' 'Pet' 'Children' 'Region' 'Activity' 'MedExam' 'Sport']
```

И в тестовом датасете:

```
print(test_df.columns.values)
['Id' 'Education' 'Sex' 'Age' 'Activity' 'Sport' 'IsAlone' 'Age*Education']
```

Это даст нам первое представление о наших данных. Далее посмотрим на размеры наших табличных данных. Выполнив построчно код ниже как для 

```python
train_df.shape  	# мы увидим информацию о размерности нашего датафрейма 
train_df.info() 	# покажет информацию о размерности данных 
              	        # описание индекса, количество not-a-number элементов 
train_df.head()         # показывает первые 10 значений датасета
train_df.describe() 	# показывает статистики count,mean, std, min, 25%-50%-75% percentile, max 
train_df.nunique() 	# количество уникальных значений для каждого столбца
```

Приведем общую терминологию касающауюся наборов данных. Столбцы таблицы датасета это *атрибуты* объекта или его *признаки*. Строки - *объекты*.

**Объект** описывается как набор атрибутов. Объект также известен как запись, случай, пример, строка таблицы и т.д.

**Атрибут** - свойство, характеризующее объект. Атрибут также называют переменной, полем таблицы, измерением, характеристикой.

**Переменная** (variable) - свойство или характеристика, общая для всех изучаемых объектов, проявление которой может изменяться от объекта к объекту.

**Значение** (value) переменной является проявлением признака.

**Генеральная совокупность** (population) - вся совокупность изучаемых объектов, интересующая исследователя.

**Выборка** (sample) - часть генеральной совокупности, определенным способом отобранная с целью исследования и получения выводов о свойствах и характеристиках генеральной совокупности.

**Параметры** - числовые характеристики генеральной совокупности.

**Статистики** - числовые характеристики выборки.

**Гипотеза** - предположение относительно параметров совокупности объектов, которое должно быть проверено на ее части. Гипотеза - частично обоснованная закономерность знаний, служащая либо для связи между различными эмпирическими фактами, либо для объяснения факта или группы фактов.


**Дополнительные источники литературы по данному разделу:**

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	


### Описание задачи и данных <a name="3_3_2"></a>


В нашем хакатоне мы будем использовать задачу анализа факторов активного долголетия. К долголетним будем относить пожилых людей, доживших до 90 лет и сохранивших физическую и социальную активность. Представленные в датасетах данные носят иммитационный характер, однако на реальных данных. Данные представляют информацию о пожилых людях в возрасте от 70 до 80 лет, для которых известен класс активного долголетия (зависимый параметр Longevity) в будущем (будет ли достигнут вораст 90 лет).

В датасетах приведена следующая информация о пожилых людях (атрибуты объекта):

- Longevity - Класс активного долголетия: 1 - человек доживет до 90 лет; 0 - нет
- Id - Идентификатор пожилого человека;
- Education - Образование: 1 - высшее; 2 - среднее; 3 без образования;
- Sex - Пол;
- Age - Возраст;
- Pet - Пожилой человек ухаживает за домашними животными: указано количество;
- Children - Пожилой человек проживают с детьми/внуками/близкими родственниками: указано количество проживающих совместно в пожилым человеком;
- Region - Регион проживания;
- Activity - Уровень физической активности (количество шагов в день): данные получены от специального приложения;
- MedExam - Посещение поликлиники (за послений год): кодирование посещений на основе заполненной медицинской карточки;
- Sport - Физические упражнения: '+' пожилой человек занимается спортом (ходьба, бег, плаванье); '-' не занимается.


Более детально посмотрим на информацию о количестве каждого уникального значения для каждого столбца в наборе данных:

```python
feature_names = train_df.columns.tolist() 
for column in feature_names: 
    print (column) 
    print (train_df[column].value_counts(dropna=False))
```
Теперь определим количество не-нулевых значений и определенные библиотекой pandos типы атрибутов:

```pytohn
train_df.info()
print('_'*40)
test_df.info()
```

Итак, теперь мы можем судить о том, какие пробемы могут возникнуть с обработкой датасетов:

- Наличие пустых ячеек;
- Нечисловые значения nan (т.н. нечисла в формате Ч.П.З);
- Категориальные данные;
- Недостоверные/ошибочные значения (LINE, ЗНАЧ и другие). 

Начнем последовательно работать с данными, не изменяя исходных файлов csv.

*Плохим тоном в машинном обучении является изменения исходного датасета. Такой подход ведет к безвозвратной потере части значений, возможно, представляющих интерес для последующих итераций исследования.* **Правильным считается программное изменение загруженного датасета  с помощью подмены значений, фильтров и т.д.**.




В целом, целью анализа данных является ответ на следующие вопросы:
 
- Какие признаки доступны в наборе данных?
- Какие признаки являются категориальными? Являются ли категориальные значения уникальными наименованиями, порядковыми значениями, отношениями или интервалами? Например, атрибуты Sex, MedExam, Education, Longevity, Id, Region,  являются - категориальными, представленными наименованиями категорий. Education, Longevity, Id, Region являются порядковый категориальный признаками.
- Какие функции являются числовыми? Атрибуты Age, Pet, Children, Activity являются числовыми. - 
- Какие числовые признаки являются непрерывными, какие являются дискретными. Непрерывные: Age, Activity. Дискретность: Pet, Children
- Какие признаки являются смешанными типами данных (числовые, буквенно-цифровые данные внутри одной и той же функции)? Такие атрибуты доблжны быть исправлены. Атрибуты MedExam являются буквенно-цифровыми.
- Какие признаки могут содержать ошибки или опечатки? Функции MedExam, Age, Sport содержат ряд нулевых значений.
- Какие типы данных для различных признаков?
- Каково распределение числовых значений признаков по выборкам? Это помогает нам определить, помимо прочего, насколько репрезентативен обучающий набор данных для фактической проблемной области.


**Анализ данных по сводным таблицам (pivot tables)**

Чтобы подтвердить некоторые из наших наблюдений и предположений, мы можем быстро проанализировать наши корреляции признаков, используя сводные таблицы. На этом этапе мы можем сделать это только для функций, которые не имеют пустых значений. Это также имеет смысл делать только для функций, которые являются категориальными (Sex), порядковыми (Education) или дискретными (Pet, Children).

Пример создания сводной таблицы:

```python
train_df[['Education', 'Longevity']].groupby(['Education'], as_index=False).mean().sort_values(by='Longevity', ascending=False)
```

Проведите аналогичные исследования для групп параметров:

- ("Sex", "Longevity")
- ("Pet", "Longevity")
- ("Children", "Longevity")

**Анализ данных путем визуализации**

Теперь мы можем продолжить подтверждать некоторые из наших предположений, используя визуализации для анализа данных.

Давайте начнем с понимания корреляции между числовыми характеристиками и нашей целью решения (Долгожительство).

Гистограммы полезны для анализа непрерывных числовых переменных, таких как возраст. Гистограмма может указывать распределение выборок с использованием автоматически определенных интервалов или полос одинакового диапазона. Обратите внимание, что ось X в визуализациях неподготовленных данных не позволяет отобразить распрееление.

```python
g = sns.FacetGrid(train_df, col='Longevity')
g.map(plt.hist, 'Age', bins=20)
```

Подготовим данные к визуализации:

```python
 #Correct errors in Age column
train_df['Age'].unique()
test_df['Age'].unique()
```

Удалить некорректрное значение можно с помощью функции map и lambda оператора (см. манулаы по Python):

```python
idmax = train_df['Age'].value_counts().idxmax()
train_df['Age'] = train_df['Age'].map(lambda v: idmax if v == 'НЕДОСТОВЕРНОЕ ЗНАЧЕНИЕ' else v).astype(float)
test_df['Age'] = train_df['Age'].map(lambda v: idmax if v == 'НЕДОСТОВЕРНОЕ ЗНАЧЕНИЕ' else v).astype(float)
```


Замена категориальных значений на числовые (int или float) может быть выполнена вот так:

```python
 dataset['КАТЕГОРИЯ'] = dataset['КАТЕГОРИЯ'].map( {'ЗНАЧЕНИЕ#1': 1, 'ЗНАЧЕНИЕ#2': 2, 'ЗНАЧЕНИЕ#3': 3} ).astype(int)```
```

Повторите построение гистограммы уже для числовых (float) значений Age. 

Сделайте выводы.

**Корреляция числовых и порядковых признаков**

Предварительно можно ознакомиться с корреляционной матрицей:

```python
train_df.corr()
```

Мы можем объединить несколько признаков для определения корреляций, используя один график. Это можно сделать с помощью числовых и категориальных функций, которые имеют числовые значения.


```python
grid = sns.FacetGrid(train_df, col='Longevity', row='Education', height=2.2, aspect=1.6)
grid.map(plt.hist, 'Age', alpha=.5, bins=20)
grid.add_legend();
```

Сделайте выводы.


**Корреляция категориальных и числовых признаков**

Нам также понадобится определить корреляцию категориальные характеристики (с нечисловыми значениями) и числовые характеристики. Мы можем рассмотреть корреляцию Sport (категориальное нечисловое значение), Пол (категориальное нечисловое), Активность (числовое непрерывное) с Долгожительством (категориальное числовые)

```python
grid = sns.FacetGrid(train_df, row='Sport', col='Longevity', height=2.2, aspect=1.6)
grid.map(sns.barplot, 'Sex', 'Activity', alpha=.5, ci=None)
grid.add_legend()
```

Сделайте выводы.

**Дополнительные источники литературы по данному разделу:**

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	


### Задание <a name="3_3_3"></a>

1) Признак Age должен быть дополнен и фильтрован для обработки алгоритмами.

2) Признак MedExam может быть отброшен, поскольку является крайне неполным или содержит много нулевых значений как в обучающем, так и в тестовом наборе данных.

3) Необходимо дополнить функцию Sport, поскольку она также может соотноситься с долголетием.

4) Признак Region может быть исключен из нашего анализа, так как содержит большое количество дубликатов (22%) и может отсутствовать корреляция между ним и целевым признаком.

5) Пизнак Id может быть удален из набора обучающих данных, поскольку он не способствует целевому признаку Longevity.

6) Cоздать новый признак под названием  Family (Семья на основе детей и домашних животных), чтобы получить общее количество членов семьи.

7) Заздать новый признак IsAlone, проживающих одиноко без домашних животных и родственников.

7) Cоздать новый признак для возрастных групп (Age,Education), указывающий на социальный статус пожилого человека. Это превращает непрерывный числовой признак в порядковый категориальный признак.

8) Cоздать новый признак диапазонов возрасного равновесия на основе признака Age, т.к. это поможет разделить пожилых людей на группы условно равновесного состояния  (смертность повышается в определенные периоды времени между 70 и 80 годами, и в другие моменты резко снижается) для следующих интервалов: { (...,70](70,72],(72,74],(74,76],(76,78],(80,..)}


**Дополнительные источники литературы по данному разделу:**

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	

- [Введение в pandas: анализ данных на Python](https://khashtamov.com/ru/pandas-introduction/)	




****
# День 3. Алгоритмы машинного обучения <a name="4"></a>


## Алгоритмы машинного обучения <a name="4_3"></a>

После того, как мы предварительно обработали собранные данные и разбили их на три подмножества, мы приступаем к обучению модели. Этот процесс влечет за собой снабжение алгоритма данными обучения. Затем алгоритм обрабатывает данные и выводит модель, которая может найти целевое значение (атрибут) в новых данных (ответ, который мы хотим получить с помощью прогнозного анализа). Целью обучения модели является разработка модели.

Наиболее распространены два модельных стиля обучения - обучение с учителем и обучение без учителя. Выбор каждого стиля зависит от того, должны ли мы прогнозировать конкретные атрибуты или группировать объекты данных по сходству.

• *обучение с учителем*: Контролируемое обучение позволяет обрабатывать данные с целевыми атрибутами или помеченными данными. Обучение с учителем решает проблемы классификации и регрессии.

• *обучение без учителя*: Во время этого стиля обучения алгоритм анализирует немеченые данные. Цель обучения модели - найти скрытые взаимосвязи между объектами данных и объектами структуры по сходствам или различиям. Обучение без учителя направлено на решение таких проблем, как кластеризация, обучение правилам ассоциаций и уменьшение размерности. Например, его можно применять на этапе предварительной обработки данных, чтобы уменьшить сложность данных.

Существует два других стиля обучения модели: *полу-контролируемый*, в котором набор данных содержит как помеченные, так и немаркированные примеры. Второй стиль назвается *обучение с подкреплением*, при котором машина пытается выучить политику действий в соответствии с получением вознаграждения за каждое действие.
Мы опишем алгоритмы, которые являются не только самыми известными, но и либо очень эффективными сами по себе, либо используются в качестве строительных блоков для самых эффективных алгоритмов обучения.


![](assets/ml_03.png)
**Алгоритмы машинного обучения**

**Дополнительные источники литературы по данному разделу:**

- [scikit-learn / Machine Learning in Python](https://scikit-learn.org/stable/)

###  Линейная регрессия<a name="4_3_1"></a>

*Линейная регрессия* –  метод поиска зависимости между входными и выходными переменными с линейной функцией связи. Один из способов вычислить значения параметров модели является метод наименьших квадратов (МНК), который минимизирует среднеквадратичную ошибку между реальным значением зависимой переменной и прогнозом, выданным моделью. 

Пусть мы хотим построить модель объясняемой (зависимой) переменной в виде линейной комбинации остальных (независимых) признаков. Цель алгоритма МНК – построить гиперплоскость, максимально приближенную ко всем обучающим примерам.

<img src="assets/ml_04.png" width="400">
**Линейная регрессия**

На рисунке показана линия регрессии (красным цветом) для одномерных примеров (синие точки). Мы можем использовать эту прямую, чтобы предсказать значение объясняемой переменной *y(new)* для нового значения независимой переменной *x(new)*.

Чтобы получить эту оптимальную линию (или гиперплоскость в n-мерном случае), процедура оптимизации пытается минимизировать следующее выражение *функции стоимости*:

<img src="assets/ml_f2.png" width="200">

Выражение под знаком суммы называется *функцией потерь*. Суммировав этy функции для всех значений признака и разделив на количество значений *N* мы получаем среднюю ошибку при выборе конкретной прямой, которая и является функцией стоимости для линейной регрессии. Параметры *w* и *b* для 2-мерной регрессии определяет наклон прямой и ее смещение (в выражении f(x) = w\*x + b). Для большей размерности *w* и *b* представляют собой вектора, определяющие гиперплоскость. В этом случае функция *f* принимает вид:

<img src="assets/ml_f3.png" width="270">

**Дополнительные источники литературы по данному разделу:**

- [Регрессионный анализ. Кольцов С.Н.](https://www.hse.ru/data/2014/08/29/1313619461/%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F%205.pdf)

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	


###  Деревья решений<a name="4_3_2"></a>

*Деревья решений* - это это способ представления правил в иерархической виде, где каждому объекту соответствует единственный узел, пораждающий решение. Структура дерева представляет собой «листья» и «ветки». На рёбрах дерева решения записаны атрибуты, от которых зависит целевая функция, в «листьях» записаны значения целевой функции, а в остальных узлах - атрибуты, по которым различаются случаи.

На основе деревьев решений строятся многие важные алгоритмы классификации данных и регрессии.  Достоинством таких алгоритмов является высокая наглядность представления и простота интерпретации результатов, что может быть очень важным для предметной области: оказывается возможным не только провести процесс классификации, но и объяснить почему тот или иной объект отнесён к какому-либо классу. 

Идея этого алгоритма довольно проста. Дерево строится «сверху вниз» от корня. Начинается процесс с определения, какой атрибут следует выбрать для проверки в корне дерева. Для этого каждый атрибут исследуется на предмет, как хорошо он классифицирует набор данных (разделяет на классы по целевому атрибуту). При этом выбирается тот из атрибутов, который порождает наибольший количественный критерий оценки. Когда атрибут выбран, для каждого его значения создается ветка дерева, набор данных разделяется в соответствии со значением к каждой ветке, процесс повторяется рекурсивно для каждой ветки. Также следует проверять критерий остановки.

В деревьях классификации часто используются перекрестная энтропия, энтропия Шеннона и коэффициент Джини. В деревьях регрессии минимизируется сумма функций потерь. Мы выполняем эту процедуру рекурсивно для каждого узла и завершаем работу, когда выполняем критерии остановки. 

В качестве критерия остановки могут быть выбраны: минимальное количество уровней дерева от листа до вершины, минимальное значение критерия оценки в узле и пр. 

![](assets/ml_05.png)
**Деревья решений**

**Дополнительные источники литературы по данному разделу:**

- [Деревья решений — общие принципы работы](https://basegroup.ru/community/articles/description)

- [Открытый курс машинного обучения](https://habr.com/ru/company/ods/blog/322534/)

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	



###  Метод опорных векторов<a name="4_3_3"></a>

*Метод опорных векторов* (SVM – support vector machine) – используется в тех слечаях, когда в данных присутствует шум, и результаты применения других регрессионных подходов не удовлетворяют по качеству решения: никакая гиперплоскость не может идеально отделить положительные примеры от отрицательных. 

Алгоритм SVM ищет объекты данных (вектора), которые расположены ближе всего к линии разделения. Эти точки называются опорными векторами. Затем, алгоритм вычисляет расстояние между опорными векторами и разделяющей плоскостью (это расстояние называется зазором). Основная цель алгоритма — максимизировать расстояние зазора. Лучшей гиперплоскостью считается такая гиперплоскость, для которой этот зазор является максимально большим.

<img src="assets/ml_06.png" width="400">
**Метод опорных векторов**

На практике случаи, когда данные можно разделить гиперплоскостью, случаются крайне редко. Если найденная гиперплоскость не позволяет уверенно отделить классы (т.н. *линейная неразделимость*), используется прием увеличения размерности пространства. Действительно, если нам удастся преобразовать исходное пространство в пространство более высокой размерности, мы могли бы надеяться, что примеры станут линейно разделимыми в этом преобразованном пространстве. В SVM используются различные функции для неявного преобразования исходного пространства в пространство более высокого измерения во время оптимизации функции стоимости.

<img src="assets/ml_07.png" width="400">
**Линейная неразделимость**

Например, на приведенной выше картинке можно ввести третью координату *z* (фактически: круг) разделяющую вектора на два класса (внутри круга и снаружи круга).

<img src="assets/ml_f4.png" width="150">

В этом случае классы определяются более точно.

 
**Дополнительные источники литературы по данному разделу:**

- [Лекции по методу опорных векторов. К.В. Воронцов](http://www.ccas.ru/voron/download/SVM.pdf)

- [Классификация данных методом опорных векторов](https://habr.com/ru/post/105220/)

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	


###  Алгоритм k-ближайших соседей<a name="4_3_4"></a>

kNN (k Nearest Neighbor) – алгоритм *k-ближайших соседей* использует весь набор данных в качестве обучающего набора, а не разделяет набор данных на обучающий набор и набор тестов.
Когда для нового экземпляра данных требуется результат, алгоритм KNN просматривает весь набор данных, чтобы найти k-ближайших экземпляров для нового экземпляра, или k экземпляров, наиболее похожих на новую запись, а затем выводит среднее значение результаты (для регрессии) или наиболее близкий класс (для задачи классификации). 

Сходство между экземплярами рассчитывается с использованием таких мер, как *Евклидово расстояние*, *мантхэттенское расстояние*, *расстояние Хемминга* и других.
Это самый четкий метод кластеризации, который все еще имеет некоторые недостатки. Прежде всего, мы должны знать количество кластеров. Во-вторых, результат зависит от точек, случайно выбранных в начале, и алгоритм не гарантирует, что мы достигнем глобального минимума функционала.

Для классификации каждого из объектов тестовой выборки необходимо последовательно выполнить следующие операции:

- Вычислить расстояние до каждого из объектов обучающей выборки

- Отобрать k объектов обучающей выборки, расстояние до которых минимально

- Класс классифицируемого объекта — это класс, наиболее часто встречающийся среди k ближайших соседей


<img src="assets/ml_08.png" width="400">
**Алгоритм k-ближайших соседей**

**Дополнительные источники литературы по данному разделу:**

- [Классификатор kNN](https://habr.com/ru/post/149693/)

- [Метод k-ближайших соседей. Wiki](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_k-%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B8%D1%85_%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9)

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	


## Оценка и тестирование модели <a name="4_4"></a>

Цель этого этапа, разработать простейшую модель, способную быстро и достаточно хорошо сформулировать целевое значение. Эта цель достигнута с помощью модели тюнинга. Это оптимизация параметров модели для достижения максимальной производительности алгоритма.


###  Перекрестная проверка<a name="4_4_1"></a>

Одним из наиболее эффективных методов оценки и настройки модели является перекрестная проверка. *Перекрестная проверка* является наиболее часто используемым методом настройки. Это влечет за собой разделение учебного набора данных на десять равных частей (складок). Данная модель обучается только в девяти сгибах, а затем проверяется на десятой (ранее не учтенной). Тренировка продолжается до тех пор, пока каждая складка не будет оставлена ​​в стороне и использована для тестирования. В результате измерения производительности модели для каждого набора гиперпараметров рассчитывается перекрестная оценка. Модели обучаются с использованием различных наборов гиперпараметров, чтобы определить, какая модель имеет самую высокую точность прогнозирования. Перекрестно подтвержденный балл указывает на среднюю производительность модели по десяти сгибам удержания.
Затем мы тестируем модели с набором значений гиперпараметров, которые получили лучший перекрестно проверенный результат. Существуют различные метрики ошибок для задач машинного обучения.


###  Улучшение прогнозов с помощью методов ансамбля<a name="4_4_2"></a>

Улучшение прогнозов с помощью методов ансамбля – исследователи данных в основном создают и обучают одну или несколько десятков моделей, чтобы иметь возможность выбрать оптимальную модель среди хорошо работающих. Модели обычно показывают разные уровни точности, поскольку они допускают разные ошибки в новых точках данных. Есть способы улучшить аналитические результаты. Методы ансамбля моделей позволяют достичь более точного прогноза, используя несколько наиболее эффективных моделей и комбинируя их результаты. Точность, как правило, рассчитывается по средним ормедианским выходам всех моделей в ансамбле. Среднее значение - это общее количество голосов, поделенное на их количество. Медиана представляет собой средний балл для голосов, упорядоченных по размеру.

Распространенными методами ансамбля являются Stacking, Bagging и Boosting.

*Stacking* –  Этот подход, также известный как многоуровневое обобщение, предлагает разработку метамодели или ученика более высокого уровня путем объединения нескольких базовых моделей. Stacking обычно используются для объединения моделей различных типов. Цель этого метода – уменьшить ошибку обобщения.

*Bagging* –  (начальная загрузка). Это метод последовательного объединения моделей. Сначала обучающий набор данных разбивается на подмножества. Затем модели обучаются на каждом из этих подмножеств. После этого прогнозы объединяются с использованием среднего или большинства голосов. Bagging помогает уменьшить ошибку дисперсии и избежать модели переобучения.

*Boosting* –  Согласно этой методике, работа делится на два этапа. Сначала мы используем подмножества исходного набора данных, чтобы разработать несколько моделей со средней эффективностью, а затем объединяем их, чтобы повысить производительность, используя большинство голосов. Каждая модель обучается на подмножестве, полученном в результате исполнения предыдущей модели, и концентрируется на неправильно классифицированных записях.

Можно развернуть модель, которая наиболее точно прогнозирует значения результатов в тестовых данных.


##  Развертывание модели<a name="4_5"></a>


Развертывание модели (Model deployment) – этап развертывания модели включает в себя ввод модели в эксплуатацию.
После того, как мы выбрали надежную модель и определили ее требования к производительности, мы интегрируем модель с производственной средой.

Затем мы измеряем производительность модели с помощью A / B-тестирования. Тестирование может показать, как, например, число клиентов, работающих с моделью, используемой для персональной рекомендации, соотносится с бизнес-целью.

Рабочий процесс развертывания зависит от бизнес-инфраструктуры и проблемы, которую мы хотим решить. Прогностическая модель может быть ядром новой отдельной программы или может быть включена в существующее программное обеспечение.

Производительность модели также зависит от того, выполнили ли мы вышеупомянутые этапы (подготовка и предварительная обработка набора данных, моделирование) вручную с использованием собственной ИТ-инфраструктуры или автоматически с одним из машинного обучения в качестве сервисных продуктов.


*Пакетный прогноз* – Это вариант развертывания подходит, когда нам не нужны прогнозы на постоянной основе. Когда мы выбираем этот тип развертывания, мы получаем один прогноз для группы наблюдений. Модель обучается на статическом наборе данных и выводит прогноз. Развертывание не требуется, если необходим единый прогноз. Например, мы можем решить проблему классификации, чтобы узнать, принимает ли определенная группа клиентов предложение или нет.

*Веб-сервис* – такой рабочий процесс машинного обучения позволяет получать прогнозы практически в реальном времени. Модель, однако, обрабатывает одну запись из набора данных за раз и делает для нее прогнозы.

*Прогноз в реальном времени* (потоковое в реальном времени) – с помощью потоковой аналитики в реальном времени мы можем мгновенно анализировать потоковые данные в реальном времени и быстро реагировать на события, которые происходят в любой момент. Прогнозирование в реальном времени позволяет обрабатывать данные датчиков или рынка, данные из Интернета вещей или мобильных устройств, а также из мобильных или настольных приложений и веб-сайтов.


## Практическая часть <a name="4_6"></a>

Теперь мы готовы обучить модель и предсказать требуемое решение. Существует более 60 алгоритмов прогнозного моделирования. Мы должны понимать тип проблемы и требования решения, чтобы сузить наш выбор. Во первых важо пнять, что наша проблема - это проблема классификации и регрессии. Мы хотим определить связь между выходом (Longevity) с другими переменными или функциями (Sex, Age, Activity и другими). Во вторых, мы пприменяем обучение с учителем, так как мы обучаем нашу модель заданному размеченному набору данных. Используя эти два подхода (обучение с учителем и классификация/регрессия), мы можем сузить выбор моделей до слудующих:

-    Логистическая регрессия
-    KNN или k-Ближайшие соседи
-    Машина опорных векторов
-    Наивный байесовский классификатор
-    Дерево решений
-    Случайный лес
-    Персептон
-    Искусственная нейронная сеть
-    Метод релевантных векторов

### Проверка данных <a name="4_6_1"></a>

Перед запуском алгоритмов провери наши данные на дефекты для обучающего набора:

```python
train_df.head()         # показывает первые 10 значений датасета
``` 
Отчет об уникальных значениях:

```python
feature_names = train_df.columns.tolist() 
for column in feature_names: 
    print (column) 
    print (train_df[column].value_counts(dropna=False))
```

<img src="assets/ml_09.png" width="400">
**Обучающий набор**



Для тестового набора:

```python
test_df.head()         # показывает первые 10 значений датасета
``` 

Отчет об уникальных значениях:

```python
feature_names = test_df.columns.tolist() 
for column in feature_names: 
    print (column) 
    print (test_df[column].value_counts(dropna=False))
```

<img src="assets/ml_10.png" width="400">
**Тестовый набор**

### Обучение моделей <a name="4_6_2"></a>


Запустим алгоритм логистической регресии для наших данных.

Создадим экземпляр модели логистической регрессии с использованием функции «LogisticRegression» и применим модель к обучающему набору данных с помощью функции `fit`. Методо predict() определяет значения классов для всех переданных в качестве аргументов объектов X_test (т.е., собственно выполняет классификацию).

```python
 # Logistic Regression

logreg = LogisticRegression(solver='liblinear')
logreg.fit(X_train, Y_train)
Y_pred = logreg.predict(X_test)
acc_log = round(logreg.score(X_train, Y_train) * 100, 2)
acc_log
```

Мы можем проверить предположения, принятые нами при подготовке данных. Это можно сделать, рассчитав коэффициенты регрессии. 

```python
coeff_df = pd.DataFrame(train_df.columns.delete(0))
coeff_df.columns = ['Feature']
coeff_df["Correlation"] = pd.Series(logreg.coef_[0])
coeff_df.sort_values(by='Correlation', ascending=False)
```

Положительные значения коэффициентов говорят об увеличении вероятности долголетия при росте соответствующего критерия. Отметим, что регрессия учитывает знаки чисел, поэтому большие отрицательные значения также также указывают на сильное влияние с обратной зависимостью. 

**Дополнительные источники литературы по данному разделу:**

- [Логистическая регрессия в Python: от теории до трейдинга](http://distrland.blogspot.com/2019/05/python.html)

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	


### Задание <a name="4_6_3"></a>


1) Постройте и примените модель Машины опорных векторов (SVC и LinearSVC).

2) Постройте и примените модель k-Ближайших соседей (KNeighborsClassifier).

3) Постройте и примените модель Наивного байесовского классификатора (GaussianNB).

4) Постройте и примените модель Персептрона (Perceptron).

5) Постройте и примените классификатор на основе метода стохастического градиентного спуска (SGDClassifier)

6) Примените Дерево решений (DecisionTreeClassifier)

7) Примените классификатор на основе случайного леса деревьев (RandomForestClassifier)


### Оценка моделей <a name="4_6_4"></a>

Теперь мы можем оценить наши модели и выбрать лучшую для классификации по выбранному нами критерию  долгожительства Longevity. 

```python
models = pd.DataFrame({
    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 
              'Random Forest', 'Naive Bayes', 'Perceptron', 
              'Stochastic Gradient Decent', 'Linear SVC', 
              'Decision Tree'],
    'Score': [acc_svc, acc_knn, acc_log, 
              acc_random_forest, acc_gaussian, acc_perceptron, 
              acc_sgd, acc_linear_svc, acc_decision_tree]})
models.sort_values(by='Score', ascending=False)
```


****
# День 4. Распознавание образов и компьютерное зрение <a name="5"></a>

Большая часть информации медицинского характера накапливается в графическом виде: снимки УЗИ, рентгеновские снимки, снимки эндоскопов и пр. Поэтому не менее важно использовать средств ИИ для распознавания образов на графической информации.

В этом задании мы рассмотрим средства и алгоритмы компьютерного зрения и решим тестовую задачу распозавания объектов на серии фотографий с использованием библиотек `Tensorflow` и `Keras`. 

## Нейронная сеть <a name="5_1"></a>

Нейронной сетью называется математическая модель, реализующая фукнции искусственного интеллекта путём воспроизведения нервной системы человека. Они используются для решения сложных задач, которые требуют аналитических вычислений, подобных тем, что делает человеческий мозг. К таким задачам относятся, например, классификация, кластеризация, прогнозирование, распознавание и т.д.

Искусственный нейрон представляет собой сумматор входных сигналов, применяющий к полученной взвешенной сумме некоторую простую функцию. Нейрон имеет синапсы - однонаправленные входные связи, соединённые с выходами других нейронов, а также аксон - выходную связь.
![Схема искусственного нейрона](assets/artificial-neuron.PNG)
 
Текущее состояние нейрона определяется взвешенной суммой его входов (см. схему). Выход нейрона определяется его активационной функцией.

Совокупность нейронов, расположенных на одном уровне в нейронной сети, называется слоем. В общем случае нейронная сеть включает в себя входной, выходной и промежуточные слои. Нейроны входного и выходного слоёв, как правило, имеют линейную функцию активации и предназначены для приёма и передачи данных. Нейроны промежуточных слоёв - нелинейные; их функцией активации чаще всего является сигмоид (логистическая функция):
![equation](https://latex.codecogs.com/png.latex?f%28x%29%20%3D%20%5Cfrac%7B1%7D%7B1%20&plus;%20e%5E%7B-%5Calpha%20x%7D%7D)

На схеме показан пример полносвязной нейронной сети, имеющей входной, промежуточный и выходной слои.
![Схема нейронной сети](assets/neuro_scheme.png)

Нейронная сеть обучаема. В процессе обучения параметры сети настраиваются в соответствии с обучающими наборами данных, моделирующих среду, в которой будет функционировать сеть. В зависимости от способа подстройки параметров различают обучение с учителем и без учителя.

Обучение с учителем представляет собой предъявление сети выборки обучающих примеров. Каждый образец подаётся на входы сети, проходит обработку и перерабатывается в выходной сигнал, который сравнивается с эталонным значением. Затем в зависимости от степени расхождения реального и идеального результатов изменяются весовые коэффициенты связей внутри сети. Обучение длится до тех пор, пока ошибка по всему обучающему массиву не достигнет приемлемо низкого уровня.

При обучении без учителя обучающее множество состоит лишь из входных векторов. Алгоритм обучения подстраивает веса внутри сети так, чтобы предъявление достаточно близких входных векторов давало одинаковые результаты.

Почитать подробнее про нейронные сети можно [здесь](http://www.aiportal.ru/articles/neural-networks "Статьи о нейронных сетях").



## Серточны нейронные сети <a name="5_2"></a>

*Свёрточная нейронная сеть* — специальная архитектура искусственных нейронных сетей, основанные на представлении изображений в виде тензоров и нацеленная на эффективное распознавание образов в графических изображениях. Тензоры — это 3-х мерные массивы.

Для распознавания образов могут использоваться и простые модели нейронных сетей, такие как  многослойный персептрон. Однако, если размеры изображения велики, то число и сложность слоев нейронной сети многократно увеличивается, а процесс обучения существенно усложняется. Другим недостатком многослойных нейронных сетей является векторный характер представления данных, что делает невозможный двумерную локализации пикселей и обработку деталей на изображении.

Как и полносвязная нейронная сеть, свёрточная сеть обучается с помощью алгоритма обратного распространения ошибки. Сначала выполняется прямое распространение от первого слоя к последнему, после чего вычисляется ошибка на выходном слое и распространяется обратно. При этом на каждом слое вычисляются градиенты обучаемых параметров, которые в конце обратного распространения используются для обновления весов с помощью градиентного спуска.

Свёрточные нейронные сети состоят из последовательно соединенных слоев нескольких типов, выполняющими преобразования над поступающей матрицей или несколькими матрицами (например, исходное изображение представляется тремя матрицами R,G и B компонент цветности). Обычно, свёрточные НС построены на чередовании свёрточных слоёв, реализующих функцию свёртки, и субдискретизирующих слоёв, ответственных за выборку наиболее подходящего раздражителя — и тем самым уменьшающих размер обрабатываемого изображения. Также в сверточных НС используются элементы полносвязных персептронов: слои активации и полносвязные слои. Из слоев различных типов можно конструировать НС, наиболее подходящие для каждой конкретной задачи. 

## Библиотека Tensorflow <a name="5_3"></a>

*TensorFlow*  — это фреймворк машинного обучения, предоставляющий разработчикам функционал для сбора данных, а также для построения и обучения моделей, основанных на нейронных сетях. 
TensorFlow позволяет разработчикам создавать специальные обрабатывающие структуры - графы потоков данных, которые описывают, как данные перемещаются через последовательность узлов обработки. Каждый узел в графе представляет математическую операцию, а каждое соединение между узлами представляет собой многомерный массив данных или тензор.

TensorFlow использует синтаксис языка Python, т.к. он прост в освоении и предоставляет удобные способы выражения алгоритмов действий в виде высокоуровневых абстракций. Узлы и тензоры в TensorFlow являются объектами Python, а приложения TensorFlow являются Python-приложениями. Однако, для повышения производительности обработки данных, TensorFlow написан на языке C++ и скомпилирован в бинарные файлы. Python используется лишь для упрощения синтаксических конструкций и использования высокоуровневых программных абстракций.

Приложения TensorFlow можно запускать как на локальном компьютере, так и в облачном кластере, на устройствах iOS и Android, на мобильных процессорах, встраиваемых системах типа RaspberryPi или графических процессорах. Полученные в результате исследований модели TensorFlow могут быть развернуты на любом устройстве, где они будут использоваться для рапознавания и формирования прогнозов.

## Пример использования Tensorflow и Keras<a name="5_4"></a>

Мы разработаем программный код для распознавания объектов на фотографиях. Будем использовать небольшую обучающую выборку задачи распознавания :

<img src="assets/cv_01.jpg" width="500">
**Классика жанра**

Загрузите архив с графическими файлами на сервер:

```shell
wget https://github.com/alexbmstu/2019/raw/master/data/chihuahua-muffin.zip
unzip chihuahua-muffin.zip
cd chihuahua-muffin
```

Вы можете использовать терминал из Jupyter Notebooks.

![](assets/cv_02.png)
**Терминал Jupyter Notebooks**

или перенесите файлы любым другим способом:

- [Изображения](data/chihuahua-muffin.zip)

Подключим необходимые библиотеки Tensorflow и keras

```python
 # TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras
 #import keras.utils
from keras import utils as np_utils
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.optimizers import SGD

 # Helper libraries
import numpy as np
import matplotlib.pyplot as plt
import glob, os
import re

 # Pillow
import PIL
from PIL import Image
```

Цветность изоражения необходимо преобразовать в 8-бит Grayscale. Также приведем все изображения к одинаковому размеру 100х100 пикселей.

```python
 # Use Pillow library to convert an input jpeg to a 8 bit grey scale image array for processing.
def jpeg_to_8_bit_greyscale(path, maxsize):
        img = Image.open(path).convert('L')   # convert image to 8-bit grayscale
        # Make aspect ratio as 1:1, by applying image crop.
    # Please note, croping works for this data set, but in general one
    # needs to locate the subject and then crop or scale accordingly.
        WIDTH, HEIGHT = img.size
        if WIDTH != HEIGHT:
                m_min_d = min(WIDTH, HEIGHT)
                img = img.crop((0, 0, m_min_d, m_min_d))
        # Scale the image to the requested maxsize by Anti-alias sampling.
        img.thumbnail(maxsize, PIL.Image.ANTIALIAS)
        return np.asarray(img)
    
def load_image_dataset(path_dir, maxsize):
        images = []
        labels = []
        os.chdir(path_dir)
        for file in glob.glob("*.jpg"):
                img = jpeg_to_8_bit_greyscale(file, maxsize)
                if re.match('chihuahua.*', file):
                        images.append(img)
                        labels.append(0)
                elif re.match('muffin.*', file):
                        images.append(img)
                        labels.append(1)
        return (np.asarray(images), np.asarray(labels))
```

Загрузим изображения в датасеты

```python
maxsize_w = 100
maxsize_h = 100

maxsize = maxsize_w, maxsize_h

(train_images, train_labels) = load_image_dataset('путь к директории train_set', maxsize)

(test_images, test_labels) = load_image_dataset('путь к директории test_set', maxsize)
```

Посмотрим на параметры обучающей выборки изображений:

```python
train_images.shape
```

Набор меток в обучающей выборке:

```python
print(test_labels)
[0 0 0 1 1 1 0 1 0 1 1 0 0 1]
```

Нам монадобится функция вывода таблицы изображений:

```python
class_names = ['chihuahua', 'muffin']
def display_images(images, labels):
        plt.figure(figsize=(10,10))
        grid_size = min(25, len(images))
        for i in range(grid_size):
                plt.subplot(5, 5, i+1)
                plt.xticks([])
                plt.yticks([])
                plt.grid(False)
                plt.imshow(images[i], cmap=plt.cm.binary)
                plt.xlabel(class_names[labels[i]])
```

Выведем обучающую выборку:

```python
display_images(train_images, train_labels)
plt.show()
```

Создадим и обучим модель `first_model`:

```python
train_images = train_images / 255.0
test_images = test_images / 255.0

 # Setting up the layers.

first_model = keras.Sequential([
    keras.layers.Flatten(input_shape=(100, 100)),
        keras.layers.Dense(128, activation=tf.nn.sigmoid),
        keras.layers.Dense(16, activation=tf.nn.sigmoid),
    keras.layers.Dense(2, activation=tf.nn.softmax)
])
sgd = keras.optimizers.SGD(lr=0.01, decay=1e-5, momentum=0.7, nesterov=True)

first_model.compile(optimizer=sgd,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

first_model_history=first_model.fit(train_images, train_labels, epochs=100)
```

Оценим точность модели:

```python
test_loss, test_acc = model.evaluate(test_images, test_labels)
print('Test accuracy:', test_acc)
```

Сформируем метки для изображений тестовой выборки по полученной модели и выведем то, что получилось:

```python
predictions = model.predict(test_images)
display_images(test_images, np.argmax(predictions, axis = 1))
plt.show()
```

Создайте альтернативную модель second_model и используйте функцию `fit` для ее обучения:

```python
second_model_history=second_model.fit(train_images, train_labels, epochs=100)
```

Сравните модели 

```python
def plot_history(histories, key='accuracy'):
  plt.figure(figsize=(16,10))

  for name, history in histories:
    val = plt.plot(history.epoch, history.history[key],
                   '--', label=name.title()+' Val')
    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),
             label=name.title()+' Train')

  plt.xlabel('Epochs')
  plt.ylabel(key.replace('_',' ').title())
  plt.legend()

  plt.xlim([0,max(history.epoch)])
```

```python
plot_history([('Первая модель', first_model_history),
              ('Вторая модель', second_model_history)])
```

![](assets/cv_03.png)
**Сравнение истории обучения моделей**


## Задание <a name="5_5"></a>

1) Разработайте 4-6 различных моделей, меняя количество эпох обучения и свойства изображения.

2) Добейтесь точности распознавания >=92%.



**Дополнительные источники литературы по данному разделу:**


- [Basic classification: Classify images of clothing](https://www.tensorflow.org/tutorials/keras/classification)

- [Что такое Keras](https://neurohive.io/ru/tutorial/nejronnaya-set-keras-python/)

- [List of datasets for machine-learning research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research)

- [Как работает сверточная нейронная сеть](https://neurohive.io/ru/osnovy-data-science/glubokaya-svertochnaja-nejronnaja-set/)

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	

****
# День 5. Облачная платформа IBM Cloud и платформа ИИ Watson Studio <a name="6"></a>


В этом задании мы рассмотрим средства машинного обучения и ИИ, развернутые на облачной платформе IBM Cloud. 

Для вычислений мы будем использовать платформу IBM Watson Studio, доступную студентам и преподаватеям МГТУ по академической программе IBM.

Набор средств машинного обучения и анализа данных Watson Studio очень широк и позволяет решать задачи ИИ, в том числе с использованием распределенных вычислений и GPGPU.

Для знакомства с имеющимися средства машинного обучения и анализа данных следует ознакомиться с инструкцией: [Choosing a tool in Watson Studio](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/tools.html?audience=wdp&context=wdp)


## Регистрация в IBM Cloud и получение студенческого промо-кода <a name="6_1"></a>

Для доступа к ресурсам IBM Watson Studio необходимо:

- Получить почтовый адрес из домена `@student.bmstu.ru` или `@bmstu.ru`. Инструкция по получению почтового адрес из домена `@student.bmstu.ru` находится [тут](https://mail.bmstu.ru/~postmaster/mail_for_students_and_aspirants.pdf).

- Пройти регистрацию на облачной платформе IBM Cloud, используя почтовый адрес `@student.bmstu.ru` или `@bmstu.ru`.

- Пройти регистрацию в программе Академической инициативе по адресу [ibm.biz/academic](ibm.biz/academic)

- Получить бесплатный студенческий промо-код, дающий возможность использовать расширенные ресурсы IBM Cloud в течении 6 месяцев.

- Зарегистрировать промо код в вашем аккоунте IBM Cloud (инструкция доступна на странице заказа промо-кода). 


Доступ к платформе осуществляется по адресу [cloud.ibm.com](https://cloud.ibm.com/login).


![Облачная платформа PaaS *IBM Cloud*](assets/IBMcloud_01.png)
**Облачная платформа PaaS IBM Cloud**



**Дополнительные источники литературы по данному разделу:**

- [Описание ресурсов IBM Cloud](https://www.ibm.com/ru-ru/cloud)
- [Introduction to IBM Watson Studio](https://developer.ibm.com/articles/introduction-watson-studio/)

## Облачная платформа *IBM Cloud* <a name="6_2"></a>

***IBM Cloud*** — это открытое облачная система типа PaaS
(*Platform-as-a-Service*) на базе проекта с открытым исходным кодом Cloud Foundry. Эта платформа предназначена для разработки и хостинга приложений, а также упрощения задач по управлению инфраструктурой. Она позволяет быстро создавать и развертывать приложения, а также управлять ими.

***IBM Cloud*** обеспечивает следующие возможности:

-   быстрое и инкрементное составление приложений из сервисов;
-   непрерывное внесение изменений в приложения и обеспечение постоянной доступности;
-   поддержка высокоспециализированных моделей программирования и сервисов для конкретных рабочих нагрузок;
-   встраивание высокой степени управляемости в сервисы и приложения;
-   оптимизация и эластичная адаптация к рабочей нагрузке.


![Каталог компонентов IBM Cloud](assets/IBMcloud_02.png)
**Каталог компонентов IBM Cloud**


Платформа *IBM Cloud* достигает этих целей посредством абстрагирования и скрытия большинства сложностей, традиционно сопутствующих хостингу приложений в облаке и управлению ими в облачной среде. *IBM Cloud* может быть использована разработчиками для создания и применения самых разных приложений, включая веб-приложения, мобильные приложения, приложения для работы с большими данными, приложения для разумных устройств и т.д. *IBM Cloud* поддерживает разработку на популярных языках программирования и средах разработки. Java-технологии, средства создания серверных частей для мобильных приложений, мониторинг приложений, технологии с открытым исходным кодом и т. д. — все эти возможности
доступны в облаке как сервисы.

Каталог *IBM Cloud* содержит большую часть из того, что необходимо для быстрого начала работы, большое количество шаблонов, заранее сконфигурированны наборов сервисов, сред исполнения и примеров кода, готовых к использованию:

-   инструментов и библиотек обработки данный: R Studion, Jupyter Notebooks, Streams flow editor, SPSS Modeler, Spark MLlib modeler, Decision Optimization model builder и других;
-   сред исполнения, в том числе: Liberty for Java, Node.js, Ruby on Rails;
-   веб-сервисов и сервисов приложений, в том числе: Data/Session Cache,  ElasticMQ, Decision, SSO, Log Analysis, Redis, RabbitMQ, Twilio;
-   мобильных сервисов, в том числе: push-уведомлений, Cloud Code,     Mobile Application Management, Mobile Quality Assurance;
-   сервисов управления данными, в том числе: MongoDB, реляционной базы данных от IBM, JSON-базы данных от IBM, MySQL, PostgreSQL, MobileData, Mobile Sync, BLU Data Warehouse, MapReduce;
-   сервисов мониторинга и анализа;
-   сервисов DevOps Services (прежнее название: JazzHub).
-   проприетарные сервисы IBM, включая аналитическую систему SPSS и другие. 
-   когнитивные сервисы IBM Watson.

![Компоненты IBM Cloud для анализа данных и ИИ](assets/IBMcloud_03.png)
**Компоненты IBM Cloud для анализа данных и ИИ**


****
### Краткое описание концепций *IBM Cloud* <a name="6_2_1"></a>


В терминологии *IBM Cloud* приложение (*application*) — это созданный вами артефакт, т. е. весь программный код (исходный код или исполняемые двоичные файлы), который необходимо запустить или на который необходимо сослаться в процессе исполнения. Мобильные приложения выполняются за пределами среды *IBM Cloud* и используют сервисы *IBM Cloud*, представленные приложениями. В случае веб-приложений приложение — это код, загруженный на платформу *IBM Cloud* с целью хостинга. Кроме того, платформа *IBM Cloud* способна осуществлять хостинг программного кода приложения, который вы хотите выполнять на внутреннем сервере в среде на базе контейнера.

На рисунке показаны принципы взаимодействия *IBM Cloud* с клинтскими приложениями.


![Принципы взаимодействия *IBM Cloud* с клинтскими приложениями](assets/intro05.jpg)
**Принципы взаимодействия *IBM Cloud* с клинтскими приложениями**


***Сервис (service)*** — это код, работающий на платформе *IBM Cloud* и предлагающий некоторую функциональность, которую могут использовать приложения. Это может быть готовый сервис, используемый непосредственно — например, push-уведомления для мобильных приложений или эластичное кэширование для веб-приложения. Вы также можете создавать собственные сервисы в диапазоне от простых служебных функций до сложной
бизнес-логики.

***Организация (organization) и пространство (space)*** — это организационные единицы инфраструктуры, способные хранить и отслеживать ресурсы приложения. Организация содержит домены (domain), пространства и пользователей. Пространство содержит приложения и сервисы. По умолчанию используется три пространства: Development (разработка), Production (производство) и Staging (подготовка). Для приложений, которым требуется среда типа PaaS, предоставляются buildpack-пакеты, каждый из которых представляет собой набор скриптов для подготовки кода к исполнению на целевой PaaS-платформе. Buildpack-пакеты, которые включают необходимую вашим приложениям среду исполнения и могут также содержать специализированные инфраструктуры, упрощают развертывание приложения в облаке по сравнению с самостоятельной установкой и конфигурированием среды исполнения.

Использование сервисов в *IBM Cloud* включает три этапа:
1.  Сообщите платформе *IBM Cloud*, что вам требуется новый экземпляр сервиса и какое конкретное приложение будет использовать этот новый экземпляр.
2.  *IBM Cloud* автоматически инициализирует новый экземпляр этого сервиса и свяжет его с приложением.
3.  Приложение взаимодействует с сервисом.

***Пакеты сервисов (Service bundles)*** — это коллекции API-интерфейсов, используемых в конкретных областях. Например, пакет Mobile Services включает сервисы MobileData, Cloud Code, Push и Mobile Application Management. Доступные сервисы и среды исполнения представлены в каталоге IBM Cloud. Кроме того, вы можете зарегистрировать собственные сервисы.

****
### Развертывание и управление приложением <a name="6_2_2"></a>

Чтобы развернуть свое приложение, необходимо загрузить его в среду *IBM Cloud* и указать, сколько экземпляров этого приложения должно исполняться, а затем сконфигурировать *IBM Cloud*, введя необходимую информацию для поддержки этого приложения.

В случае мобильного приложения среда *IBM Cloud* содержит артефакт, который представляет серверную часть мобильного приложения — набор сервисов, который использует приложение для взаимодействия с сервером. *IBM Cloud* поддерживает серверные компоненты мобильного приложения, взаимодействующие с сервисами PushWorks, Cloud Code и Mobile Data, непосредственно из пользовательского интерфейса *IBM Cloud*.

В случае веб-приложения необходимо предоставить в *IBM Cloud* соответствующую информацию о среде исполнения и среде разработки, чтобы платформа смогла сформировать надлежащую инфраструктуру для исполнения этого приложения.

При развертывании приложений и управлении ими можно использовать инструмент командной строки cf, веб-интерфейс *IBM Cloud* или сервисы DevOps Services.

Браузерные и мобильные клиенты — а также другие приложения, развернутые на платформе *IBM Cloud* и выполняющиеся за ее пределами — взаимодействуют с приложениями, работающими на платформе *IBM Cloud*, через API-интерфейсы типа REST/HTTP. Каждый клиентский запрос маршрутизируется к одному из экземпляров приложения или составляющих его сервисов. Среды исполнения приложений в *IBM Cloud* изолированы друг от друга даже тогда, когда они находятся на одной и той же физической машине.

В ходе управления приложениями можно запускать, останавливать, перезапускать экземпляры приложения (или, в случае веб-приложения, изменять их количество), а также изменять объем памяти, используемый приложением. Ключевая конструктивная особенность *IBM Cloud* — отличные показатели при хостинге масштабируемых приложений и артефактов приложений. На данный момент эта платформа не масштабирует приложение автоматически в соответствии с нагрузкой, поэтому этим процессом необходимо управлять самостоятельно посредством создания или удаления экземпляров при изменении рабочей нагрузки. По этой причине ваши приложения должны сохранять все персистентные данные за пределами приложения в одном из сервисов хранения данных, предоставляемых платформой *IBM Cloud*. При повторном развертывании приложения после обновления используется тот же процесс, что и при начальном развертывании. *IBM Cloud* останавливает все исполняющиеся экземпляры и переводит новые экземпляры в рабочее состояние автоматически.


****
# Дополнительные источники <a name="a001"></a>

<a name="pub1">[1]</a> [Yuji Roh. A Survey on Data Collection for Machine Learning: a Big Data - AI Integration Perspective](https://arxiv.org/abs/1811.03402)

<a name="pub2">[2]</a> [Behera, Rabi. A Survey on Machine Learning: Concept, Algorithms and Applications](https://www.researchgate.net/publication/316273553_A_Survey_on_Machine_Learning_Concept_Algorithms_and_Applications)


<a name="pub3">[3]</a> [scikit-learn / Machine Learning in Python](https://scikit-learn.org/stable/)

<a name="pub4">[4]</a> [Learn Data Science / Open content for self-directed learning in data science](http://learnds.com/)

<a name="pub5">[5]</a> [ML Boot Camp / Руководство для начинающих](https://mlbootcamp.ru/article/tutorial/)
