****
# Введение <a name="1"></a>

Благодаря достижениям в области искусственного интеллекта в последние годы открываются новые области применения методов и алгоритмов машинного обучения. В то время как проекты машинного обучения различаются по размеру и сложности, требуя различных навыков работы с данными, их общая структура как правило одинакова. Можно констатировать, что для получения хороших результатов анализа, важно иметь хорошо подобранные данные и команду экспертов, обладающую необходимыми навыками для управления проектом машинного обучения. В этом исследовании мы будем следовать конвейеру машинного обучения, состоящему из шести ступеней.

![Последовательность этапов конвейера машинного обучения](assets/ml_pipeline.png)
**Последовательность этапов конвейера машинного обучения**

Далее мы кратко опишем цели указанных этапов и алгоритмы, применяемые для их реализации.

## Алгоритмы предварительной обработки данных <a name="1_1"></a>

Предварительная обработка данных - это метод анализа данных, который включает преобразование необработанных данных в понятный формат. Реальные данные часто являются неполными, непоследовательными и/или недостающими в определенных видах поведения или тенденциях, могут содержать много ошибок. Предварительная обработка данных является проверенным методом решения таких проблем. Поскольку это важно для успеха любого процесса машинного обучения, мы дадим краткое описание основных методов предварительной обработки данных, включая:

1) обработку пропущенных значений;

2) преобразование признаков, нормализацию и стандартизацию;

3) уменьшение размерности;

4) выбор признаков;

5) разделение данных.

Полученные от первоисточника числовые данные могут быть представлены в различной форме. Поэтому, сначала выполняется очистка данных, которая включает в себя обработку пропущенных значений и отбрасывание всех данных, которые имеют недостоверные значения или могут быть классифицированы как т.н. выбросы. Так как такой процесс требует глубокого знания предметной области, он должен выполняться совместно с прикладными специалистами. Например, искаженные значения могут быть вызваны сбоем в работе механизма, который сгенерировал набор данных, или лицом, ответственным за запись данных. Только специалисты с высоким уровнем знаний в предметной области могут определить, какие значения следует опускать, а какие показывают целесообразное измерение. Тщательный анализ первичных данных должен сопровождаться ведением протокола, отражающего внесенные изменения. Это может оказаться существенным при дальнейшем понимании результатов машинного обучения. Пропущенные значения обычно заменяются некоторыми правдоподобными значениями, такими как среднее или наиболее частое значение столбца.

Далее принято производить масштабирование данных для каждого из призаков, чтобы поместить весь набор данных в один общий интервал. Очень важно, чтобы диапазон всех атрибутов был нормализован, чтобы каждый атрибут вносил одинаковый вклад в конечный результат [[1]](https://arxiv.org/abs/1811.03402). Тем не менее, это действие не должно влиять на тип категориальных атрибутов. Некоторыми популярными методами масштабирования являются: нормализация мин-макс, нормализация среднего значения, логарифмическое преобразование и стандартизация атрибутов.

После выполняется разработка признаков. Разработка признаков в основном означает сохранение наиболее важных признаков для достижения желаемой цели. Это можно сделать извлекая новые признаки из имеющихся в настоящее время признаки или удаляя признаки, которые не влияют на результат или добавляют шум, что приводит к снижению точности результатов.

Последний этап предварительной обработки данных, это разделение доступных данных на две части. Первая часть данных используется для создания модели (обучающая выборка), а другая - для оценки качества модели (тестовая выборка).

## Алгоритмы машинного обучения <a name="1_2"></a>

Выбор правильного алгоритма является ключевой частью любого проекта по машинному обучению, и, поскольку есть десятки вариантов на выбор, важно понять их сильные и слабые стороны в различных бизнес-приложениях [[2]](https://www.researchgate.net/publication/316273553_A_Survey_on_Machine_Learning_Concept_Algorithms_and_Applications). В этом курсе мы поговорим об алгоритмах в двух видах обучения: 

- Обучение с учителем, 
   
- Обучение без учителя.


### Обучение с учителем <a name="1_2_1"></a>

Алгоритмы обучения с учителем (контролируемого обучения) строят математическую модель набора данных, который содержит как входы, так и желаемые результаты. Этот набор известен как данные обучения и состоит из обучающих примеров. Каждый обучающий пример имеет один или несколько входов и желаемый выход, также известный как контрольный сигнал. В случае полуобучаемых алгоритмов, обучения в некоторых учебных примерах отсутствует желаемый результат. В математической модели каждый обучающий пример представлен массивом или вектором, а обучающие данные матрицей. Посредством итеративной оптимизации целевой функции, алгоритмы обучения с учителем изучают функцию, которая может использоваться для прогнозирования результатов, связанных с новыми входными данными. Оптимальная функция позволит алгоритму правильно определять выходные данные для входов, которые не были частью обучающих данных. 

Говорят, что алгоритм, улучшающий точность результатов или прогнозов с течением времени, научился выполнять эту задачу. Алгоритмы обучения с учителем включают классификацию и регрессию. Алгоритмы классификации используются, когда выходные данные ограничены ограниченным набором значений, а алгоритмы регрессии используются, когда выходные данные могут иметь любое числовое значение в пределах диапазона. Обучение сходству является областью контролируемого машинного обучения, тесно связанной с регрессией и классификацией, но цель состоит в том, чтобы учиться на примерах с использованием функции сходства, которая измеряет, насколько похожи или связаны два объекта. Оно имеет приложения для ранжирования, системы рекомендаций, визуального отслеживания личности, проверки лица и проверки докладчика.

### Обучение без учителя <a name="1_2_2"></a>

Алгоритмы обучения без учителя берут набор данных, который содержит только входные данные, и находят структуру данных, такую ​​как группировка или кластеризация точек данных. Алгоритмы поэтому учатся на тестовых данных, которые не были помечены, классифицированы или разделены на кластеры. Вместо того, чтобы реагировать на обратную связь, неконтролируемые алгоритмы обучения выявляют общие черты в данных и реагируют на основании наличия или отсутствия таких общих черт в каждом новом фрагменте данных. 

*Кластерный анализ* - это распределение набора наблюдений в подмножества (называемые кластерами), чтобы наблюдения в пределах одного кластера были похожи в соответствии с одним или несколькими заранее определенными критериями, в то время как наблюдения, полученные из разных кластеров, отличаются. Различные методы кластеризации делают разные предположения о структуре данных, часто определяемые некоторой метрикой сходства и оцениваемые, например, по внутренней компактности или сходству между членами одного кластера и различию - разнице между кластерами. Другие методы основаны на оценке плотности и связности графа.



****
## Типовая вычислительная инфраструктура для машинного обучения  <a name="1_3"></a>

В ходе выполнения конкурсных заданий и самостоятельной разработки конвейера машинного обучения мы предлагаем каждой команде развернуть набор средств для обработки тестовых данных.

Примером подобной системы является структура, представленная на следующем рисунке.

![](assets/hackathon2019.png)

Для выполнения заданий хакатона каждой команде предоставляется следующие вычислительные ресурсы:

- Пул ресурсов в ЦОД МГТУ им Баумана: 8 ГБ ОЗУ, 4 ядра микропроцессора 2400 МГц, 50 ГБ дискового пространства.
- Предоставляется ip адрес в DMZ МГТУ с открытыми портами.
- Предоставляется Локальная сеть в среде виртуализации VMWare VSphere.
- Допускается объединять ресурсы команд в кластеры (например, )
- Дополнительно команды могут использовать ресурсы облачной платформы `IBM Cloud`, которая может использоваться для реализации сервисов аналитической обработки и визуализации данных.  Для получения доступ к ресурсам IBM, необходимо использовать почтовый адрес из домена `student.bmstu.ru` или `bmstu.ru` при регистрации в программе Академической инициативе по адресу [ibm.biz/academic](ibm.biz/academic). Инструкция по получению почтового адрес из домена `student.bmstu.ru` находится [тут](https://mail.bmstu.ru/~postmaster/mail_for_students_and_aspirants.pdf).
 


****
## Проект хакатона <a name="1_4"></a>

Всем командам предлагается собрать template-проект, который может быть модифицирован командами для реализации собственных идей.
В проекте использовано следующие технологии:

-  Платформой виртуализации VMware vSphere 6.5
-  ОС Linux (Debian/Ubuntu/CentOS)
-  Anaconda/Jupyter Notebooks
-  Язык Python 3
-  IBM Cloud / Watson Studio
 

****
## Оборудование и настройка компьютера разработчика <a name="1_5"></a>

Участник хакатона может выполнить все задания на компьютере в аудитории, однако рекомендуется использовать собственный ноутбук. Желательно использование операционной системы Linux (Ubuntu, Arch, CentOS, RHEL) или MacBook. В случае использования компьютера под управлением OC Windows рекомендуется воспользоваться готовым образом виртуальной машины Oracle Virtual Box, который можно найти на портале: [https://www.osboxes.org/ubuntu/](https://www.osboxes.org/ubuntu/).

****
# День 1. Управление виртуальным ЦОД <a name="2"></a>

Понятие виртуализации (от Virtual - действительный, лат.) в широком смысле представляет собой сокрытие настоящей реализации какого-либо объекта или процесса от истинного его представления для пользователя. Результатом виртуализации является нечто удобное для использования, на самом деле, имеющее более сложную или совсем иную структуру, отличную от той, которая воспринимается при работе с объектом. Происходит отделение представления от реализации. В компьютерных технологиях под термином «виртуализация» обычно понимается абстракция вычислительных ресурсов (чаще всего серверов) и предоставление пользователю системы, которая «инкапсулирует» (скрывает в себе) собственную реализацию. Проще говоря, пользователь работает с удобным для себя представлением объекта, и для него не имеет значения, как объект устроен в действительности.

![Виртуальные машины, работающие на физическом оборудовании](assets/virtualisation.png)
**Виртуальные машины, работающие на физическом оборудовании**

Долгое время в центрах обработки данных стремились к децентрализованной архитектуре, масштабированию приложений и системной инфраструктуры в горизонтальном направлении. Эта тенденция, как правило, приводила к росту числа серверов. Со временем инфраструктура ЦОД становилась все более сложной, что сказывалось на возможности его модернизации, обслуживании, стоимости и пр. Централизованная архитектура может уменьшить количество серверов и увеличить степень использования оборудования при сравнимой производительности, а также уменьшить стоимость владения оборудованием. 

`Частичная виртуализация` (нативная виртуализация) базируется на принципе эмуляции только необходимого количества ресурсов, чтобы виртуальная машина могла быть запущена изолировано. Напротив, при `полной эмуляции` различных архитектур, гостевая система работает с определенной специфической системой команд процессора, отличной от системы команд процессора хостовой системы. Каждую команду процессору гостевой системы нужно транслировать в соответствующую команду хостовой системы, что невероятно уменьшает быстродействие.

При использовании нативной виртуализации никакой трансляции команд не происходит, так как гостевая операционная система разработана под ту же архитектуру, на которой работает хостовая система. Это позволяет значительно повысить быстродействие гостевой системы и максимально приблизить его к быстродействию реальной системы.

Для повышения быстродействия нативной виртуализации применяется специализированная программная прослойка – `гипервизор`. Гипервизор является посредником между гостевой операционной системой и физическим аппаратным обеспечением. Он позволяет гостевой системе напрямую обращаться к аппаратным ресурсам, что и является секретом высокого быстродействия данного вида виртуализации. Гипервизор является одним из ключевых понятий в мире виртуализации.

Частичная эмуляция является самым распространенным видом виртуализации в наше время. Основным ее недостатком является зависимость виртуальных машин от конкретной аппаратной архитектуры [3].
Примеры продуктов для частичной эмуляции: `VMware Workstation, VMware Server, VMware ESXI Server, Virtual Iron, Microsoft Hyper-V Server, Microsoft Virtual PC, Sun VirtualBox, Parallels Desktop` и другие.

В хакатоне мы будем использовать ЦОД под управлением гипервизора `ESXi 6.5` и платформы централизованного управления `VMware vCenter`. Вы можете ознакомиться с документацией по `VMware vCenter` [[тут]](https://docs.vmware.com/en/VMware-vSphere/index.html). В комплекте с `vCenter` предоставляется веб-клиент `VMware vSphere Web Client`, с помощью которого можно управлять виртуальной инфраструктурой через один из распространенных браузеров c поддержкой `Flash Player`).

Для выполнения заданий хакатона для каждой команды выделен пул виртуальных ресурсов:

- Оперативная память:  8 ГБ.

- Эквивалентная микропроцессорная частота: 2400 МГц.

- Объем дискового пространства 50 ГБ.

- Доступ во внешнюю сеть VM-Netwotk. 

- Виртуальная локальная сеть team**.


****
## Доступ к виртальному ЦОД <a name="21"></a>

Для доступа к выделенному для команды пула ресурсов необходимо открыть страницу по адресу:

* [https://195.19.40.127/vsphere-client/?csp](https://195.19.40.127/vsphere-client/?csp)

Для доступа необходимо использовать логин и пароль, предоставленные организаторами хакатона.

![Доступ к VMware vCenter](assets/vmware.png)
**Доступ к VMware vCenter**

После ввода логина и пароля вы на главную административную панель `VMware vSphere Web Client` (т.н. Home).

![](assets/esx_1.png)
**Главную административную панель `VMware vSphere Web Client`**

При первом входе вам необходимо изменить пароль. 

Войдите в меню **Roles -> Users and Groups**. Выберите в списке вашу команду (team), введите текущий пароль и дважды новый пароль. Требования к сложности пароля: только цифры, не менее 6 цифр.

![](assets/esx_2.png)


****
## Создание виртуальной машины сетевого шлюза и установка ОС <a name="22"></a>

Перейдите из главной административной панели в раздел **Hosts and Clasters**.

Иерархия виртуального ЦОД состоит из следующих уровней:

1) Уровень Центра обработки данных: `195.19.40.127`

2) Уровен кластера: `IU6`

3) Уровень хоста: `195.19.40.124` или `195.19.40.125` или `195.19.40.126`. 

4) Уровень консолидированного вычислительного пула (так называемое виртуальное приложение `vAPP`): `team**`.

Дальнейшие действия необходимо производить только на уровне 4). Управление остальными уровнями иерархии выполняется только администратором ЦОД.

![](assets/esx_3.png)
**Иерархия виртуального ЦОД**

Вызовите контекстное меню для вычислительного пула. Выберите пункт **New Virtual Machine**.

![](assets/esx_4.png)
**Создание новой виртуальной машины**

Далее следуйте указаниям диалога. 

В пункте **2a** выберите кластер для развертывания виртуальной машины: `IU6`. 

![](assets/esx_5_u.png)

В пункте **2b** выберите вычислительный пул: `team**`. 

![](assets/esx_6.png)

В диалоге выбора ресурсов виртуальной машины выберите следующие ресурсы:

- 1 CPU

- 1 GB RAM

- 10 GB HDD. 

- Две сетевые карты типа `VMXNET 3`, подключенные к сетям `VM-Netwotrk` и `team**`.

![](assets/esx_8.png)

!!!По умолчанию выбирается так называемый Thick (толстый) тип диска. Все пространство такого диска выделяется в момент создания, при этом блоки не очищаются от данных, которые находились там ранее. Это может создавать потенциальные угрозы безопасности, поскольку виртуальная машина может получить доступ к данным на хранилище VMFS, которые ей не принадлежат. При обращении к блокам такого диска их содержимое предварительно не очищается со стороны ESX. Преимущество дисков типа thick - производительность и быстрота создания, недостаток - безопасность

Поэтому мы будем использовать диски типа Thin ("тонкие диски"), позволяющий автоматически расширять занимаемое дисковое пространство по мере заполнения диска. 
Эти диски создаются минимального размера и растут по мере их наполнения данными до выделенного объема. При выделении нового блока - он предварительно очищается. Эти диски наименее производительны (выделение нового блока и его очистка), однако наиболее оптимальны для экономии дискового пространства на системе хранения данных.

![](assets/esx_8_1.png)


Для установки операционной системы необходимо подключить виртуальный привод CD/DVD и смонтировать в него установочный образ системы. Выберите пункт **Datastore ISO file**. В открывшемся диалоге выберите один из образов Ubuntu в папке **OS_images**. Включите пункт **Connect** для CD/DVD.

!!! Рекомендуется использовать версию ОС Linux без установленного рабочего стола. Например: `Ubuntu-18.04.3-live-server-amd64.iso`. Вы также можете использовать любой другой дистрибутив Linux, имеющийся в папке OS_images или ваш дистрибутив, подключив к CD/DVD файл iso с локального компьютера !!!


![](assets/esx_7_u.png)


Проверьте параметры ВМ. Создайте виртуальную машину.

![](assets/esx_9.png)

Теперь необходимо установить операционную систему. 

В контекстом меню ВМ выберите пункт **Power -> Power On**.

![](assets/esx_10.png)

Откройте консоль ВМ:

![](assets/esx_11.png)

Установите ОС со следующими сетевыми настройками:

На сетевом интерфейсе ens160:

    Параметры сети: 195.19.36.64/27
    IP адрес: выдается организаторами из диапазона 195.19.36.64/27
    Шлюз: 195.19.36.65
    DNS: 195.19.32.2

На сетевом интерфейсе ens192:

    Параметры сети: 192.168.1.0/24
    IP адрес: 192.168.1.1

![](assets/esx_12u.png)

При установке серверной версии ОС Ubuntu обязательно разрешите использование LVM (Logical Volume Manager), что позволит расширять объем логических томов в процессе работы Linux.

![](assets/esx_13.png)

Разрешите установку и запуск OpenSSH сервера, что понадобится для соединения с Вашей виртуальной системой по протоколу SSH.

![](assets/esx_14.png)

После установки системы перегрузите виртуальную машину и выполните вход по введенным вами реквизитам пользователя.

![](assets/esx_15.png)

Проверьте, что сетевое соединение работает:

    ping www.ya.ru

Если соединение отсутствует, его необходимо установить. Использовать следует способы и ПО, поставляемое с выбранной вами ОС.

В случае использования серверной версии Ubuntu, отредактируйте файл /etc/netplan/50-cloud-init.yaml. После этого актуализируйте конфигурацию сети по команде:

    sudo netplan apply

Внимание! В других версиях ОС Linux, отличных от Ubuntu 18.04.3 может потребоваться изменение файла /etc/network/interfaces или использование другого механизма настройки сети (см. мануалы к версии вашей ОС).

Используя терминал операционной системы можно выполнить пинг вашей системы:

    ping 195.19.36.**
    PING 195.19.36.** (195.19.36.**) 56(84) bytes of data.
    64 bytes from 195.19.36.**: icmp_seq=1 ttl=58 time=5.94 ms
    64 bytes from 195.19.36.**: icmp_seq=2 ttl=58 time=5.79 ms


Если успешно установлено соединение с вашей ВМ, то можно подключиться к ней используя терминал с поддержкой протокола SSH (Tilix, Putty, term и пр.).
В ОС Linux это можно сделать по команде:

    ssh username@ip_address

где ip_address - ip адрес вашей ВМ.


![](assets/esx_16.png)



****
## Создание виртуальной машины в локальной сети и настройка шлюза<a name="24"></a>

Если в проекте понадобится использовать несколько серверов (например, сервер видеоаналитики) и обеспечить к ним доступ по единственному выделенному ip адресу, потребуется использовать локальную сеть (сеть team** с диапазоном адресов 192.168.1.0/24). Доступ ко всем локальным серверам будет осуществляться через шлюз, в котором необходимо выполнить конфигурирование доступа.

Создадим новую виртуальную машину в вашем пуле ресурсов со следующими параметрами:

В диалоге выбора ресурсов виртуальной машины выберите следующие ресурсы:

- 2 CPU

- 4 GB RAM

- 20 GB HDD. 

- Одна сетевая карта типа `VMXNET 3`, подключенная к сети `team**`.
 

![](assets/esx_17.png)

Далее повторите установку операционной системы аналогично предыдущей. 

Настройте сетевой интерфейс следующим образом:

    Параметры сети: 192.168.1.0/24
    IP адрес: 192.168.1.2/24
    Шлюз: 192.168.1.1
    DNS: 195.19.32.2

После установки проверьте доступность шлюза по команде:

    ping 192.168.1.1

При необходимости отредактируйте файл /etc/netplan/50-cloud-init.yaml и выполните команду:

    sudo netplan apply

Далее настроим шлюз (виртуальную машину с внешним ip адресом `195.19.36.**`) таким образом, чтобы соединение с портом 2022 на интерфейсе ens160 перенаправлялось через интерфейс ens192 на порт 22 по сетевому адресу 192.168.1.2.

Войдите через ssh консоль на шлюз:

    ssh username@ip_address

Далее получите права root:

    sudo -i

Введите следующие команды (замените `**` на ваш ip адрес):

    iptables -t nat -A POSTROUTING -s 192.168.1.2 -o ens160 -j MASQUERADE
    iptables -t nat -A PREROUTING -p tcp -d 195.19.36.** --dport 2022 -j DNAT --to-destination 192.168.1.2:22
    iptables -t filter -A FORWARD -i ens192 -d 192.168.1.2 -p tcp --dport 2022 -j ACCEPT

Сохраним конфигурацию  iptables в файл:

    iptables-save > /etc/iptable.restore

Удостоверьтесь, что в системе разрешен форвардинг:

    cat /proc/sys/net/ipv4/ip_forward
    1

Если форвардинг запрещен (ответ:`0`), разрешите его в файле /etc/sysctl.conf. Необходимо раскомментировать строку:

    net.ipv4.ip_forward = 1

Далее применим изменения:

    sysctl -p 

Далее разрешим настройку iptables из файла /etc/iptable.restore при старте системы. Для этого создадим сервис:

    nano /etc/systemd/system/restore-iptables-rules.service


Скопируйте в файл следующие строки:

    [Unit]
    Description = Apply iptables rules

    [Service]
    Type=oneshot
    ExecStart=/bin/sh -c 'iptables-restore < /etc/iptable.restore'

    [Install]
    WantedBy=network-pre.target

Далее разрешим сервис командой:

    systemctl enable restore-iptables-rules.service


Далее выполним `reboot` и проверим, что iptables настроены правильно: 

    sudo iptables -L

 
Проверьте, что форвардинг портов работает и из внешней сети (подключитесь с вашего рабочего компьютера или с компьютера на кафедре):

    ssh username@ip_address -p 2022
 

****
## Уcтановка фреймворка Anaconda и Jupyter Notebooks <a name="25"></a>


Anaconda Distribution это фреймворк с открытым исходным кодом, который объединяет библиотеки и средства разработки кода на языках Python / R для решения задач в области машинного обучения и науки о данных. Anaconda доступна как в виде десктопного приложения на платформах Linux, Windows и Mac OS X, так и в составе серверного ПО. В состав дистрибутива входит более 1500 пакетов Python / R для научных исследований, средста управления бибилиотеками Conda, средства разработки  обучения моделей машинного обучения и глубокого обучения scikit-learn, TensorFlow и Theano, известные бибилотеки для анализа данных (Dask, NumPy, pandas и Numba и т.д.) и визуализации результатов (Matplotlib, Bokeh, Datashader, Holoviews и пр.)


Подключимся к виртуальной машине:

    ssh username@ip_address -p 2022


Подготовим виртуальную машину, созданную нами в локальной сети, к установке дистрибутива. Необходимо по крайней мере 2 ГБ свободного места на жестком диске в томе **/**. Проверим это: 

    df -h
    Filesystem                         Size  Used Avail Use% Mounted on
    udev                               463M     0  463M   0% /dev
    tmpfs                               99M  1.1M   98M   2% /run
    /dev/mapper/ubuntu--vg-ubuntu--lv  3.9G  3.7G     0 100% /
    tmpfs                              493M     0  493M   0% /dev/shm
    tmpfs                              5.0M     0  5.0M   0% /run/lock
    tmpfs                              493M     0  493M   0% /sys/fs/cgroup
    /dev/loop0                          89M   89M     0 100% /snap/core/7270
    /dev/sda2                          976M   76M  834M   9% /boot
    tmpfs                               99M     0   99M   0% /run/user/1000


Место, выделенное на диске во время установки системы явно недостаточно. Если на вашей виртуальной машине достаточно места, пропустите следющие шаги, перейдите [сюда](#230).

Так как мы использовали `LVM`, нам необходимо расширить логический том за счет имеющегося свободного пространства на диске. Выведем список логических томов:

    sudo lvdisplay

    --- Logical volume ---
    LV Path                /dev/ubuntu-vg/ubuntu-lv
    LV Name                ubuntu-lv
    VG Name                ubuntu-vg
    LV UUID                nZSozr-hX7F-foyY-op40-8uKp-IYL9-wLplX0
    LV Write Access        read/write
    LV Creation host, time ubuntu-server, 2019-09-29 15:54:06 +0000
    LV Status              available
    # open                 1
    LV Size                4.00 GiB
    Current LE             1024
    Segments               1
    Allocation             inherit
    Read ahead sectors     auto
    - currently set to     256
    Block device           253:0

В отчете указано, что логический том размещен в группе физических томов **ubuntu-vg**, где мы и будем выделять пространство. Уточним, есть ли свободное место в **ubuntu-vg**:

    sudo vgdisplay

    --- Volume group ---
    VG Name               ubuntu-vg
    System ID             
    Format                lvm2
    Metadata Areas        1
    Metadata Sequence No  3
    VG Access             read/write
    VG Status             resizable
    MAX LV                0
    Cur LV                1
    Open LV               1
    Max PV                0
    Cur PV                1
    Act PV                1
    VG Size               <49.00 GiB
    PE Size               4.00 MiB
    Total PE              12543
    Alloc PE / Size       1024 / 3.70 GiB
    Free  PE / Size       11519 / 46.30 GiB
    VG UUID               dvjRPM-MLcl-QsFw-sXPp-72CA-mZpy-Oeb3fy


Также можно узнать, на каком из физических дисков емеется свободное пространство (т.к. у нас только один диск, показан будет он)

    sudo pvdisplay

    --- Physical volume ---
    PV Name               /dev/sda3
    VG Name               ubuntu-vg
    PV Size               <49.00 GiB / not usable 0   
    Allocatable           yes 
    PE Size               4.00 MiB
    Total PE              12543
    Free PE               1024
    Allocated PE          11519
    PV UUID               Z1Ya4U-czwy-37JR-DAYw-SDTm-v9JI-oP3zCT


В отчете lvdisplay указан системный путь к логическому устройству тома: **/dev/ubuntu-vg/ubuntu-lv**. Расширим его на 2048 блоков по 4 MiB: 


    sudo lvextend -l +2048 /dev/ubuntu-vg/ubuntu-lv

    sudo resize2fs /dev/ubuntu-vg/ubuntu-lv 

    sudo lvdisplay

    --- Logical volume ---
    LV Path                /dev/ubuntu-vg/ubuntu-lv
    LV Name                ubuntu-lv
    VG Name                ubuntu-vg
    LV UUID                nZSozr-hX7F-foyY-op40-8uKp-IYL9-wLplX0
    LV Write Access        read/write
    LV Creation host, time ubuntu-server, 2019-09-29 15:54:06 +0000
    LV Status              available
    # open                 1
    LV Size                12.00 GiB
    Current LE             3072
    Segments               1
    Allocation             inherit
    Read ahead sectors     auto
    - currently set to     256
    Block device           253:0


Далее уточним состояние логического тома:

    df -h

    Filesystem                         Size  Used Avail Use% Mounted on
    udev                               463M     0  463M   0% /dev
    tmpfs                               99M  1.1M   98M   2% /run
    /dev/mapper/ubuntu--vg-ubuntu--lv   12G  3.7G    8G  30% /
    tmpfs                              493M     0  493M   0% /dev/shm
    tmpfs                              5.0M     0  5.0M   0% /run/lock
    tmpfs                              493M     0  493M   0% /sys/fs/cgroup
    /dev/loop0                          89M   89M     0 100% /snap/core/7270
    /dev/sda2                          976M   76M  834M   9% /boot
    tmpfs                               99M     0   99M   0% /run/user/1000

<a name="230"></a>
Теперь места на диске достаточно, начнем установку Anaconda. Установим дистрибутив в домашней директории пользователя виртуальной машины:

    cd ~
    wget https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh
    chmod +x Anaconda3-2019.03-Linux-x86_64.sh
    bash ./Anaconda3-2019.03-Linux-x86_64.sh

Проверим список установленных пакетов:

    conda list

Обновим устаревшие пакеты:

    conda update conda

Теперь необходимо настроить Jupyter Notebook. Сгенерируем шаблон конфигурационного файла:

    jupyter notebook --generate-config

Теперь добавим в конфигурационный файл, указанный при создании (`~/.jupyter/jupyter_notebook_config.py`) следующие строки:


    # Разрешим доступ из внешней сети
    c.NotebookApp.ip = '192.168.1.2'
    c.NotebookApp.open_browser = False
    # Укажем порт, на котором будет работать Jupyter Notebook
    c.NotebookApp.port = 8888

Далее создадим файл с хешированным паролем:

    jupyter notebook password


Теперь все готово для запуска Jupyter Notebook. Вы можете сделать это в консоли по команде:


    jupyter notebook &
    # или
    jupyter lab &
    # если вы не хотите читать лог приложения в консоли, вы можете перенаправить поток сообщений на устройство /dev/null:
    jupyter notebook &>/dev/null &


Далее необходимо разрешить фовардинг порта 8888 на шлюзе `195.19.36.**`. Подключимся к виртуальной машине шлюза (не перепутайте!!!):

    ssh username@ip_address

Далее:

    sudo -i
    iptables -t nat -A PREROUTING -p tcp -d 195.19.36.** --dport 8888 -j DNAT --to-destination 192.168.1.2:8888
    iptables -t filter -A FORWARD -i ens192 -d 192.168.1.2 -p tcp --dport 8888 -j ACCEPT

Сохраним конфигурацию  iptables в файл:

    iptables-save > /etc/iptable.restore


Теперь вы можете подключиться к Jupyter Notebook в броузере с вашего десктопа по адресу:

    http://<ip адрес вашего сервера>:8888/

Помните, что порт 8888 должен быть открыт в firewall вашей ОС


****
# День 2. Практикум в Jupyter Notebook <a name="3"></a>

## Методология машинного обучения <a name="3_1"></a>

Существует мнение о том, что не существует единого метода машинного обучения, который лучше всего справлялся бы со всеми проблемами. Независимо от того, насколько сложен или прост применяемый метод или алгоритм, он не будет работать наилучшим образом для всех проблем. Поэтому, чтобы найти наилучший метод и его алгоритмическую реализацию, которые соответствовали бы потребностям конкретной задачи, необходимо обладать широким кругозором методов и алгоритмов, а также владеть техническими средствами анализа данных.

В нашем конкурсе будем придерживаться следующей упрощенной методологии машинного обучения:

    - Прежде чем углубляться в сложные методы и тратить время на тонкую настройку модели, лучше попробовать более простые методы и алгоритмы. По мере продвижения к более сложным методам мы можем обнаружить, что для  наших нужд оказывается достаточно уже примененнного и более простого подхода.

    - Гибкая методология машинного обучения предполагают, что мы должны развивать понимание данных итерационно. Это означает, что мы не должны пытаться разрешить все проблемы сразу. Для науки о данных это означает, что мы начинаем с простого подхода и готовимся к применению более сложных методов, методик, алгоритмов, моделей и т.д и т.п.. 

    - Первая итерация должна иметь наиболее простой вариант реализации каждого этапа конвейера машинного обучения(например, обработка, извлечение признаков и пр.). Дополнительным достоинством такого подхода является то, что применение упрощенных подходов менее затратно с точки зрения вычислительной мощьности, не требует интенсивных вычислений или дорогих поисков гиперпараметров. Бесспорно, простая модель может работать плохо, но получение этой модели возможно максимально быстро и с минимальными затратами ресурсов. 

    - Для интерпретации полученных результатов и уточнения применяемых методов необходимо привлечение специалистов по предметной области, которые могут интерпретировать полученный результат. 

Пример такого простого метода: поиск ближайшего соседа и другие подобные методы. Для их реализации требуется лишь несколько строк кода. Вместе с тем нет никаких причин, по которым более простые методы не могут быть лучше сложных. На последующих итерациях конверйера машинного обучения исследуются другие подходы. Это позволит нам сравнить, как методологические изменения влияют на производительность, и отслеживать улучшения с течением времени. Тем не менее, по мере дальнейших исследований улучшение качества модели становится все более проблематичным. Например, если мы достигли точности 99%, возможно, нам не следует тратить больше времени и ресурсов, чтобы пробовать доводить ее до 99,2%. 


[Словарь терминов машинного обучения](https://docs.microsoft.com/ru-ru/dotnet/machine-learning/resources/glossary)

Далее подробнее рассмотрим этапы конвейера машинного обучения.


## Предварительная обработка данных <a name="3_2"></a>


Целью предварительной обработки является преобразование необработанных данных в форму, которая подходит для машинного обучения. Структурированные и чистые данные позволяют получать более точные результаты из прикладной модели. Этап предполагает форматирование данных, очистку и выборку достоверных значений. Этот этап может также включать в себя сокращение числа несвязанных признаков с помощью композиции признаков, если к исследованиям подключены специалисты, которые могут это сделать (для этого требуются расширенные знания предметной области). Привлечение специалистов в преметной области является важнейшим фактором успеха применения машинного обучения. 


### Очистка данных <a name="3_2_1"></a>

*Очистка данных*, это набор процедур, которые позволяют удалить шум и устранить несоответствия в данных. Этот процесс включает в себя заполнение недостающих данных с использованием метдов подстановки правдоподобных значений (таких, как замена отсутствующих значений на средние или максимальные значения и т.д.). Обнаружение выбросов (наблюдений, которые значительно отличаются от остальной части распределения) также важно для понимания данных. Анализ таких данных и принятие решения об очистке должен приниматься совместно со специалистом по предметной области.  Если какие-либо выбросы указывают на ошибочные данные, их следует удалить или исправить, если это возможно. Этот этап также включает в себя удаление неполных и бесполезных записей данных и столбцов.

### Масштабирование <a name="3_2_2"></a>

*Масштабирование* – данные могут иметь числовые атрибуты (признаки), которые охватывают сильно отличающиеся диапазоны, например, миллиметры, метры и километры. Масштабирование - это преобразование таких атрибутов таким образом, чтобы они имели одинаковый масштаб, например, в диапазоне от 0 до 1 или от 1 до 10 для наименьшего и наибольшего значения для атрибута. 

*Минимаксная* нормализация представляет собой линейное отображение данных из одного интервала в другой. Данный подход предполагает вычитание минимального значения объекта из остальных значений, после чего значения делятся на полученный диапазон. Он сохраняет исходное распределение функции и не вносит существенных изменений в информацию, встроенную в исходные данные. 

*Нормализация стандартным отклонением* модифицирует признаки путем вычитания среднего значения, а затем деления на стандартное отклонение. Это изменяет распределение признаков и приводит к распределению со средним значением, равным нулю, и стандартным отклонением, равным единице. 

*Логарифмическое преобразование* важно для преобразования мультипликативных отношений между признаками в аддитивные отношения. Так как большие значения уменьшаются больше, чем маленькие, логарифмическое преобразование имеет эффект псевдоскейлинга, поскольку различия между большими и малыми значениями в наборе данных уменьшаются. 

Выбор одного метода масштабирования среди других зависит не только от набора данных, но и от выбранного метода машинного обучения, поскольку различные методы машинного обучения фокусируются на разных аспектах данных. Например, метод кластеризации фокусируется на анализе сходства точек данных, в то время как анализ главных компонентов (PCA) выявляет наиболее существенные признаки данных. Использование наиболее адекватного метода масштабирования может улучшить результаты кластеризации.

**Дополнительные источники литературы по данному разделу:**

- [Практическое руководство по подготовке данных (en)](https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/)

- [Масштабирование данных данных (en)](https://codefellows.github.io/sea-python-401d5/lectures/rescaling_data.html#)


### Уменьшение размерности<a name="3_2_3"></a>

*Уменьшение размерности* (Dimesion Reduction) – основная проблема при интерпретации многомерных данных заключается в том, что соответствующие истинные закономерности скрыты нерелевантными данными или шумом. Для малых и средних наборов данных можно использовать классические статистические методы, которые фокусируются на идентификации сильных статистических закономеростей. Такие методы, однако, не могут быть распространены на случаи, когда размерность данных намного превышает размер записей, а слабые истинные сигналы окружены значительным количеством шума (например, как в случае современного геномного анализа). Кроме того, диапазон шума имеет тенденцию увеличиваться с увеличением размерности данных, что часто делает существующие методы непрактичными [3]. По мере роста количества признаков или измерений, объем данных, которые нам необходимы для точного обобщения, растет в геометрической прогрессии.

При добавлении нового признака в модель иногда не хватает данных для поддержания отношений, и, следовательно, новый признак может не иметь положительное влияние на модель. Например, в нашем исследовании мы имеем 299 переменных (p = 299). В этом случае мы можем иметь 299 (299-1) / 2 = 44551 различных парных групп признаков. Нет смысла визуализировать каждый из них в отдельности. В тех случаях, когда у нас большое количество переменных, лучше выбрать подмножество этих переменных (p << 100), которое собирает столько информации, сколько и исходный набор переменных. Вот некоторые преимущества применения уменьшения размерности к набору данных:

• пространство, необходимое для хранения данных, уменьшается по мере уменьшения количества измерений;

• уменьшение размеров приводит к меньшему времени вычислений / обучения;

• некоторые алгоритмы плохо обрабатывают большие размеры данных. Таким образом, чтобы эти алгоритмы были полезны, необходимо уменьшить размерность задачи;

• уменьшение размерности способствует т.н. мультиколлинеарности (наличии линейной зависимости между объясняющими переменными), удаляя лишние признаки. Например, у вас есть две переменные - «время, проведенное на беговой дорожке в минутах» и «потраченные калории». Эти переменные сильно коррелируют: чем больше времени вы проводите на беговой дорожке, тем больше калорий вы будете сжигать. Следовательно, нет смысла хранить оба, так как достаточно одного из них для создания адекватной модели.

• это помогает в визуализации данных. Как обсуждалось ранее, очень трудно визуализировать данные для многих измерений. Поэтому сокращение пространства до 2D или 3D может позволить нам более четко отображать и наблюдать кластеры данных.

Уменьшение размерности может быть сделано двумя различными способами:

• выбором признаков: сохраняются только самые важные переменные из исходного набора данных,

• путем нахождения меньшего набора новых переменных, каждая из которых является комбинацией входных переменных, содержащих в основном ту же информацию, что и входные переменные.


**Дополнительные источники литературы по данному разделу:**

- [Практическое руководство по подготовке данных (en)](https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/)

- [Масштабирование данных данных (en)](https://codefellows.github.io/sea-python-401d5/lectures/rescaling_data.html#)

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	



### Алгоритм отбора признаков<a name="3_2_4"></a>

*Алгоритм отбора признаков* – один из наиболее широко используемых алгоритмов выбора объектов для построения модели данных. Этот алгоритм помогает выбрать меньшее подмножество признаков по сравнению с первоначальным. Алгоритм отбора признаков можно рассматривать как комбинацию техник поиска для представления нового поднабора признаков вместе с вычислением меры, которая отражает различие подмножеств признаков. 

    Wiki цитата: В традиционной статистике наиболее популярной формой отбора признаков является ступенчатая регрессия, которая является техникой оборачивания. Это жадный алгоритм, который добавляет лучший признак (или удаляет худший) на каждом шаге алгоритма. Главная проблема — когда остановить алгоритм. При обучении машин это обычно делается путём перекрёстной проверки. В статистике некоторые критерии оптимизированы. Это ведёт к наследованию проблемы вложения. Исследовались и более устойчивые методы, такие как метод ветвей и границ и кусочно-линейная сеть.

**Дополнительные источники литературы по данному разделу:**

- [Алгоритмы отбора признаков (ру)](https://ru.wikipedia.org/wiki/%D0%9E%D1%82%D0%B1%D0%BE%D1%80_%D0%BF%D1%80%D0%B8%D0%B7%D0%BD%D0%B0%D0%BA%D0%BE%D0%B2)

### Обратное удаление признаков<a name="3_2_5"></a>

*Обратное удаление признаков* (Backward Feature Elimination) - рекурсивно удаляет некоторые отобранные признаки, строит модель с использованием оставшихся признаков и вычисляет точность модели. Он включает в себя следующие этапы:

1) сначала берутся все «n» переменных, присутствующих в наборе данных, и на них обучатся модель;

2) далее рассчитывается точность модели,

3) затем вычисляется точность модели после исключения каждой из переменной (всего получается n упрощенных моделей), то есть каждый раз отбрасывается одна переменная и модель строится на оставшихся «n-1» переменных;

4) определяется переменная, удаление которой дало наименьшее изменение точности модели, а затем эта переменная отбрасывается;

5) весь процесс повторяется пока какая-либо переменная может быть отброшена.

Для работы алгоритма нужно указать алгоритм и количество объектов, которые нужно выбрать. С помощью данного алгоритма также может быть выполнено ранжирование переменных.


### Выбор вперед<a name="3_2_6"></a>

*Выбор вперед* (Forward Feature Elimination) - это противоположный *обратному удалению признаков* процесс. Вместо того, чтобы исключать признаки и удалять их, мы пытаемся найти лучшие признаки для представления более точной модели. Эта техника работает следующим образом:

1) мы начинаем с одного признака. По сути, мы обучаем модель «n» раз, используя каждый признак отдельно;

2) переменная, дающая наилучшую точность обученной модели выбирается в качестве начальной переменной;

3) затем мы повторяем этот процесс и добавляем одну переменную за каждый проход алгоритма;

4) переменная, которая дает наибольшее увеличение производительности, фиксируется в модели;

5) мы повторяем этот процесс до тех пор, пока не будут замечены значительные улучшения в производительности модели.

    ПРИМЕЧАНИЕ. Как обратное удаление, так и прямой выбор функций требуют много времени и вычислительных затрат.

**Дополнительные источники литературы по данному разделу:**

- [Forward Feature Elimination Wiki](https://en.wikipedia.org/wiki/Feature_selection)

### Анализ главных компонентов<a name="3_2_7"></a>

*Анализ главных компонентов* (Principal Component Analysis) PCA, это методика, которая помогает нам извлекать новый набор переменных из существующего большого набора переменных. Эти вновь извлеченные переменные и называются главными компонентами. 

Вот некоторые из ключевых моментов, которые следует знать о PCA:

1) главные компоненты представляет собой линейную комбинацию исходных переменных;

2) основные компоненты выбираются таким образом, что первый основной компонент обеспечивал максимальную дисперсию в наборе данных;

3) второй основной компонент пытается объяснить оставшуюся дисперсию в наборе данных и не связан с первым главным компонентом,

4) каждое дополнительное измерение, которое мы добавляем в методике PCA, отражает все меньше и меньше дисперсии в модели. Первый компонент является наиболее важным, затем следует второй, затем третий и т. д.

5) чтобы найти каждый компонент, алгоритм пытается максимизировать дисперсию, и каждый новый компонент должен быть ортогональным к другим.

![](assets/ml_01.png)
**Анализ главных компонентов**

Слева мы имеем представление простого двумерного набора данных с тремя одномерными гиперплоскостями. С другой стороны, справа показан результат проецирования набора данных на каждую из этих одномерных гиперплоскостей. Cтановится ясно, что гиперплоскость, представленная сплошной линией, выявляет максимальную дисперсию в наборе данных. Она также позволяет найти вторую ось (пунктирную линию), ортогональную первой, которая учитывает наибольшее количество оставшихся отклонений данных.

Если бы мы имели дело с большим количеством измерений (как это и есть в реальных задачах), PCA нашел бы большее количество ортогональных осей к предыдущим осям (фактически столько осей, каково число измерений в наборе данных). Единичный вектор, который определяет i-ю ось, называется главным компонентом (PC). В этом случае первый PC - это С1, а второй PC - С2. После того, как мы определили наши основные компоненты, пришло время уменьшить размерность набора данных до d измерений, проецируя его на гиперплоскость, определенную первыми d основными компонентами. Обычно, в практических задачах, большое количество измерений, это то, что составляет достаточно большую часть дисперсии (~ 90%).

**Дополнительные источники литературы по данному разделу:**

- [Метод главных компонент (Wiki)](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D1%8B%D1%85_%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82)


### Независимый компонентный анализ<a name="3_2_8"></a>


*Независимый компонентный анализ* (Independent Component Analysis, ICA) - основан на теории информации, а также является одним из наиболее широко используемых методов уменьшения размерности. Основное различие между PCA и ICA состоит в том, что PCA ищет некоррелированные факторы, в то время как ICA ищет независимые факторы. Если две переменные некоррелированы, это означает, что между ними нет линейной зависимости. Если они независимы, это означает, что они не зависят от других переменных. Например, возраст человека не зависит от того, что он ест, или от того, сколько он смотрит телевизор.

Этот алгоритм предполагает, что данные переменные представляют собой линейные смеси некоторых неизвестных скрытых переменных. Также предполагается, что эти скрытые переменные являются взаимно независимыми, то есть они не зависят от других переменных и, следовательно, их называют независимыми компонентами наблюдаемых данных.

**Дополнительные источники литературы по данному разделу:**

- [Анализ независимых компонент Wiki](https://ru.wikipedia.org/wiki/%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D0%BD%D0%B5%D0%B7%D0%B0%D0%B2%D0%B8%D1%81%D0%B8%D0%BC%D1%8B%D1%85_%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82)


### Факторный анализ<a name="3_2_9"></a>


*Факторный анализ* (Factor Analysis) –  предположим, у нас есть две переменные: доход и образование. Эти переменные потенциально могут иметь высокую корреляцию, поскольку люди с более высоким уровнем образования, как правило, имеют значительно более высокий доход, и наоборот. В методе факторного анализа переменные сгруппированы по их корреляциям, то есть все переменные в определенной группе будут иметь высокую корреляцию между собой, но низкую корреляцию с переменными другой группы (групп). Здесь каждая группа известна как фактор. Эти факторы невелики по сравнению с исходными размерами данных. Однако эти факторы трудно наблюдать.

**Дополнительные источники литературы по данному разделу:**

- [Факторный анализ](https://ru.wikipedia.org/wiki/%D0%A4%D0%B0%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7)

- [Просто о факторном анализе](https://habr.com/ru/post/224495/)


### Многообразное обучение или нелинейное уменьшение размерности<a name="3_2_10"></a>


*Многообразное обучение или нелинейное уменьшение размерности* (Manifold Learning) –  это нелинейная версия PCA. Проблема состоит в том, что PCA ищет плоские поверхности для описания данных, которые не всегда наилучшим образом показывают макимальную дисперсию. Если плоской поверхности не существует, мы используем Mainfold Learning, чтобы попытаться решить эту проблему более эффективно.
Существует много подходов для решения этой проблемы, таких как Isomap, *Локально линейное вложение*, *Лапласово собственное отображение*, *Полуопределенное вложение* и т.д. Эти алгоритмы работают для извлечения низкоразмерного многообразия, которое можно использовать для описания многомерных данных.


### Локально линейное вложение<a name="3_2_11"></a>

*Локально линейное вложение* (LLE, Locally-Linear Embedding) –  это метод коллективного обучения, который не опирается на проекции в гиперплоскости, подобно PCA. Он работает, изучая, как каждое наблюдение линейно связано с его ближайшими соседями, а затем ищет низкоразмерное представление обучающего набора, где эти отношения лучше всего сохраняются. Для случаев, когда нет большого шума, LLE показывает лучшие результаты по сравнению с PCA. 

**Дополнительные источники литературы по данному разделу:**

- [An Introduction to Locally Linear Embedding](https://cs.nyu.edu/~roweis/lle/papers/lleintro.pdf)

### Cтохастическое вложение соседей с t-распределением<a name="3_2_12"></a>

*Cтохастическое вложение соседей с t-распределением* (t-SNE, t- Distributed Stochastic Neighbor Embedding) –  ищет шаблоны нелинейным способом. t-SNE - это один из немногих алгоритмов, который способен одновременно сохранять как локальную, так и глобальную структуру данных. Он рассчитывает вероятностное сходство точек в многомерном пространстве, а также в низкоразмерном пространстве. Эвклидовы расстояния больших размеров между точками данных преобразуются в условные вероятности, которые представляют сходства.
UMAP –  t-SNE очень хорошо работает с большими наборами данных, но также имеет свои ограничения, такие как потеря крупномасштабной информации, медленное время вычислений и неспособность осмысленно представлять очень большие наборы данных. 

**Дополнительные источники литературы по данному разделу:**

- [t-SNE Вики](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%B2%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9_%D1%81_t-%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%D0%BC)

- [Алгоритм t-SNE. Иллюстрированный вводный курс](http://datareview.info/article/algoritm-t-sne-illyustrirovannyiy-vvodnyiy-kurs/)

### Унифицированная аппроксимация и проекция многообразия<a name="3_2_13"></a>

*Унифицированная аппроксимация и проекция многообразия* (UMAP) - это метод сокращения размерности, который может сохранить как большую часть локальной, так и большей глобальной структуры данных по сравнению с t-SNE, с более коротким временем выполнения. 

Некоторые из ключевых преимуществ UMAP:

• он может обрабатывать большие наборы данных и данных большого размера без особых проблем,

• он сочетает в себе возможности визуализации с возможностью уменьшения размеров данных,

• наряду с сохранением локальной структуры, он также сохраняет глобальную структуру данных. UMAP отображает близкие точки на многообразии в соседние точки в низкоразмерном представлении и делает то же самое для удаленных точек,

• этот метод использует концепцию k-ближайшего соседа и оптимизирует результаты с использованием стохастического градиентного спуска. Сначала он вычисляет расстояние между точками в многомерном пространстве, проецирует их на низкоразмерное пространство и вычисляет расстояние между точками в этом низкоразмерном пространстве. Затем он использует Stochastic Gradient Descent, чтобы минимизировать разницу между этими расстояниями.

Корреляция между компонентами, полученными из UMAP, значительно меньше по сравнению с корреляцией между компонентами, полученными из t-SNE. Следовательно, UMAP имеет тенденцию давать лучшие результаты.
Краткое описание того, когда использовать каждую методику уменьшения размерности

**Дополнительные источники литературы по данному разделу:**

- [Uniform Approximation and Projection (UMAP) Вики](https://ru.wikipedia.org/wiki/UMAP)



### Краткий обзор алгоритмов и методов уменьшения размерности <a name="3_2_14"></a>
Кратко подведем итоги использования каждого метода уменьшения размерности, который мы рассмотрели. Важно понимать, где использовать определенную технику, поскольку это помогает сэкономить время, усилия и вычислительные мощности.

![](assets/ml_02.png)
**Использование методов уменьшения размерности**

#### Соотношение пропущенных значений<a name="3_2_14_1"></a>

Если в наборе данных слишком много пропущенных значений, мы используем этот подход для уменьшения количества переменных. Мы можем отбросить переменные с большим количеством пропущенных значений.

#### Фильтр низкой дисперсии<a name="3_2_14_2"></a>

Мы применяем этот подход для определения и удаления постоянных переменных из набора данных. На целевую переменную не влияют чрезмерно переменные с низкой дисперсией, и, следовательно, эти переменные могут быть безопасно отброшены

#### Фильтр высокой корреляции<a name="3_2_14_3"></a>

Пара переменных, имеющих высокую корреляцию, увеличивает мультиколлинеарность в наборе данных. Таким образом, мы можем использовать эту технику, чтобы найти сильно коррелированные функции и отбросить их соответственно.

#### Случайный лес<a name="3_2_14_4"></a>

Это один из наиболее часто используемых методов, который говорит нам о важности каждого атрибута, присутствующего в наборе данных. Мы можем найти важность каждого атрибута и сохранить самые верхние атрибуты, что приведет к уменьшению размерности. Как методы обратного удаления, так и прямого выбора элементов занимают много вычислительного времени и поэтому обычно используются в небольших наборах данных.

#### Факторный анализ<a name="3_2_14_5"></a>

Этот метод лучше всего подходит для ситуаций, когда у нас есть сильно коррелированный набор переменных. Он делит переменные на основе их соотношения на разные группы и представляет каждую группу с коэффициентом

#### Анализ основных компонентов<a name="3_2_14_6"></a>

Это один из наиболее широко используемых методов работы с линейными данными. Он делит данные на набор компонентов, которые пытаются объяснить как можно больше различий
Независимый анализ компонентов: мы можем использовать ICA для преобразования данных в независимые компоненты, которые описывают данные с использованием меньшего количества компонентов

- ISOMAP: мы используем эту технику, когда данные сильно нелинейны;

- t-SNE: этот метод также хорошо работает, когда данные сильно нелинейны или же требуется более понятная визуализаци данных.

- UMAP: эта методика хорошо работает для многомерных данных. Время его выполнения короче по сравнению с t-SNE.

### Разделение набора данных<a name="3_2_20"></a>

**Разделение набора данных** (Dataset splitting) – набор данных, используемый для машинного обучения, должен быть разделен на три подмножества - наборы обучения, тестирования и проверки.

**Обучающий набор**: используется для обучения модели и определения ее оптимальных параметров - параметров, которые он должен изучить из данных.

**Тестовый набор**: тестовый набор необходим для оценки обученной модели и ее способности к обобщению. Обобщение означает способность модели идентифицировать закономерности в новых невидимых данных после того, как они прошли обучение по данным обучения. Крайне важно использовать различные подмножества для обучения и тестирования, чтобы избежать переобучения модели, что является неспособностью к обобщению, о котором мы упоминали выше.

**Валидационный набор**: цель выделения валидационный набор набора состоит в том, чтобы настроить гиперпараметры модели - структурные параметры более высокого уровня, которые не могут быть непосредственно изучены из данных. Эти параметры могут указывать, например, насколько сложна модель и как быстро она находит шаблоны в данных.

Соотношение обучения и тестового набора обычно составляет 80 процентов. Затем обучающий набор снова разделяется, и его 20 процентов будут использоваться для формирования валидационного набора.
Чем больше используемых данных обучения, тем лучше будет работать потенциальная модель. Следовательно, больше используемых данных тестирования приводит к лучшей производительности модели и возможности обобщения.


## Практическая часть <a name="3_3"></a>

Чтобы работать с данными, необходимо понимать, что они из себя представляют. В начале любого исследования необходимо их загрузить и вывелить некоторые статистики.

Создайте новый Notebook и перенести в папку проекта наборы данных с сайта хакатона:

- [Обучающая выборка данных](data/train_longevity.csv)
- [Тестовая выборка данных](data/test_longevity.csv)

### Первичный анализ данных <a name="3_3_1"></a>


Импортируем библиотеки, нужные нам для работы

```python
 # data analysis and wrangling
import pandas as pd
import numpy as np
import random as rnd

 # visualization
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline 
```

Загрузим данные из исходных файлов:

```python
train_df = pd.read_csv('train_longevity.csv')
test_df = pd.read_csv('test_longevity.csv')
combine = [train_df, test_df]
```

Далее выведем названия столбцов в обучающем датасете:

```python
print(train_df.columns.values)
['Id' 'Longevity' 'Education' 'Sex' 'Age' 'Pet' 'Children' 'Region' 'Activity' 'MedExam' 'Sport']
```

И в тестовом датасете:

```
print(test_df.columns.values)
['Id' 'Education' 'Sex' 'Age' 'Activity' 'Sport' 'IsAlone' 'Age*Education']
```

Это даст нам первое представление о наших данных. Далее посмотрим на размеры наших табличных данных. Выполнив построчно код ниже как для 

```python
train_df.shape  	# мы увидим информацию о размерности нашего датафрейма 
train_df.info() 	# покажет информацию о размерности данных 
              	        # описание индекса, количество not-a-number элементов 
train_df.head()         # показывает первые 10 значений датасета
train_df.describe() 	# показывает статистики count,mean, std, min, 25%-50%-75% percentile, max 
train_df.nunique() 	# количество уникальных значений для каждого столбца
```

Приведем общую терминологию касающауюся наборов данных. Столбцы таблицы датасета это *атрибуты* объекта или его *признаки*. Строки - *объекты*.

**Объект** описывается как набор атрибутов. Объект также известен как запись, случай, пример, строка таблицы и т.д.

**Атрибут** - свойство, характеризующее объект. Атрибут также называют переменной, полем таблицы, измерением, характеристикой.

**Переменная** (variable) - свойство или характеристика, общая для всех изучаемых объектов, проявление которой может изменяться от объекта к объекту.

**Значение** (value) переменной является проявлением признака.

**Генеральная совокупность** (population) - вся совокупность изучаемых объектов, интересующая исследователя.

**Выборка** (sample) - часть генеральной совокупности, определенным способом отобранная с целью исследования и получения выводов о свойствах и характеристиках генеральной совокупности.

**Параметры** - числовые характеристики генеральной совокупности.

**Статистики** - числовые характеристики выборки.

**Гипотеза** - предположение относительно параметров совокупности объектов, которое должно быть проверено на ее части. Гипотеза - частично обоснованная закономерность знаний, служащая либо для связи между различными эмпирическими фактами, либо для объяснения факта или группы фактов.


**Дополнительные источники литературы по данному разделу:**

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	


### Описание задачи и данных <a name="3_3_2"></a>


В нашем хакатоне мы будем использовать задачу анализа факторов активного долголетия. К долголетним будем относить пожилых людей, доживших до 90 лет и сохранивших физическую и социальную активность. Представленные в датасетах данные носят иммитационный характер, однако на реальных данных. Данные представляют информацию о пожилых людях в возрасте от 70 до 80 лет, для которых известен класс активного долголетия (зависимый параметр Longevity) в будущем (будет ли достигнут вораст 90 лет).

В датасетах приведена следующая информация о пожилых людях (атрибуты объекта):

- Longevity - Класс активного долголетия: 1 - человек доживет до 90 лет; 0 - нет
- Id - Идентификатор пожилого человека;
- Education - Образование: 1 - высшее; 2 - среднее; 3 без образования;
- Sex - Пол;
- Age - Возраст;
- Pet - Пожилой человек ухаживает за домашними животными: указано количество;
- Children - Пожилой человек проживают с детьми/внуками/близкими родственниками: указано количество проживающих совместно в пожилым человеком;
- Region - Регион проживания;
- Activity - Уровень физической активности (количество шагов в день): данные получены от специального приложения;
- MedExam - Посещение поликлиники (за послений год): кодирование посещений на основе заполненной медицинской карточки;
- Sport - Физические упражнения: '+' пожилой человек занимается спортом (ходьба, бег, плаванье); '-' не занимается.


Более детально посмотрим на информацию о количестве каждого уникального значения для каждого столбца в наборе данных:

```python
feature_names = train_df.columns.tolist() 
for column in feature_names: 
    print (column) 
    print (train_df[column].value_counts(dropna=False))
```
Теперь определим количество не-нулевых значений и определенные библиотекой pandos типы атрибутов:

```pytohn
train_df.info()
print('_'*40)
test_df.info()
```

Итак, теперь мы можем судить о том, какие пробемы могут возникнуть с обработкой датасетов:

- Наличие пустых ячеек;
- Нечисловые значения nan (т.н. нечисла в формате Ч.П.З);
- Категориальные данные;
- Недостоверные/ошибочные значения (LINE, ЗНАЧ и другие). 

Начнем последовательно работать с данными, не изменяя исходных файлов csv.

*Плохим тоном в машинном обучении является изменения исходного датасета. Такой подход ведет к безвозвратной потере части значений, возможно, представляющих интерес для последующих итераций исследования.* **Правильным считается программное изменение загруженного датасета  с помощью подмены значений, фильтров и т.д.**.




В целом, целью анализа данных является ответ на следующие вопросы:
 
- Какие признаки доступны в наборе данных?
- Какие признаки являются категориальными? Являются ли категориальные значения уникальными наименованиями, порядковыми значениями, отношениями или интервалами? Например, атрибуты Sex, MedExam, Education, Longevity, Id, Region,  являются - категориальными, представленными наименованиями категорий. Education, Longevity, Id, Region являются порядковый категориальный признаками.
- Какие функции являются числовыми? Атрибуты Age, Pet, Children, Activity являются числовыми. - 
- Какие числовые признаки являются непрерывными, какие являются дискретными. Непрерывные: Age, Activity. Дискретность: Pet, Children
- Какие признаки являются смешанными типами данных (числовые, буквенно-цифровые данные внутри одной и той же функции)? Такие атрибуты доблжны быть исправлены. Атрибуты MedExam являются буквенно-цифровыми.
- Какие признаки могут содержать ошибки или опечатки? Функции MedExam, Age, Sport содержат ряд нулевых значений.
- Какие типы данных для различных признаков?
- Каково распределение числовых значений признаков по выборкам? Это помогает нам определить, помимо прочего, насколько репрезентативен обучающий набор данных для фактической проблемной области.


**Анализ данных по сводным таблицам (pivot tables)**

Чтобы подтвердить некоторые из наших наблюдений и предположений, мы можем быстро проанализировать наши корреляции признаков, используя сводные таблицы. На этом этапе мы можем сделать это только для функций, которые не имеют пустых значений. Это также имеет смысл делать только для функций, которые являются категориальными (Sex), порядковыми (Education) или дискретными (Pet, Children).

Пример создания сводной таблицы:

```python
train_df[['Education', 'Longevity']].groupby(['Education'], as_index=False).mean().sort_values(by='Longevity', ascending=False)
```

Проведите аналогичные исследования для групп параметров:

- ("Sex", "Longevity")
- ("Pet", "Longevity")
- ("Children", "Longevity")

**Анализ данных путем визуализации**

Теперь мы можем продолжить подтверждать некоторые из наших предположений, используя визуализации для анализа данных.

Давайте начнем с понимания корреляции между числовыми характеристиками и нашей целью решения (Долгожительство).

Гистограммы полезны для анализа непрерывных числовых переменных, таких как возраст. Гистограмма может указывать распределение выборок с использованием автоматически определенных интервалов или полос одинакового диапазона. Обратите внимание, что ось X в визуализациях неподготовленных данных не позволяет отобразить распрееление.

```python
g = sns.FacetGrid(train_df, col='Longevity')
g.map(plt.hist, 'Age', bins=20)
```

Подготовим данные к визуализации:

```python
 #Correct errors in Age column
train_df['Age'].unique()
test_df['Age'].unique()
```

Удалить некорректрное значение можно с помощью функции map и lambda оператора (см. манулаы по Python):

```python
idmax = train_df['Age'].value_counts().idxmax()
train_df['Age'] = train_df['Age'].map(lambda v: idmax if v == 'НЕДОСТОВЕРНОЕ ЗНАЧЕНИЕ' else v).astype(float)
test_df['Age'] = train_df['Age'].map(lambda v: idmax if v == 'НЕДОСТОВЕРНОЕ ЗНАЧЕНИЕ' else v).astype(float)
```


Замена категориальных значений на числовые (int или float) может быть выполнена вот так:

```python
 dataset['КАТЕГОРИЯ'] = dataset['КАТЕГОРИЯ'].map( {'ЗНАЧЕНИЕ#1': 1, 'ЗНАЧЕНИЕ#2': 2, 'ЗНАЧЕНИЕ#3': 3} ).astype(int)
```

Повторите построение гистограммы уже для числовых (float) значений Age. 

Сделайте выводы.

**Корреляция числовых и порядковых признаков**

Предварительно можно ознакомиться с корреляционной матрицей:

```python
train_df.corr()
```

Мы можем объединить несколько признаков для определения корреляций, используя один график. Это можно сделать с помощью числовых и категориальных функций, которые имеют числовые значения.


```python
grid = sns.FacetGrid(train_df, col='Longevity', row='Education', height=2.2, aspect=1.6)
grid.map(plt.hist, 'Age', alpha=.5, bins=20)
grid.add_legend();
```

Сделайте выводы.


**Корреляция категориальных и числовых признаков**

Нам также понадобится определить корреляцию категориальные характеристики (с нечисловыми значениями) и числовые характеристики. Мы можем рассмотреть корреляцию Sport (категориальное нечисловое значение), Пол (категориальное нечисловое), Активность (числовое непрерывное) с Долгожительством (категориальное числовые)

```python
grid = sns.FacetGrid(train_df, row='Sport', col='Longevity', height=2.2, aspect=1.6)
grid.map(sns.barplot, 'Sex', 'Activity', alpha=.5, ci=None)
grid.add_legend()
```

Сделайте выводы.

**Дополнительные источники литературы по данному разделу:**

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	


### Задание <a name="3_3_3"></a>

1) Признак Age должен быть дополнен и фильтрован для обработки алгоритмами.

2) Признак MedExam может быть отброшен, поскольку является крайне неполным или содержит много нулевых значений как в обучающем, так и в тестовом наборе данных.

3) Необходимо дополнить функцию Sport, поскольку она также может соотноситься с долголетием.

4) Признак Region может быть исключен из нашего анализа, так как содержит большое количество дубликатов (22%) и может отсутствовать корреляция между ним и целевым признаком.

5) Пизнак Id может быть удален из набора обучающих данных, поскольку он не способствует целевому признаку Longevity.

6) Cоздать новый признак под названием  Family (Семья на основе детей и домашних животных), чтобы получить общее количество членов семьи.

7) Заздать новый признак IsAlone, проживающих одиноко без домашних животных и родственников.

7) Cоздать новый признак для возрастных групп (Age,Education), указывающий на социальный статус пожилого человека. Это превращает непрерывный числовой признак в порядковый категориальный признак.

8) Cоздать новый признак диапазонов возрасного равновесия на основе признака Age, т.к. это поможет разделить пожилых людей на группы условно равновесного состояния  (смертность повышается в определенные периоды времени между 70 и 80 годами, и в другие моменты резко снижается) для следующих интервалов: { (...,70](70,72],(72,74],(74,76],(76,78],(80,..)}


**Дополнительные источники литературы по данному разделу:**

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	

- [Введение в pandas: анализ данных на Python](https://khashtamov.com/ru/pandas-introduction/)	

- [Развертывание модели машинного обучения Python в качестве веб-службы](https://developer.ibm.com/tutorials/deploy-a-python-machine-learning-model-as-a-web-service/)




****
# День 3. Алгоритмы машинного обучения <a name="4"></a>


## Алгоритмы машинного обучения <a name="4_3"></a>

После того, как мы предварительно обработали собранные данные и разбили их на три подмножества, мы приступаем к обучению модели. Этот процесс влечет за собой снабжение алгоритма данными обучения. Затем алгоритм обрабатывает данные и выводит модель, которая может найти целевое значение (атрибут) в новых данных (ответ, который мы хотим получить с помощью прогнозного анализа). Целью обучения модели является разработка модели.

Наиболее распространены два модельных стиля обучения - обучение с учителем и обучение без учителя. Выбор каждого стиля зависит от того, должны ли мы прогнозировать конкретные атрибуты или группировать объекты данных по сходству.

• *обучение с учителем*: Контролируемое обучение позволяет обрабатывать данные с целевыми атрибутами или помеченными данными. Обучение с учителем решает проблемы классификации и регрессии.

• *обучение без учителя*: Во время этого стиля обучения алгоритм анализирует немеченые данные. Цель обучения модели - найти скрытые взаимосвязи между объектами данных и объектами структуры по сходствам или различиям. Обучение без учителя направлено на решение таких проблем, как кластеризация, обучение правилам ассоциаций и уменьшение размерности. Например, его можно применять на этапе предварительной обработки данных, чтобы уменьшить сложность данных.

Существует два других стиля обучения модели: *полу-контролируемый*, в котором набор данных содержит как помеченные, так и немаркированные примеры. Второй стиль назвается *обучение с подкреплением*, при котором машина пытается выучить политику действий в соответствии с получением вознаграждения за каждое действие.
Мы опишем алгоритмы, которые являются не только самыми известными, но и либо очень эффективными сами по себе, либо используются в качестве строительных блоков для самых эффективных алгоритмов обучения.


![](assets/ml_03.png)
**Алгоритмы машинного обучения**

**Дополнительные источники литературы по данному разделу:**

- [scikit-learn / Machine Learning in Python](https://scikit-learn.org/stable/)

###  Линейная регрессия<a name="4_3_1"></a>

*Линейная регрессия* –  метод поиска зависимости между входными и выходными переменными с линейной функцией связи. Один из способов вычислить значения параметров модели является метод наименьших квадратов (МНК), который минимизирует среднеквадратичную ошибку между реальным значением зависимой переменной и прогнозом, выданным моделью. 

Пусть мы хотим построить модель объясняемой (зависимой) переменной в виде линейной комбинации остальных (независимых) признаков. Цель алгоритма МНК – построить гиперплоскость, максимально приближенную ко всем обучающим примерам.

<img src="assets/ml_04.png" width="400">
**Линейная регрессия**

На рисунке показана линия регрессии (красным цветом) для одномерных примеров (синие точки). Мы можем использовать эту прямую, чтобы предсказать значение объясняемой переменной *y(new)* для нового значения независимой переменной *x(new)*.

Чтобы получить эту оптимальную линию (или гиперплоскость в n-мерном случае), процедура оптимизации пытается минимизировать следующее выражение *функции стоимости*:

<img src="assets/ml_f2.png" width="200">

Выражение под знаком суммы называется *функцией потерь*. Суммировав этy функции для всех значений признака и разделив на количество значений *N* мы получаем среднюю ошибку при выборе конкретной прямой, которая и является функцией стоимости для линейной регрессии. Параметры *w* и *b* для 2-мерной регрессии определяет наклон прямой и ее смещение (в выражении f(x) = w\*x + b). Для большей размерности *w* и *b* представляют собой вектора, определяющие гиперплоскость. В этом случае функция *f* принимает вид:

<img src="assets/ml_f3.png" width="270">

**Дополнительные источники литературы по данному разделу:**

- [Регрессионный анализ. Кольцов С.Н.](https://www.hse.ru/data/2014/08/29/1313619461/%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F%205.pdf)

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	


###  Деревья решений<a name="4_3_2"></a>

*Деревья решений* - это это способ представления правил в иерархической виде, где каждому объекту соответствует единственный узел, пораждающий решение. Структура дерева представляет собой «листья» и «ветки». На рёбрах дерева решения записаны атрибуты, от которых зависит целевая функция, в «листьях» записаны значения целевой функции, а в остальных узлах - атрибуты, по которым различаются случаи.

На основе деревьев решений строятся многие важные алгоритмы классификации данных и регрессии.  Достоинством таких алгоритмов является высокая наглядность представления и простота интерпретации результатов, что может быть очень важным для предметной области: оказывается возможным не только провести процесс классификации, но и объяснить почему тот или иной объект отнесён к какому-либо классу. 

Идея этого алгоритма довольно проста. Дерево строится «сверху вниз» от корня. Начинается процесс с определения, какой атрибут следует выбрать для проверки в корне дерева. Для этого каждый атрибут исследуется на предмет, как хорошо он классифицирует набор данных (разделяет на классы по целевому атрибуту). При этом выбирается тот из атрибутов, который порождает наибольший количественный критерий оценки. Когда атрибут выбран, для каждого его значения создается ветка дерева, набор данных разделяется в соответствии со значением к каждой ветке, процесс повторяется рекурсивно для каждой ветки. Также следует проверять критерий остановки.

В деревьях классификации часто используются перекрестная энтропия, энтропия Шеннона и коэффициент Джини. В деревьях регрессии минимизируется сумма функций потерь. Мы выполняем эту процедуру рекурсивно для каждого узла и завершаем работу, когда выполняем критерии остановки. 

В качестве критерия остановки могут быть выбраны: минимальное количество уровней дерева от листа до вершины, минимальное значение критерия оценки в узле и пр. 

![](assets/ml_05.png)
**Деревья решений**

**Дополнительные источники литературы по данному разделу:**

- [Деревья решений — общие принципы работы](https://basegroup.ru/community/articles/description)

- [Открытый курс машинного обучения](https://habr.com/ru/company/ods/blog/322534/)

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	



###  Метод опорных векторов<a name="4_3_3"></a>

*Метод опорных векторов* (SVM – support vector machine) – используется в тех слечаях, когда в данных присутствует шум, и результаты применения других регрессионных подходов не удовлетворяют по качеству решения: никакая гиперплоскость не может идеально отделить положительные примеры от отрицательных. 

Алгоритм SVM ищет объекты данных (вектора), которые расположены ближе всего к линии разделения. Эти точки называются опорными векторами. Затем, алгоритм вычисляет расстояние между опорными векторами и разделяющей плоскостью (это расстояние называется зазором). Основная цель алгоритма — максимизировать расстояние зазора. Лучшей гиперплоскостью считается такая гиперплоскость, для которой этот зазор является максимально большим.

<img src="assets/ml_06.png" width="400">
**Метод опорных векторов**

На практике случаи, когда данные можно разделить гиперплоскостью, случаются крайне редко. Если найденная гиперплоскость не позволяет уверенно отделить классы (т.н. *линейная неразделимость*), используется прием увеличения размерности пространства. Действительно, если нам удастся преобразовать исходное пространство в пространство более высокой размерности, мы могли бы надеяться, что примеры станут линейно разделимыми в этом преобразованном пространстве. В SVM используются различные функции для неявного преобразования исходного пространства в пространство более высокого измерения во время оптимизации функции стоимости.

<img src="assets/ml_07.png" width="400">
**Линейная неразделимость**

Например, на приведенной выше картинке можно ввести третью координату *z* (фактически: круг) разделяющую вектора на два класса (внутри круга и снаружи круга).

<img src="assets/ml_f4.png" width="150">

В этом случае классы определяются более точно.

 
**Дополнительные источники литературы по данному разделу:**

- [Лекции по методу опорных векторов. К.В. Воронцов](http://www.ccas.ru/voron/download/SVM.pdf)

- [Классификация данных методом опорных векторов](https://habr.com/ru/post/105220/)

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	


###  Алгоритм k-ближайших соседей<a name="4_3_4"></a>

kNN (k Nearest Neighbor) – алгоритм *k-ближайших соседей* использует весь набор данных в качестве обучающего набора, а не разделяет набор данных на обучающий набор и набор тестов.
Когда для нового экземпляра данных требуется результат, алгоритм KNN просматривает весь набор данных, чтобы найти k-ближайших экземпляров для нового экземпляра, или k экземпляров, наиболее похожих на новую запись, а затем выводит среднее значение результаты (для регрессии) или наиболее близкий класс (для задачи классификации). 

Сходство между экземплярами рассчитывается с использованием таких мер, как *Евклидово расстояние*, *мантхэттенское расстояние*, *расстояние Хемминга* и других.
Это самый четкий метод кластеризации, который все еще имеет некоторые недостатки. Прежде всего, мы должны знать количество кластеров. Во-вторых, результат зависит от точек, случайно выбранных в начале, и алгоритм не гарантирует, что мы достигнем глобального минимума функционала.

Для классификации каждого из объектов тестовой выборки необходимо последовательно выполнить следующие операции:

- Вычислить расстояние до каждого из объектов обучающей выборки

- Отобрать k объектов обучающей выборки, расстояние до которых минимально

- Класс классифицируемого объекта — это класс, наиболее часто встречающийся среди k ближайших соседей


<img src="assets/ml_08.png" width="400">
**Алгоритм k-ближайших соседей**

**Дополнительные источники литературы по данному разделу:**

- [Классификатор kNN](https://habr.com/ru/post/149693/)

- [Метод k-ближайших соседей. Wiki](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_k-%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B8%D1%85_%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9)

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	


## Оценка и тестирование модели <a name="4_4"></a>

Цель этого этапа, разработать простейшую модель, способную быстро и достаточно хорошо сформулировать целевое значение. Эта цель достигнута с помощью модели тюнинга. Это оптимизация параметров модели для достижения максимальной производительности алгоритма.


###  Перекрестная проверка<a name="4_4_1"></a>

Одним из наиболее эффективных методов оценки и настройки модели является перекрестная проверка. *Перекрестная проверка* является наиболее часто используемым методом настройки. Это влечет за собой разделение учебного набора данных на десять равных частей (складок). Данная модель обучается только в девяти сгибах, а затем проверяется на десятой (ранее не учтенной). Тренировка продолжается до тех пор, пока каждая складка не будет оставлена ​​в стороне и использована для тестирования. В результате измерения производительности модели для каждого набора гиперпараметров рассчитывается перекрестная оценка. Модели обучаются с использованием различных наборов гиперпараметров, чтобы определить, какая модель имеет самую высокую точность прогнозирования. Перекрестно подтвержденный балл указывает на среднюю производительность модели по десяти сгибам удержания.
Затем мы тестируем модели с набором значений гиперпараметров, которые получили лучший перекрестно проверенный результат. Существуют различные метрики ошибок для задач машинного обучения.


###  Улучшение прогнозов с помощью методов ансамбля<a name="4_4_2"></a>

Улучшение прогнозов с помощью методов ансамбля – исследователи данных в основном создают и обучают одну или несколько десятков моделей, чтобы иметь возможность выбрать оптимальную модель среди хорошо работающих. Модели обычно показывают разные уровни точности, поскольку они допускают разные ошибки в новых точках данных. Есть способы улучшить аналитические результаты. Методы ансамбля моделей позволяют достичь более точного прогноза, используя несколько наиболее эффективных моделей и комбинируя их результаты. Точность, как правило, рассчитывается по средним ормедианским выходам всех моделей в ансамбле. Среднее значение - это общее количество голосов, поделенное на их количество. Медиана представляет собой средний балл для голосов, упорядоченных по размеру.

Распространенными методами ансамбля являются Stacking, Bagging и Boosting.

*Stacking* –  Этот подход, также известный как многоуровневое обобщение, предлагает разработку метамодели или ученика более высокого уровня путем объединения нескольких базовых моделей. Stacking обычно используются для объединения моделей различных типов. Цель этого метода – уменьшить ошибку обобщения.

*Bagging* –  (начальная загрузка). Это метод последовательного объединения моделей. Сначала обучающий набор данных разбивается на подмножества. Затем модели обучаются на каждом из этих подмножеств. После этого прогнозы объединяются с использованием среднего или большинства голосов. Bagging помогает уменьшить ошибку дисперсии и избежать модели переобучения.

*Boosting* –  Согласно этой методике, работа делится на два этапа. Сначала мы используем подмножества исходного набора данных, чтобы разработать несколько моделей со средней эффективностью, а затем объединяем их, чтобы повысить производительность, используя большинство голосов. Каждая модель обучается на подмножестве, полученном в результате исполнения предыдущей модели, и концентрируется на неправильно классифицированных записях.

Можно развернуть модель, которая наиболее точно прогнозирует значения результатов в тестовых данных.


## Практическая часть <a name="4_6"></a>


Теперь мы готовы обучить модель и предсказать требуемое решение. Существует более 60 алгоритмов прогнозного моделирования. Мы должны понимать тип проблемы и требования решения, чтобы сузить наш выбор. Во первых важо пнять, что наша проблема - это проблема классификации и регрессии. Мы хотим определить связь между выходом (Longevity) с другими переменными или функциями (Sex, Age, Activity и другими). Во вторых, мы пприменяем обучение с учителем, так как мы обучаем нашу модель заданному размеченному набору данных. Используя эти два подхода (обучение с учителем и классификация/регрессия), мы можем сузить выбор моделей до слудующих:

-    Логистическая регрессия
-    KNN или k-Ближайшие соседи
-    Машина опорных векторов
-    Наивный байесовский классификатор
-    Дерево решений
-    Случайный лес
-    Персептон
-    Искусственная нейронная сеть
-    Метод релевантных векторов

### Проверка данных <a name="4_6_1"></a>

Перед запуском алгоритмов провери наши данные на дефекты для обучающего набора:

```python
train_df.head()         # показывает первые 10 значений датасета
``` 
Отчет об уникальных значениях:

```python
feature_names = train_df.columns.tolist() 
for column in feature_names: 
    print (column) 
    print (train_df[column].value_counts(dropna=False))
```

<img src="assets/ml_09.png" width="400">
**Обучающий набор**



Для тестового набора:

```python
test_df.head()         # показывает первые 10 значений датасета
``` 

Отчет об уникальных значениях:

```python
feature_names = test_df.columns.tolist() 
for column in feature_names: 
    print (column) 
    print (test_df[column].value_counts(dropna=False))
```

<img src="assets/ml_10.png" width="400">
**Тестовый набор**

### Обучение моделей <a name="4_6_2"></a>


Запустим алгоритм логистической регресии для наших данных.

Создадим экземпляр модели логистической регрессии с использованием функции «LogisticRegression» и применим модель к обучающему набору данных с помощью функции `fit`. Методо predict() определяет значения классов для всех переданных в качестве аргументов объектов X_test (т.е., собственно выполняет классификацию).

```python
 # Logistic Regression

logreg = LogisticRegression(solver='liblinear')
logreg.fit(X_train, Y_train)
Y_pred = logreg.predict(X_test)
acc_log = round(logreg.score(X_train, Y_train) * 100, 2)
acc_log
```

Мы можем проверить предположения, принятые нами при подготовке данных. Это можно сделать, рассчитав коэффициенты регрессии. 

```python
coeff_df = pd.DataFrame(train_df.columns.delete(0))
coeff_df.columns = ['Feature']
coeff_df["Correlation"] = pd.Series(logreg.coef_[0])
coeff_df.sort_values(by='Correlation', ascending=False)
```

Положительные значения коэффициентов говорят об увеличении вероятности долголетия при росте соответствующего критерия. Отметим, что регрессия учитывает знаки чисел, поэтому большие отрицательные значения также также указывают на сильное влияние с обратной зависимостью. 


**Дополнительные источники литературы по данному разделу:**

- [Логистическая регрессия в Python: от теории до трейдинга](http://distrland.blogspot.com/2019/05/python.html)

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	


### Развертывание модели машинного обучения Python в качестве веб-службы flask<a name="4_6_3"></a>

Развертывание модели (Model deployment) – этап развертывания модели включает в себя ввод модели в эксплуатацию.
После того, как мы выбрали надежную модель и определили ее требования к производительности, мы интегрируем модель с производственной средой.

Затем мы измеряем производительность модели с помощью A / B-тестирования. Тестирование может показать, как, например, число клиентов, работающих с моделью, используемой для персональной рекомендации, соотносится с бизнес-целью.

Рабочий процесс развертывания зависит от бизнес-инфраструктуры и проблемы, которую мы хотим решить. Прогностическая модель может быть ядром новой отдельной программы или может быть включена в существующее программное обеспечение.

Производительность модели также зависит от того, выполнили ли мы вышеупомянутые этапы (подготовка и предварительная обработка набора данных, моделирование) вручную с использованием собственной ИТ-инфраструктуры или автоматически с одним из машинного обучения в качестве сервисных продуктов.


*Пакетный прогноз* – Это вариант развертывания подходит, когда нам не нужны прогнозы на постоянной основе. Когда мы выбираем этот тип развертывания, мы получаем один прогноз для группы наблюдений. Модель обучается на статическом наборе данных и выводит прогноз. Развертывание не требуется, если необходим единый прогноз. Например, мы можем решить проблему классификации, чтобы узнать, принимает ли определенная группа клиентов предложение или нет.

*Веб-сервис* – такой рабочий процесс машинного обучения позволяет получать прогнозы практически в реальном времени. Модель, однако, обрабатывает одну запись из набора данных за раз и делает для нее прогнозы.

*Прогноз в реальном времени* (потоковое в реальном времени) – с помощью потоковой аналитики в реальном времени мы можем мгновенно анализировать потоковые данные в реальном времени и быстро реагировать на события, которые происходят в любой момент. Прогнозирование в реальном времени позволяет обрабатывать данные датчиков или рынка, данные из Интернета вещей или мобильных устройств, а также из мобильных или настольных приложений и веб-сайтов.


После обучения модели, которое мы выполнили в строке `logreg = LogisticRegression(solver='liblinear')`, модель можно экспортировать в файл на сервере и использовать ее не только в Jupyter Notebooks, но и в серверной части веб-службы. 

Установим необходимые компоненты в консоли и создадим директорию внутри нашего проекта:

```shell
conda install flask
mkdir ~/deploy
```

Для экспорта модели добавим строки в наш код Jupyter Notebook:  

```python
import pickle
pickle.dump(logreg, open("../deploy/linearmodel.pkl","wb"))
```

Предоставление модели в виде веб-службы может быть выполнено путем создания приложения Python Flask, которое может принимать запрос JSON и возвращать результат работы в качестве результата работы модели.

Создайте новый файл в каталоге развертывания и назовите его app.py:

```python
import flask
import pickle
import os
import pandas as pd

port = int(os.getenv("PORT", 9099))
app = flask.Flask(__name__, template_folder='./')
```
Далее необходимо загрузить нашу модель из файла:

```python
with open(f'linearmodel.pkl', 'rb') as f:
        model = pickle.load(f)
```

Далее опишем возвращаемое значение:

```python
@app.route('/', methods=['GET', 'POST'])
def main():
    if flask.request.method == 'GET':
        return(flask.render_template('main.html'))
    if flask.request.method == 'POST':

        Education = flask.request.form['Education']
        Sex = flask.request.form['Sex']
        Age = flask.request.form['Age']
        Activity = flask.request.form['Activity']
        Sport = flask.request.form['Sport']
        IsAlone = flask.request.form['IsAlone']
        AgeEducation = flask.request.form['AgeEducation']
        input_variables = pd.DataFrame([[Education,Sex,Age,Activity,Sport,IsAlone,AgeEducation]],
                                       columns=['Education','Sex','Age','Activity','Sport','IsAlone','Age*Education'],
                                       dtype=int64)
        prediction = model.predict(input_variables)[0]
        return flask.render_template('main.html',
                                     original_input={'Education':Education,'Sex':Sex,'Age':Age,'Activity':Activity,'Sport':Sport,'IsAlone':IsAlone,'Age*Education':AgeEducation},
                                     result=prediction,
                                     )
if __name__ == '__main__':
    app.run(host='192.168.1.2', port=port)

```

Теперь создадим файл main.html:

```html
<!doctype html>
<html>
<style>
form {
    margin: auto;
    width: 35%;
}
.result {
    margin: auto;
    width: 35%;
    border: 1px solid #ccc;
}
</style>
```

Заголовочная часть:
```html
<head>
    <title>Хакатон МГТУ им Н.Э.Баумана</title>
</head>
```

И основная форма (в файле нужно удалить обратный слеш "\" в конструкциях \%):

```html
<form action="{{ url_for('main') }}" method="POST">
    <fieldset>
        <legend>Input values:</legend>
        Education:
        <input name="Education" type="number" required>
        <br>
        <br> 
        Sex:
        <input name="Sex" type="number" required>
        <br>
        <br> 
        Age:
        <input name="Age" type="number" required>
        <br>
        <br> 
        Activity:
        <input name="Activity" type="number" required>
        <br>
        <br> 
        Sport:
        <input name="Sport" type="number" required>
        <br>
        <br> 
        IsAlone:
        <input name="IsAlone" type="number" required>
        <br>
        <br> 
        Age*Education:
        <input name="AgeEducation" type="number" required>
        <br>
        <br> 
        <input type="submit">
    </fieldset>
</form>
<br>
<div class="result" align="center">
    {\% if original_input \%}
        {\% for variable, value in original_input.items() \%}
            <b>{{ variable }}</b> : {{ value }}
        {\% endfor \%}
        <br>
        <br> Predicted Longevity is:
           <p style="font-size:50px">{{ result }}</p>
    {\% endif \%}
</div>
</html>
```

Далее необходимо открыть порт 9099 на шлюзе (см. День 1).

Далее запустим наше приложение и сервер flask:

```shell
python app.py
```

Результат работы серевера будет следующим:

<img src="assets/ml_11.png" width="400">
**Работающее веб-приложение на основе модели логистической регрессии**


**Дополнительные источники литературы по данному разделу:**

- [Развертывание модели машинного обучения Python в качестве веб-службы](https://developer.ibm.com/tutorials/deploy-a-python-machine-learning-model-as-a-web-service/)

- [Deploying a machine learning model to the web](https://blog.cambridgespark.com/deploying-a-machine-learning-model-to-the-web-725688b851c7)


## Задание <a name="4_7"></a>


1) Постройте и примените модель Машины опорных векторов (SVC и LinearSVC).

2) Постройте и примените модель k-Ближайших соседей (KNeighborsClassifier).

3) Постройте и примените модель Наивного байесовского классификатора (GaussianNB).

4) Постройте и примените модель Персептрона (Perceptron).

5) Постройте и примените классификатор на основе метода стохастического градиентного спуска (SGDClassifier)

6) Примените Дерево решений (DecisionTreeClassifier)

7) Примените классификатор на основе случайного леса деревьев (RandomForestClassifier)


## Выбор лучшей модели <a name="4_8"></a>

Теперь мы можем оценить наши модели и выбрать лучшую для классификации по выбранному нами критерию  долгожительства Longevity. 

```python
models = pd.DataFrame({
    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 
              'Random Forest', 'Naive Bayes', 'Perceptron', 
              'Stochastic Gradient Decent', 'Linear SVC', 
              'Decision Tree'],
    'Score': [acc_svc, acc_knn, acc_log, 
              acc_random_forest, acc_gaussian, acc_perceptron, 
              acc_sgd, acc_linear_svc, acc_decision_tree]})
models.sort_values(by='Score', ascending=False)
```


**Дополнительные источники литературы по данному разделу:**

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	

- [Введение в pandas: анализ данных на Python](https://khashtamov.com/ru/pandas-introduction/)	

- [Развертывание модели машинного обучения Python в качестве веб-службы](https://developer.ibm.com/tutorials/deploy-a-python-machine-learning-model-as-a-web-service/)

****
# День 4. Распознавание образов и компьютерное зрение <a name="5"></a>

## Введение  <a name="5_0"></a>

Рост населения в крупных городах и быстро растущая информатизация городских сфер требует современного подхода к обеспечению безопасности в городской среде. В этом контексте глубокое обучение и сверточные нейронные сети, решая проблемы компьютерного зрения, используются в задачах распознавания объектов на изображениях  и определения их свойств. 

Для контроля городского пространства все чаще используются камеры видеонаблюдения для захвата и удержания - обнаружения объектов и их взаимосвязь. Визуальное отношение отражает взаимодействие между объектами в видеопотоке («человек несет чемодан» или «человек оставил чемодан»). 

### Постановка задачи  <a name="5_0_1"></a>

Дана видеозапись или видеопоток с камеры видеонаблюдения. По результатам 4 дня  требуется обнаружить и классифицировать людей и рюкзаки на данных видеозаписи или видеопотоке.

## Нейронная сеть <a name="5_1"></a>

Нейронной сетью называется математическая модель, реализующая фукнции искусственного интеллекта путём воспроизведения нервной системы человека. Они используются для решения сложных задач, которые требуют аналитических вычислений, подобных тем, что делает человеческий мозг. К таким задачам относятся, например, классификация, кластеризация, прогнозирование, распознавание и т.д.

Искусственный нейрон представляет собой сумматор входных сигналов, применяющий к полученной взвешенной сумме некоторую простую функцию. Нейрон имеет синапсы - однонаправленные входные связи, соединённые с выходами других нейронов, а также аксон - выходную связь.

<img src="assets/artificial-neuron.PNG" width="600">
**Схема искусственного нейрона**
 

Текущее состояние нейрона определяется взвешенной суммой его входов (см. схему). Выход нейрона определяется его активационной функцией.
Существует несколько вариантов функций активанции.

<img src="assets/image040.jpg" width="500">


Чаще всего для свёрточных нейронных сетей используются сигмоидальные функции или их приближения:

-	логистическая функция;

-	гиперболический тангенс;

-	линейно-пороговая функция.

Сигмоидальные и подобные им функции хороши тем, что позволяют усиливать слабые сигналы и не насыщаться от сильных сигналов. 


Совокупность нейронов, расположенных на одном уровне в нейронной сети, называется слоем. В общем случае нейронная сеть включает в себя входной, выходной и промежуточные слои. Нейроны входного и выходного слоёв, как правило, имеют линейную функцию активации и предназначены для приёма и передачи данных. Нейроны промежуточных слоёв - нелинейные; их функцией активации чаще всего является сигмоид (логистическая функция):
![equation](https://latex.codecogs.com/png.latex?f%28x%29%20%3D%20%5Cfrac%7B1%7D%7B1%20&plus;%20e%5E%7B-%5Calpha%20x%7D%7D)

На схеме показан пример полносвязной нейронной сети, имеющей входной, промежуточный и выходной слои.

<img src="assets/image073.png" width="600">



Нейронная сеть обучаема. В процессе обучения параметры сети настраиваются в соответствии с обучающими наборами данных, моделирующих среду, в которой будет функционировать сеть. В зависимости от способа подстройки параметров различают обучение с учителем и без учителя.

Обучение с учителем представляет собой предъявление сети выборки обучающих примеров. Каждый образец подаётся на входы сети, проходит обработку и перерабатывается в выходной сигнал, который сравнивается с эталонным значением. Затем в зависимости от степени расхождения реального и идеального результатов изменяются весовые коэффициенты связей внутри сети. Обучение длится до тех пор, пока ошибка по всему обучающему массиву не достигнет приемлемо низкого уровня.

При обучении без учителя обучающее множество состоит лишь из входных векторов. Алгоритм обучения подстраивает веса внутри сети так, чтобы предъявление достаточно близких входных векторов давало одинаковые результаты.

Почитать подробнее про нейронные сети можно [здесь](http://www.aiportal.ru/articles/neural-networks "Статьи о нейронных сетях").



## Сверточные нейронные сети <a name="5_2"></a>

*Свёрточная нейронная сеть* — специальная архитектура искусственных нейронных сетей, основанные на представлении изображений в виде тензоров и нацеленная на эффективное распознавание образов в графических изображениях. Тензоры — это 3-х мерные массивы.

Для распознавания образов могут использоваться и простые модели нейронных сетей, такие как  многослойный персептрон. Однако, если размеры изображения велики, то число и сложность слоев нейронной сети многократно увеличивается, а процесс обучения существенно усложняется. Другим недостатком многослойных нейронных сетей является векторный характер представления данных, что делает невозможный двумерную локализации пикселей и обработку деталей на изображении.

Как и полносвязная нейронная сеть, свёрточная сеть обучается с помощью алгоритма обратного распространения ошибки. Сначала выполняется прямое распространение от первого слоя к последнему, после чего вычисляется ошибка на выходном слое и распространяется обратно. При этом на каждом слое вычисляются градиенты обучаемых параметров, которые в конце обратного распространения используются для обновления весов с помощью градиентного спуска.

Свёрточные нейронные сети состоят из последовательно соединенных слоев нескольких типов, выполняющими преобразования над поступающей матрицей или несколькими матрицами (например, исходное изображение представляется тремя матрицами R,G и B компонент цветности). Обычно, свёрточные НС построены на чередовании свёрточных слоёв, реализующих функцию свёртки, и субдискретизирующих слоёв, ответственных за выборку наиболее подходящего раздражителя — и тем самым уменьшающих размер обрабатываемого изображения. Также в сверточных НС используются элементы полносвязных персептронов: слои активации и полносвязные слои. Из слоев различных типов можно конструировать НС, наиболее подходящие для каждой конкретной задачи. 

### Сверточный слой <a name="5_2_1"></a>

Когда входное изображение поступает на этот слой, к нему применяется операция свёртки, которая заключается в перемножении элементов фрагмента изображения с соответствующими элементами ядра свёртки и записью результата напротив центрального элемента фрагмента. Следующий пример показывает, как в практических приложениях производится операция свёртки над изображением с ядром размером 3х3:
 
<img src="assets/image050.gif" width="800">

где буквы *a - i* — соответствующие пиксели фрагмента изображения, цифры 1-9 — соответствующие коэффициенты ядра функции свёртки, [2,2] — координаты элемента, на место которого необходимо вставить получившуюся сумму попарных произведений.

В этом слое операция свёртки производится параллельно над каждым пикселем изображения. Если пиксель находится в углу или так, что одному или нескольким коэффициентам ядра нет соответствующего пикселя, применяют одну из двух стратегий:

а.	недостающие пиксели заполняются тем же значением, что и ближайший к нему пиксель изображения;

б.	такие пиксели отбрасываются и выходное изображение получается несколько меньшего размера.

Результаты применения стратегии a) свертки и ядра 2x2
![equation](https://latex.codecogs.com/gif.latex?%5Cdpi%7B200%7D%20%5Ctiny%20%5Cbegin%7Bpmatrix%7D%201%20%26%202%5C%5C%203%20%26%204%20%5Cend%7Bpmatrix%7D)
показаны соответственно на рисунках:

<img src="assets/image062.png" width="500">

<img src="assets/image063.png" width="250">
**Результаты применений стратегий а) свертки к изображению.**


Иногда после свёрточного слоя вставляется так называемый слой *ReLU — Rectified Linear Unit* — блок линейной свёртки. Он состоит из матрицы активационных функций размером с выходное изображение. Активационные функции выбираются чаще всего не сигмоидальные, а ненасыщаемые функции вида 

![equation](https://latex.codecogs.com/gif.latex?F_%7Bact%7D%28y%29%3Dmax%20%5Cleft%20%28%200%2Cy%20%5Cright%20%29)

или

![equation](https://latex.codecogs.com/gif.latex?F_%7Bact%7D%28y%29%3D%20%5Cbeta%20e%5E%7B-%5Calpha%20y%7D). 

Цель применения свёрточного слоя — выявить общие детали множества изображений. Так как ядра свёртки обучаются под общие детали, влияние шума на них минимизируется по сравнению с многослойным перцептроном. В них для каждого входного слоя есть соединение с каждым нейтроном на входном слое, тем самым способствуя обучению и общим и частным деталям.

Таким образом использование свёрточных слоёв позволяет расположить нейросеть к выделению более общих частей по сравнению с перцептроном, вместе с тем занимая меньше места при хранении и в оперативной памяти, так как хранятся не сонмы связей, а небольшое количество ядер свёртки.

Добавление блока ReLU позволяет ещё сильнее снизить влияние входно-го шума на выход слоя, тем самым упрощая работу при обучении.



### Субдискретизирующий слой <a name="5_2_2"></a>

Слой субдискретизации служит для уменьшения изображения посредством нелинейных преобразований. Идея, положенная в основу создания этого слоя такова: когда свёрточный слой выявил признаки, чрезмерно высокую детализацию можно отсечь и получить на выходе только общую картину.

Этот слой не является необходимым в свёрточных нейронных сетях, и иногда опускается, однако его использование даёт выигрыш по скорости работы и обучения слоёв, идущим за ним. Можно сказать, что этот слой помогает свёрточной нейронной сети абстрагироваться от деталей и частично увидеть общую картину.

Слой субдискретизации характеризуется следующими параметрами:

а)	размер субдискретизируемой группы пикселей – обычно 2х2;

б)	шаг применения функции дискретизации – обычно равен ширине субдискретизируемой группы пикселей, то есть двум;

в)	функция субдискретизации – обычно выбирается максимальное значение из группы пикселей, но бывает выборка минимума или выборка среднего арифметического значения.

Принцип работы этого слоя представлен на рисунке ниже: к группе пиксе-лей, например, 2х2, применяется функция субдискретизации, в текущем случае – выборка максимального значения. Результат функции субдискретизации, применённой к группе пикселей входного изображения, записывается в единственный пиксель выходного изображения.

<img src="assets/image070.png" width="500">
**Результат применения слоя субдискретизации к изображению**

Если перед субдискретизирующим слоем уже стоял блок ReLU, бессмысленно ставить такой же после этого слоя, так как выходное значение не изменится. Если блок ReLU будет использовать иную активационную функцию, значение изменится, но того же эффекта можно было бы добиться, изменив функцию предыдущего блока.

Таким образом, после слоя субдискретизации обыкновенно не ставят слой ReLU. Применение субдискретизирующего слоя заключается в упрощении дальнейшей обработки изображения, проходящего по конвейеру и в вычленении общих деталей изображения.


### Обучение НС <a name="5_2_3"></a>

Процесс некоторого определённого изменения весов связей нейронов и значений элементов свёрточных слоёв с целью получения некоторой опреде-лённой реакции нейронной сети на входные данные именуется обучением НС. Оно происходит до тех пор, пока не наступит определённое условие, чаще всего – процент ошибок на обучающем наборе должен опуститься ниже определённого значения.

Ниже приведена классификация способов обучения НС:

а.	по наличию элемента случайности:

  1.	детерминистские методы – процедура обучения НС основана на использовании текущих значений весов нейронов и элементов ядер свёртки и желаемом выходе сети;

  2.	стохастические методы – процедура обучения НС основана на случайном изменении весов в соответствии с определён-ной функцией распределения;

б.	по способу определения корректности результата:

  1.	с учителем – процедура обучения НС основана на одновре-менной подаче входных и выходных данных. Если выход нейронной сети совпадает с требуемым, то берётся следую-щий набор данных, в противном случае нейронная сеть до-обучается;

  2.	с последовательным подкреплением знаний – процедура обучения НС основана на подаче входных данных и последующей оценке выходных данных. Оценка бывает либо вида «хорошо-плохо», либо численной;

  3.	без учителя – .процедура обучения НС основана на подаче входных данных, а дальше нейронная сеть сама пытается вы-членить особенности входных данных.

Методов обучения, попадающих под эту классификацию, очень много, однако для используемой свёрточной НС наиболее подходящий и наиболее популярный – метод обратного распространения ошибки. Он применим только к НС с прямым распространением сигнала, то есть без обратных связей, и используемая НС является таковой.

Алгоритм обратного распространения ошибки следующий:

а.	инициализировать веса нейронов и фильтров маленькими случай-ными значениями;

б.	выбрать очередную обучающую пару из обучающего множества; подать входной вектор на вход сети;

в.	вычислить выход сети;

г.	вычислить разность между выходом сети и требуемым выходом;

д.	подкорректировать веса сети для минимизации ошибки;

е.	Повторять шаги со второго по пятый для каждой пары обучающего множества до тех пор, пока ошибка на всем множестве не достигнет приемлемого уровня.

Шаги б) и в) образуют так называемый «проход вперёд», так как сигнал распространяется по сети от входа к выходу. Шаги г) и д) составляют «обратный проход», здесь вычисляемый сигнал ошибки распространяется обратно по сети и используется для подстройки весов.

При обучении НС следует избегать переобучения НС, когда на обучаю-щей выборке ошибок почти нет, а на реальных данных возникает крайне силь-ное расхождение, то есть стоит избегать слишком сильной «подгонки» резуль-татов работы НС под входные данные.

## Библиотека PyTorch <a name="5_3"></a>

*PyTorch* - это фреймворк машинного обучения для языка Python, созданный на базе библиотеки *Torch*. Фреймворк предоставляет разработчикам функционал для сбора данных, а также для построения и обучения моделей, основанных на нейронных сетях.

PyTorch предоставляет две основные высокоуровневые модели:
- Тензорные вычисления (по аналогии с NumPy) с развитой поддержкой ускорения на GPU;
- Глубокие нейронные сети на базе системы autodiff.

Тензоры в PyTorch представляют собой многомерные массивы. Они похожи на массивы пакета *numpy*, но дополнительно могут обрабатываться на видеоускорителях.

PyTorch создан на базе библиотеки *Torch*, написанная для языка Lua, ядро которой написано на языке Си. Таким образом, язык Python в PyTorch используется для упрощения синтаксических конструкций и использования высокоуровневых программных абстракций.

Приложения PyTorch можно запускать как на локальном компьютере, так и в облачном кластере, на устройствах iOS и Android, на мобильных процессорах, встраиваемых системах типа RaspberryPi или графических процессорах. Полученные в результате исследований модели PyTorch могут быть развернуты на любом устройстве, где они будут использоваться для рапознавания и формирования прогнозов.

## Поиск объектов на изображении с использованием PyTorch <a name="5_4"></a>

Итак, задачи компьютерного зрения включают в себя следующие направления:

- Localization (обнаружение объекта)
- Classification (классификация объекта)
- Identification (идентификация объекта)
- Оbject recognition (распознавание объекта)
- Object detection (обнаружение и классификация объекта)
- Object tracking (отслеживание объекта)
- Instance segmentation (попиксельное обнаружение всех объектов)
- Semantic segmentation (обнаружение границ всех объектов)
- Image segmentation (обнаружение границ объекта)

Мы разработаем программный код для решения задачи Object detection. 

Object Detection — это просто определение объектов на картинке/кадре. То есть алгоритм или нейронная сеть определяют объект и записывают его позицию и bounding boxes (параметры прямоугольников вокруг объектов). Пока что речи о других кадрах не идет, и алгоритм работает только с одним.

Пример:

<img src="assets/image074.png" width="800">

Существует множество архитектур нейронных сетей для определения объектов. В основном, они подразделяются на «двухуровневые», такие как RCNN, fast RCNN и faster RCNN, и «одноуровневые», такие как YOLO. Список наиболее популярных архитектур:

- **R-CNN**. Можно сказать первая модель для решения данной задачи. Работает как обычный классификатор изображений. На вход сети подаются разные регионы изображения и для них делается предсказания. Очень медленная так как прогоняет одно изображение несколько тысяч раз.
- **Fast R-CNN**. Улучшенная и более быстрая версия R-CNN, работает по похожему принципу, но сначала все изображение подается на вход CNN, потом из полученного внутреннего представления генерируются регионы. Но по прежнему довольно медленная для задач реального времени.
- **Faster R-CNN**. Главное отличие от предыдущих в том, что вместо selective search алгоритма для выбора регионов использует нейронную сеть для их «заучивания».
- **YOLO**. Совсем другой принцип работы по сравнению с предыдущими, не использует регионы вообще. Наиболее быстрая. Более подробно о ней пойдет речь в статье.
- **SSD**. По принципу похожа на YOLO, но в качестве сети для извлечения признаков использует VGG16. Тоже довольная быстрая и пригодная для работы в реальном времени.
- **Feature Pyramid Networks (FPN)**. Еще одна разновидность сети типа Single Shot Detector, из за особенности извлечения признаков лучше чем SSD распознает мелкие объекты.
- **RetinaNet**. Использует комбинацию FPN+ResNet и благодаря специальной функции ошибки (focal loss) дает более высокую точность (аccuracy).

"Двухуровневые" архитектуры используют так называемые регионы на картинке (от англ. *Region of Interest*), чтобы определить, находится ли в этом регионе определенный объект. Здесь используется такой примерный алгоритм с ипользованием двух нейронных сетей: 
- прогон картинки через CNN для определения карты признаков;
- поиск тех самых регионов с объектами на картинке отдельной нейронной сетью; 
- сжатие найденных регионов с помощью *RoI pooling* и их подача в следующую нейронную сеть для определения класса объекта внутри региона.

<img src="assets/image075.png" width="800">

Такие нейронные сеть не смотрят на картинку полностью, а только на отдельные регионы. Также работают они относительно медленно.

В данной статье мы будем использовать архитектуру YOLO, а именно её последнюю модификацию YOLOv3.

В чем же крутость YOLO? В том, что эта архитектура не имеет двух проблем свыше, и она доказала неоднократно свою эффективность.

Вообще архитектура YOLO в первых блоках не сильно отличается по «логике блоков» от других детекторов, то есть на вход подается картинка, дальше создаются карты признаков с помощью CNN (в YOLO используется архитектура CNN под названием Darknet-53), затем эти карты признаков анализируются, выдавая на выходе позиции и размеры ограничивающих прямоугольников и классы, которым они принадлежат.

Потребуется разобраться со следующими проблемами:

1) Найти набор данных для обучения и валидации под поставленную задачу
2) Приготовить найденный набор для выбранной модели
3) Выбрать модель для решения задачи Object detection
4) Настроить параметры обучения в аргументах командной строки
5) Запустить проверку обученной модели на выбранной видеозаписи

Итак, поехали!

### Настройка окружения <a name="5_4_0"></a>

Настройка собственного окружения для работы над проектом требуется из-за специфики мира Python и его зависимостей или пакетов, версий которых (даже для одного пакета) существует бесчисленное множество. Поэтому желательно иметь набор зависиместей исключительно под тот проект, над которым сейчас ведется разработка, и нежелательно засорять системное хранилище зависимостей. Простой пример: в ОС может вестись работа над несколькими проектами, каждый из которых не только использует один и тот же пакет разных версий, но и не может работать с пакетом иной версии. 

- Установить окружение ```conda create --name py35 python=3.5```
- Зайти в окружение ```source activate py35```
- Выйти из окружения ```source deactivate```
- итд [CONDA CHEAT SHEET](https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf "Шпаргалка по conda")

Можно использовать ```pyenv``` - выбирать вам (процесс установки зависимостей будет немного отличаться).

### Поиск набора данных <a name="5_4_1"></a>

Нашим набором данных для тренировки и валидации будет является массив изображений людей (Person) и рюкзаков (Backpack). 

Популярным источником изображений для формирования собственного набора данных является [Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html "Набор данных тут"). 

Наборы данных по категориям можно посмотреть тут [Поиск изображений по категориям](https://storage.googleapis.com/openimages/web/visualizer/index.html?set=train&type=segmentation&r=false&c=%2Fm%2F0cyhj_ "Тут можно найти апельсины").

К сожалению, данный источник не предоставляет какого-то удобного инструмента для загрузки исключительно требуемых категорий изображений.

Решить эту проблему нам поможет следующий репозиторий на GitHub: [OIDv4 ToolKit](https://github.com/EscVM/OIDv4_ToolKit/ "Инструмент юного кастомайзера датасетов").

Требуется ознакомиться с документацией в этом репозитории, а именно: Какая структура папок после загрузки требуемых категорий изображений?

Для загрузки наборов данных (Person и Backpack) выполните следующую команду (также обратите внимани на документацию параметров командной строки):

```python3 main.py downloader --classes Person Backpack --type_csv validation```

И следуйте инструкциям на экране (подсказка: соглашайтесь на все :-) ).

Вопрос с набором данных решен! (нет)

### Подготовка набора данных <a name="5_4_3"></a>

Для обучения модели необходимо подготовить данные.
Оригинальная директория с набором данных выглядит следующим образом:

```
- Dataset
  - validation
    - Backpack
      - Label
        - 123.txt
        - 456.txt
        - …
      - 123.jpg
      - 456.jpg
```
*Backpack* - класс изображений

Файлы папки *Labels* содержат следующую информацию об ограничивающих прямоугольниках в каждом изображении:

`<номер класса> <x> <y> <длина прямоугольника> <ширина прямоугольника> `

*x, y* - координаты середины прямоугольника относительно реального размера изображения.
Длина и ширина прямоугольника также даны относительно реального размера изображения.

Директорию с данными нужно привести к следующему виду:

```
- dataset
  - images
    - 123.jpeg
    - 456.jpeg
    - …
  - labels
    - 123.txt
    - 456.txt
    - …
  - train.txt
  - valid.txt
```

Папка *images* содержит набор данных, который будет подаваться на вход модели во время ее обучения или валидации.
Папка *labels* содержит текстовые файлы, названия которых соответствуют названиям набора данных из папки *images*. Эти файлы должны содержать следующую информацию о каждом ограничивающем прямоугольнике в отдельной строке файла:

`<Название класса> <x1> <y1> <x2> <y2>`

*x1, y1, x2, y2* - координаты ограничивающего прямоугольника. Диапазон координат - [0, 1].

В скачанном датасете информация об ограничивающих прямоугольниках не нормализована, то есть координаты ограничивающих прямоугольников даны относительно настоящего размера изображения. В данном случае эти данные нужно нормализовать.

Пример: `Backpack 0.4577515 0.486005 0.915503 0.971942`.

Файлы *train.txt* и *valid.txt* содержат в каждой отдельной строке полный путь от корня до изображений из папки *images* для процессов тренировки и валидации модели соответственно.

Например, в файле train.txt (valid.txt аналогично): `/Users/user/Documents/hackaton/data/dataset/images/73a95027d155bb92.jpg`

### Выбор модели <a name="5_4_2"></a>

Довольно популярным (хотя уже и старым) решением задачи Object Detection является проект YOLO.

- Официальный веб-сайт: https://pjreddie.com/darknet/yolo/
- Научная публикация с описанием модели: https://arxiv.org/pdf/1804.02767v1.pdf
- Исходный код для обучения, валидации и тестирования модели: https://github.com/eriklindernoren/PyTorch-YOLOv3/ 

Небольшое отступление (необязательно к прочтению):
> На самом деле история проекта очень мудреная.

> История проекта описана здесь: 
> https://medium.com/towards-artificial-intelligence/yolo-v5-is-here-custom-object-detection-tutorial-with-yolo-v5-12666ee1774e 

> Официально существует всего 3 версии YOLO (со ссылками на научные публикации):
> - YOLOv1 (https://arxiv.org/pdf/1506.02640.pdf)
> - YOLOv2 (https://arxiv.org/pdf/1612.08242.pdf) 
> - YOLOv3 (https://arxiv.org/abs/1804.02767v1)

> После третьей версии автор проекта остановил разработку, предложив сообществу развивать проект в свободном от него формате. Таким образом, неофициально сущесвует еще 2 версии YOLO:
> - YOLOv4 (https://arxiv.org/pdf/2004.10934)
> - YOLOv5 (https://github.com/ultralytics/yolov5) 

Почему YOLO? Да потому что YOLO считается эффективнее многих других алгоритмов для определения объектов. 

Основная идея YOLO сделать за один проход классификацию того, что может быть на картинке, а потом впаять BB (bounding boxes). Таким образом, мы можем использовать любую архитектуру классификатора, как-то подготовленную для YOLO.

YOLO или You Only Look Once — это очень популярная на текущий момент архитектура CNN, которая используется для распознавания множественных объектов на изображении.

Главная особенность этой архитектуры по сравнению с другими состоит в том, что большинство систем применяют CNN несколько раз к разным регионам изображения, в YOLO CNN применяется один раз ко всему изображению сразу. Сеть делит изображение на своеобразную сетку и предсказывает bounding boxes и вероятности того, что там есть искомый объект для каждого участка.

Плюсы данного подхода состоит в том, что сеть смотрит на все изображение сразу и учитывает контекст при детектировании и распознавании объекта. Так же YOLO в 1000 раз быстрее чем R-CNN и около 100x быстрее чем Fast R-CNN. В данной статье мы будем запускать сеть на мобильном устройстве для онлайн обработки, поэтому это для нас это самое главное качество.

YOLO (You Only Look Once) несет в себе философию смотреть на картинку один раз, и за этот один просмотр (то есть один прогон картинки через одну нейронную сеть) делать все необходимые определения объектов. Как это происходит?

Итак, на выходе от работы YOLO мы обычно хотим вот это:

<img src="assets/image077.png" width="800">

Что делает YOLO когда учится на данных (простыми словами):

**Шаг 1**: Обычно картинки решейпят под размер 416x416 перед началом обучения нейронной сети, чтобы можно было их подавать пакетами (для ускорения обучения).

**Шаг 2**: Делим картинку (пока что мысленно) на клетки размером axa. В YOLOv3-4 принято делить на клетки размером 13x13.

<img src="assets/image078.png" width="800">

Теперь фокусируемся на эти клеточках, на которые мы разделили картинку/кадр. Такие клетки, которые называются grid cells, лежат в основе идеи YOLO. 
Каждая клетка является «якорем», к которому прикрепляются ограничивающие прямоугольники. 
То есть вокруг клетки рисуются несколько ограничивающих прямоугольников для определения объекта (поскольку непонятно, 
какой формы прямоугольник будет наиболее подходящим, их рисуют сразу несколько и разных форм), и их позиции, ширина и высота вычисляются относительно центра этой клетки.

<img src="assets/image079.png" width="800">

Как же рисуются эти ограничиваюшие прямоугольники (bounding boxes) вокруг клетки? 
Как определяется их размер и позиция? 
Здесь в борьбу вступает техника anchor boxes (в переводе — якорные коробки, или «якорные прямоугольники»). 
Они задаются в самом начале либо самим пользователем, либо их размеры определяются исходя из размеров bounding boxes, 
которые есть в датасете, на котором будет тренироваться YOLO (используется K-means clustering и IoU для определения самых подходящих размеров). 
Обычно задают порядка 3 различных anchor boxes, которые будут нарисованы вокруг (или внутри) одной клетки:

<img src="assets/image080.png" width="800">

Зачем это сделано? Сейчас все будет понятно, так как мы обсудим то, как YOLO обучается.

**Шаг 3**. Картинка из набора данных прогоняется через нашу нейронную сеть 
(кроме картинки в тренировочном наборе у нас должны быть определенны позиции и размеры настоящих ограничивающих прямоугольников для объектов, которые есть на ней. Это называется «аннотация» и делается это в основном вручную).

Давайте теперь подумаем, что нам нужно получить на выходе.

Для каждой клетки, нам нужно понять две принципиальные вещи:

Какой из anchor boxes, из 3 нарисованных вокруг клетки, нам подходит больше всего и как его можно немного подправить для того, чтобы он хорошо вписывал в себя объект
Какой объект находится внутри этого anchor box и есть ли он вообще

Какой же должен быть тогда output у YOLO?

1) На выходе для каждой клетки мы хотим получить:

<img src="assets/image081.png" width="800">

2. Output должен включать в себя вот такие параметры:

<img src="assets/image082.png" width="800">

Как определяется objectness? На самом деле этот параметр определяется с помощью метрики IoU во время обучения. Метрика IoU работает так:

<img src="assets/image083.png" width="800">

В начале Вы можете выставить порог для этой метрики, и если Ваш предсказанный ограничивающий прямоугольник будет выше этого порога, то у него будет objectness равной единице, 
а все остальные ограничивающие прямоугольники, у которых objectness ниже, будут исключены. Эта величина objectness понадобится нам, когда мы будем считать общий confidence score (на сколько мы уверены, что это именно нужный нам объект расположен внутри предсказанного прямоугольника) у каждого определенного объекта.

А теперь начинается самое интересное. Представим, что мы создатели YOLO и нам нужно натренировать ее на то, чтобы распознавать людей на кадре/картинке. 
Мы подаем картинку из датасета в YOLO, там происходит feature extraction в начале, а в конце у нас получается CNN слой, который рассказывает нам о всех клеточках, на которые мы «разделили» нашу картинку. 
И если этот слой рассказывает нам «неправду» о клеточках на картинке, то у нас должен быть большой Loss, чтобы потом его уменьшать при подаче в нейронную сеть следующих картинок.

YOLO предсказывает 5 параметров (для каждого anchor box для определенной клетки):

<img src="assets/image084.png" width="800">

Чтобы было легче понять, есть хорошая визуализация на эту тему:

<img src="assets/image085.png" width="800">

Как можно понять их этой картинки, задача YOLO — максимально точно предсказать эти параметры, чтобы максимально точно определять объект на картинке. 
А confidence score, который определяется для каждого предсказанного ограничивающего прямоугольника, является неким фильтром для того, чтобы отсеять совсем неточные предсказания. 
Для каждого предсказанного ограничивающего прямоугольники мы умножаем его IoU на вероятность того, что это определенный объект 
(вероятностное распределение рассчитывается во время обучения нейронной сети), берем лучшую вероятность из всех возможных, 
и если число после умножения превышает определенный порог, то мы можем оставить этот предсказанный ограничивающий прямоугольник на картинке.

Дальше, когда у нас остались только предсказанные bounding boxes с высоким confidence score, наши предсказания (если их визуализировать) могут выглядеть примерно вот так:

<img src="assets/image086.png" width="800">

Мы можем теперь использовать технику NMS (non-max suppression), чтобы отфильтровать ограничивающие прямоугольники таким образом, чтобы для одного объекта был только один предсказанный ограничивающий прямоугольник.

<img src="assets/image087.png" width="800">

Нужно также знать, что YOLOv3-4 предсказывают на 3-х разных скейлах. То есть картинка делится на 64 grid cells, на 256 клеток и на 1024 клетки, чтобы также видеть маленькие объекты. Для каждой группы клеток алгоритм повторяет необходимые действия во время предсказания/обучения, которые были описаны сверху.

Исходники модели для обучения находят в следующем репозитории на GitHub: [skynet-dl/PyTorch-YOLOv3](https://github.com/skynet-dl/PyTorch-YOLOv3.git "Моделька для детекшена") как форк с небольшими изменениями от официальной реализации для PyTorch [eriklindernoren/PyTorch-YOLOv3](https://github.com/eriklindernoren/PyTorch-YOLOv3.git "Офишиал моделька для детекшена").

Для начала потребуется установить все требуемые зависимости, для этого запустите 2 команды:

1) `conda install --force-reinstall -y -c conda-forge --file requirements.txt`
2) `conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch`

Далее редактируем следующие файлы для настройки обучения на подготовленном наборе данных:
- `PyTorch-YOLOv3/config/custom.data` - указываем количество классов (2 - Person и Backpack), прописываем пути (относительные или абсолютные) к ранее описанным файлам `train.txt` и `valid.txt`, прописываем путь к файлу с именами классов
- `PyTorch-YOLOv3/config/custom.names` - прописываем на отдельных строчках названия классов `BAG` и `PERSON` (Внимание: номер строчки с название соответсвует номеру класса `BAG`<=>0, `PERSON`<=>1)

И вызываем скрипт `PyTorch-YOLOv3/config/create_custom_model.sh` с параметром `2` (параметр соотвествует количеству классов - Person и Backpack) для настройки парамтров слове модели для обучения.

### Настройка параметров обучения <a name="5_4_4"></a>

Пример команды запуска начала обучения:

`python3 train.py --model_def config/yolov3-custom.cfg --data_config config/custom.data --epochs 500 --batch_size 6 --n_cpu 100 --checkpoint_interval 50 --evaluation_interval 50`

Основные параметры:

- `--model_def config/yolov3-custom.cfg` - конфигурация слоев модели
- `--data_config config/custom.data` - настройки обучения (количество классов, основные пути)
- `--epochs 500` - количество эпох до конца обучения
- `--batch_size 6` - размер пакета изображений из тренировочного набора данных
- `--n_cpu 100` - количество потоков для загрузки пакета изображений
- `--checkpoint_interval 50` - частота сохранений текущего состояния обученной модели (папка сохранения - `PyTorch-YOLOv3/checkpoints`)
- `--evaluation_interval 50` - частота проверки текущего состояния модели во время обучентя

Во время обучения можно следить за измнением большого числа параметров точности и потерь при обучении (с параметрами можно ознакомиться самостоятельно из статей).
Основными параметрами для наблюдения являются суммарный `Loss` и `Precision` на каждом из 3 слоев модели (здесь имеется ввиду архитектурные слои).

Для этого запускается утилита Tensorboard:

> #####Track training progress in Tensorboard:
> Initialize training   
> Run the command below     
> Go to http://localhost:6006/  
> `$ tensorboard --logdir='logs' --port=6006`

Если обучение запускается на удаленном сервере, то можно сделать мост до удаленного `localhost:6006`.
Для этого можно использовать команду: `ssh -N -f -L localhost:6006:localhost:6006 <user>@<ip>`

Таким же образом можно запускать jupyter notebook с отображением на персональном ноутбуке.

### Проверка обученной модели <a name="5_4_5"></a>

Пример команды проверки обученной модели на видеоролике:

`python3 detect_video.py --vedio_file ../video/videoplayback3.mp4 --model_def config/yolov3-custom.cfg --weights_path ./yolov3_ckpt_200d.pth --class_path config/custom.names`

Основные параметры:

- `--vedio_file ../video/videoplayback3.mp4` - путь к видеофайлу
- `--model_def config/yolov3-custom.cfg` - путь к конфигурации слоев обученной модели
- `--weights_path ./yolov3_ckpt_200d.pth` - путь к обученным весам модели
- `--class_path config/custom.names` - путь к списку классов для поиска

После окончания работы скрипта в корне появится видеофайл `output.avi` с разметкой объектов в видеопотоке.

Конец!

<img src="assets/image076.png" width="800">


****
# День 5. Графы знаний <a name="6"></a>


В этом задании мы рассмотрим средства и методы обработки графов знаний, используемых для хранения результатов машинного обучения (фактов). 


**Дополнительные источники литературы по данному разделу:**



****
# Дополнительные источники <a name="a001"></a>

<a name="pub1">[1]</a> [Yuji Roh. A Survey on Data Collection for Machine Learning: a Big Data - AI Integration Perspective](https://arxiv.org/abs/1811.03402)

<a name="pub2">[2]</a> [Behera, Rabi. A Survey on Machine Learning: Concept, Algorithms and Applications](https://www.researchgate.net/publication/316273553_A_Survey_on_Machine_Learning_Concept_Algorithms_and_Applications)


<a name="pub3">[3]</a> [scikit-learn / Machine Learning in Python](https://scikit-learn.org/stable/)

<a name="pub4">[4]</a> [Learn Data Science / Open content for self-directed learning in data science](http://learnds.com/)

<a name="pub5">[5]</a> [ML Boot Camp / Руководство для начинающих](https://mlbootcamp.ru/article/tutorial/)
